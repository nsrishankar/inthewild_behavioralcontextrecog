{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Dataset-summary\" data-toc-modified-id=\"Dataset-summary-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Dataset summary</a></span></li><li><span><a href=\"#SMiLE-(Semi-supervised-multi-label-classification-using-incomplete-label-information)\" data-toc-modified-id=\"SMiLE-(Semi-supervised-multi-label-classification-using-incomplete-label-information)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>SMiLE (Semi-supervised multi-label classification using incomplete label information)</a></span></li><li><span><a href=\"#Raw-SVM\" data-toc-modified-id=\"Raw-SVM-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Raw SVM</a></span></li><li><span><a href=\"#1-NearestNeighbor\" data-toc-modified-id=\"1-NearestNeighbor-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>1-NearestNeighbor</a></span></li><li><span><a href=\"#Multi-MLP\" data-toc-modified-id=\"Multi-MLP-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Multi-MLP</a></span><ul class=\"toc-item\"><li><span><a href=\"#0H\" data-toc-modified-id=\"0H-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>0H</a></span></li><li><span><a href=\"#1H\" data-toc-modified-id=\"1H-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>1H</a></span></li><li><span><a href=\"#2H\" data-toc-modified-id=\"2H-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>2H</a></span></li><li><span><a href=\"#2HDrop\" data-toc-modified-id=\"2HDrop-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>2HDrop</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import glob\n",
    "import pickle\n",
    "import copy\n",
    "import math\n",
    "from io import StringIO\n",
    "import importlib.machinery\n",
    "from scipy import stats, optimize\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='dataset/Extrasensory_uuid_fl_uTAR/'\n",
    "user_sample='3600D531-0C55-44A7-AE95-A7A38519464E.features_labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parsers for header/ body for CSVs\n",
    "def parse_header_of_csv(csv_str):\n",
    "    # Isolate the headline columns:\n",
    "    headline = csv_str[:csv_str.index('\\n')];\n",
    "    columns = headline.split(',');\n",
    "\n",
    "    # The first column should be timestamp:\n",
    "    assert columns[0] == 'timestamp';\n",
    "    # The last column should be label_source:\n",
    "    assert columns[-1] == 'label_source';\n",
    "    \n",
    "    # Search for the column of the first label:\n",
    "    for (ci,col) in enumerate(columns):\n",
    "        if col.startswith('label:'):\n",
    "            first_label_ind = ci;\n",
    "            break;\n",
    "        pass;\n",
    "\n",
    "    # Feature columns come after timestamp and before the labels:\n",
    "    feature_names = columns[1:first_label_ind];\n",
    "    # Then come the labels, till the one-before-last column:\n",
    "    label_names = columns[first_label_ind:-1];\n",
    "    for (li,label) in enumerate(label_names):\n",
    "        # In the CSV the label names appear with prefix 'label:', but we don't need it after reading the data:\n",
    "        assert label.startswith('label:');\n",
    "        label_names[li] = label.replace('label:','');\n",
    "        pass;\n",
    "    \n",
    "    return (feature_names,label_names);\n",
    "\n",
    "def parse_body_of_csv(csv_str,n_features):\n",
    "    # Read the entire CSV body into a single numeric matrix:\n",
    "    full_table = np.loadtxt(StringIO(csv_str),delimiter=',',skiprows=1);\n",
    "    \n",
    "    # Timestamp is the primary key for the records (examples):\n",
    "    timestamps = full_table[:,0].astype(int);\n",
    "    \n",
    "    # Read the sensor features:\n",
    "    X = full_table[:,1:(n_features+1)];\n",
    "    \n",
    "    # Read the binary label values, and the 'missing label' indicators:\n",
    "    trinary_labels_mat = full_table[:,(n_features+1):-1]; # This should have values of either 0., 1. or NaN\n",
    "    M = np.isnan(trinary_labels_mat); # M is the missing label matrix\n",
    "    \n",
    "    #print(\"M matrix shape:\",M.shape)\n",
    "    #print(\"Matrix: \",np.argwhere(M))\n",
    "    trinary_labels_mat[M]=-1 # Replace NaNs with -1.0 for which we then apply a mask\n",
    "    unique,counts=np.unique(trinary_labels_mat,return_counts=True)\n",
    "    print(*zip(unique,counts)) \n",
    "    \n",
    "#     Y = np.where(M,0,trinary_labels_mat) > 0.; # Y is the label matrix\n",
    "    \n",
    "    return (X,trinary_labels_mat,M,timestamps);\n",
    "\n",
    "def read_user_data(directory):\n",
    "    print('Reading {}'.format(directory.split(\"/\")[-1]))\n",
    "\n",
    "    # Read the entire csv file of the user:\n",
    "    with gzip.open(directory,'rb') as fid:\n",
    "        csv_str = fid.read();\n",
    "        csv_str = csv_str.decode(\"utf-8\")\n",
    "        pass;\n",
    "\n",
    "    (feature_names,label_names) = parse_header_of_csv(csv_str);\n",
    "    n_features = len(feature_names);\n",
    "    (X,Y,M,timestamps) = parse_body_of_csv(csv_str,n_features);\n",
    "\n",
    "    return (X,Y,M,timestamps,feature_names,label_names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get a summary of the sensor feature\n",
    "'''\n",
    "# Summarize features as we are only using phone_acc,phone_gyro,phone_mag,phone_loc,phone_audio,\n",
    "# phone_app,phone_battery,phone_use,phone_callstat,phone_wifi,phone_lf,phone_time\n",
    "# We are ignoring the use of the smartwatch features. There are definitely features that will be used\n",
    "# much more (e.g. than the phone_callstat) but we'll leave that up to the ML algorithm.\n",
    "'''\n",
    "def summarize_features(feature_list):\n",
    "    summary_feature_list=np.empty_like(feature_list)\n",
    "    for (ind,feature) in enumerate(feature_list):\n",
    "        if feature.startswith('raw_acc'):\n",
    "            summary_feature_list[ind]='phone_acc' \n",
    "        if feature.startswith('proc_gyro'):\n",
    "            summary_feature_list[ind]='phone_gyro'\n",
    "        if feature.startswith('raw_magnet'):\n",
    "            summary_feature_list[ind]='phone_mag'\n",
    "        if feature.startswith('watch_acc'):\n",
    "            summary_feature_list[ind]='watch_acc'\n",
    "        if feature.startswith('watch_heading'):\n",
    "            summary_feature_list[ind]='watch_dir'\n",
    "        if feature.startswith('location'):\n",
    "            summary_feature_list[ind]='phone_loc'\n",
    "        if feature.startswith('audio'):\n",
    "            summary_feature_list[ind]='phone_audio'\n",
    "        if feature.startswith('discrete:app_state'):\n",
    "            summary_feature_list[ind]='phone_app'\n",
    "        if feature.startswith('discrete:battery'):\n",
    "            summary_feature_list[ind]='phone_battery'\n",
    "        if feature.startswith('discrete:on'):\n",
    "            summary_feature_list[ind]='phone_use'\n",
    "        if feature.startswith('discrete:ringer'):\n",
    "            summary_feature_list[ind]='phone_callstat'\n",
    "        if feature.startswith('discrete:wifi'):\n",
    "            summary_feature_list[ind]='phone_wifi'\n",
    "        if feature.startswith('lf'):\n",
    "            summary_feature_list[ind]='phone_lf'\n",
    "        if feature.startswith('discrete:time'):\n",
    "            summary_feature_list[ind]='phone_time'\n",
    "\n",
    "    return summary_feature_list\n",
    "\n",
    "\n",
    "# Get a summary of the sensor feature along with the original label that was used\n",
    "def summarize_features_worig(feature_list):\n",
    "    summary_feature_list=np.empty((len(feature_list),2),dtype=object)\n",
    "    \n",
    "    for (ind,feature) in enumerate(feature_list):\n",
    "        if feature.startswith('raw_acc'):\n",
    "            summary_feature_list[ind,0]='phone_acc'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('proc_gyro'):\n",
    "            summary_feature_list[ind,0]='phone_gyro'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('raw_magnet'):\n",
    "            summary_feature_list[ind,0]='phone_mag'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('watch_acc'):\n",
    "            summary_feature_list[ind,0]='watch_acc'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('watch_heading'):\n",
    "            summary_feature_list[ind,0]='watch_dir'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('location'):\n",
    "            summary_feature_list[ind,0]='phone_loc'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('audio'):\n",
    "            summary_feature_list[ind,0]='phone_audio'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:app_state'):\n",
    "            summary_feature_list[ind,0]='phone_app'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:battery'):\n",
    "            summary_feature_list[ind,0]='phone_battery'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:on'):\n",
    "            summary_feature_list[ind,0]='phone_use'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:ringer'):\n",
    "            summary_feature_list[ind,0]='phone_callstat'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:wifi'):\n",
    "            summary_feature_list[ind,0]='phone_wifi'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('lf'):\n",
    "            summary_feature_list[ind,0]='phone_lf'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:time'):\n",
    "            summary_feature_list[ind,0]='phone_time'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "\n",
    "    return summary_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def choose_sensors(X_train,used_sensors,summarized_feature_names):\n",
    "    used_sensor_feature_names=np.zeros(len(summarized_feature_names),dtype=bool)\n",
    "    # Creates a zero boolean vector of all possible feature names\n",
    "    for s in used_sensors:\n",
    "        used_sensor_feature_names=np.logical_or(used_sensor_feature_names,(s==summarized_feature_names))\n",
    "    X_train=X_train[:,used_sensor_feature_names]\n",
    "    return X_train\n",
    "\n",
    "def choose_sensors_dropout(X_train,used_sensors,summarized_feature_names):\n",
    "    used_sensor_feature_names=np.zeros(len(summarized_feature_names),dtype=bool)\n",
    "    data_length=len(X_train)\n",
    "    \n",
    "    # Creates a zero boolean vector of all possible feature names\n",
    "    for s in used_sensors:\n",
    "        used_sensor_feature_names=np.logical_or(used_sensor_feature_names,(s==summarized_feature_names))\n",
    "    mask=np.tile(used_sensor_feature_names,(data_length,1))\n",
    "    \n",
    "    X_train=np.multiply(X_train,mask) # Element-wise matrix multiply\n",
    "    return X_train\n",
    "\n",
    "def choose_sensors_longnames(X_train,used_sensors,long_featurenames):\n",
    "    \n",
    "    used_sensor_feature_names=np.zeros(len(long_featurenames),dtype=bool)\n",
    "    used_feature_actualnames=np.zeros(len(long_featurenames),dtype=bool)\n",
    "    # Creates a zero boolean vector of all possible feature names\n",
    "    summary_features=long_featurenames[:,0]\n",
    "    all_complete_features=long_featurenames[:,-1]\n",
    "    \n",
    "    for s in used_sensors:\n",
    "        similar=(s==summary_features)\n",
    "        \n",
    "        #used_complete_features=(all_complete_features[similar.astype(int)])\n",
    "       \n",
    "        used_sensor_feature_names=np.logical_or(used_sensor_feature_names,similar)\n",
    "        used_feature_actualnames=np.logical_or(used_feature_actualnames,similar)\n",
    "    \n",
    "    X_train=X_train[:,used_sensor_feature_names]\n",
    "    long_names=all_complete_features[used_feature_actualnames]\n",
    "    return X_train,long_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor Types, Label Possibilities variables\n",
    "sensor_types=['phone_acc','phone_gyro','phone_mag','phone_loc','phone_audio',\n",
    "'phone_app','phone_battery','phone_use','phone_callstat','phone_wifi','phone_lf',\n",
    "'phone_time']\n",
    "label_possibilities=['LOC_home','OR_indoors','PHONE_ON_TABLE','SITTING','WITH_FRIENDS',\n",
    " 'LYING_DOWN','SLEEPING','WATCHING_TV','EATING','PHONE_IN_POCKET',\n",
    " 'TALKING','DRIVE_-_I_M_A_PASSENGER','OR_standing','IN_A_CAR',\n",
    " 'OR_exercise','AT_THE_GYM','SINGING','FIX_walking','OR_outside',\n",
    " 'SHOPPING','AT_SCHOOL','BATHING_-_SHOWER','DRESSING','DRINKING__ALCOHOL_',\n",
    " 'PHONE_IN_HAND','FIX_restaurant','IN_CLASS','PHONE_IN_BAG','IN_A_MEETING',\n",
    " 'TOILET','COOKING','ELEVATOR','FIX_running','BICYCLING','LAB_WORK',\n",
    " 'LOC_main_workplace','ON_A_BUS','DRIVE_-_I_M_THE_DRIVER','STROLLING',\n",
    " 'CLEANING','DOING_LAUNDRY','WASHING_DISHES','SURFING_THE_INTERNET',\n",
    " 'AT_A_PARTY','AT_A_BAR','LOC_beach','COMPUTER_WORK','GROOMING','STAIRS_-_GOING_UP',\n",
    " 'STAIRS_-_GOING_DOWN','WITH_CO-WORKERS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 3600D531-0C55-44A7-AE95-A7A38519464E.features_labels.csv.gz\n",
      "(-1.0, 148794) (0.0, 97289) (1.0, 19270)\n"
     ]
    }
   ],
   "source": [
    "# Reading sample data\n",
    "sample_loc='{}/{}.csv.gz'.format(prefix,user_sample)\n",
    "x_user,y_user,missedlabel_user,tstamp_user,featurename_user,labelname_user=read_user_data(sample_loc)\n",
    "feature_names=summarize_features_worig(featurename_user)\n",
    "x_user,feature_long_names=choose_sensors_longnames(x_user,sensor_types,feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# SMiLE (Semi-supervised multi-label classification using incomplete label information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from smile_master.SMILE.smile_ import SMiLE\n",
    "from pomegranate import NaiveBayes, NormalDistribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# smile = SMiLE(s=0.5, alpha=0.35, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_user=np.nan_to_num(x_user)\n",
    "# smile.fit(np.transpose(x_user),np.transpose(y_user))\n",
    "# predictions=smile.predict(np.transpose(x_user))\n",
    "# probabilities=smile.predict_proba(np.transpose(x_user))\n",
    "model = NaiveBayes.from_samples(NormalDistribution, x_user, y_user, verbose=True)\n",
    "# print(predictions)\n",
    "# print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unique,counts=np.unique(predictions,return_counts=True)\n",
    "print(*zip(unique,counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from semisup_learn.frameworks.SelfLearning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from sklearn import *\n",
    "\n",
    "for ind in range(y_user.shape[-1]):\n",
    "    unique_col,counts_col=np.unique(y_user[:,ind],return_counts=True)\n",
    "    \n",
    "    skip_col=0\n",
    "    for i in range(len(unique_col)):\n",
    "        if(unique_col[i]==-1):\n",
    "            if counts_col[i]==len(y_user[:,ind]):\n",
    "                skip_col=1\n",
    "                print(\"Skipping column {}\".format(ind))\n",
    "    if(len(unique_col)==2): # If there are only 2 unique labels, and one of them is -1...don't have enough classes for SVM\n",
    "        if -1 in unique_col:\n",
    "            skip_col=1\n",
    "            print(\"Skipping column {} because 1-class\".format(ind))\n",
    "    if(skip_col!=1):\n",
    "    \n",
    "\n",
    "        print(ind,\" of \",y_user.shape[-1],\" :: \")\n",
    "        x_user=np.nan_to_num(x_user)\n",
    "        y_col=y_user[:,ind].reshape(len(y_user[:,ind]),1)\n",
    "#         smile = SMiLE(s=0.5, alpha=0.35, k=51)\n",
    "#         smile.fit(np.transpose(x_user), np.transpose(y_col))\n",
    "#         predictions=smile.predict(np.transpose(x_user))\n",
    "#         probabilities=smile.predict_proba(np.transpose(x_user))\n",
    "\n",
    "        ssmodel = SelfLearning.CPLELearningModel(sklearn.svm.SVC(kernel=\"rbf\", probability=True), predict_from_probabilities=True) # RBF SVM\n",
    "        ssmodel.fit(x_user,y_col)\n",
    "        ssmodel.predict(X, ys)\n",
    "#         print(predictions)\n",
    "#         print(probabilities)\n",
    "#         unique,counts=np.unique(predictions,return_counts=True)\n",
    "#         print(*zip(unique,counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ssmodel = CPLELearningModel(basemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# y_user[:,ind]\n",
    "\n",
    "unique,counts=np.unique(y_user[:,ind],return_counts=True)\n",
    "print(*zip(unique,counts)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Raw SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy.ma as ma # Masked array\n",
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def s3vm(x,y):\n",
    "    y_out=copy.deepcopy(y)\n",
    "    for ind in range(y.shape[-1]):\n",
    "        print(\"Col \",ind)\n",
    "        unique_col,counts_col=np.unique(y[:,ind],return_counts=True)\n",
    "        print(*zip(unique_col,counts_col))\n",
    "        \n",
    "        skip_col=0\n",
    "        for i in range(len(unique_col)):\n",
    "            if(unique_col[i]==-1):\n",
    "                if counts_col[i]==len(y[:,ind]):\n",
    "                    skip_col=1\n",
    "                    print(\"Skipping column {}\".format(ind))\n",
    "        if(len(unique_col)==2): # If there are only 2 unique labels, and one of them is -1...don't have enough classes for SVM\n",
    "            if -1 in unique_col:\n",
    "                skip_col=1\n",
    "                print(\"Skipping column {} because 1-class\".format(ind))\n",
    "        if(skip_col!=1):\n",
    "            print(ind,\" of \",y_user.shape[-1],\" :: \")\n",
    "            y_col=y[:,ind]\n",
    "            y_train_masked=ma.masked_where(y_col!=-1,y_col).mask\n",
    "            indices=np.where(y_train_masked==False)[0]\n",
    "\n",
    "            svm_classifier=svm.SVC(probability=True,class_weight='balanced')\n",
    "            print(x[y_train_masked].shape,y_col[y_train_masked].shape)\n",
    "            svm_classifier.fit(x[y_train_masked],y_col[y_train_masked])\n",
    "            for index in indices:\n",
    "                prediction=svm_classifier.predict(x[index].reshape(1, -1))\n",
    "                y_out[index,ind]=prediction\n",
    "            \n",
    "                \n",
    "    unique,counts=np.unique(y,return_counts=True)\n",
    "    unique_after,counts_after=np.unique(y_out,return_counts=True)\n",
    "    print(*zip(unique,counts),\">>>>>\",*zip(unique_after,counts_after))\n",
    "    return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for g in glob.glob('dataset/Extrasensory_uuid_fl_uTAR/*'):\n",
    "    print(g)\n",
    "    fname=g.split('/')[-1].split('.')[0]\n",
    "    (x_user_train,y_user_train,missed_label_user,tstamp_user,featurename_user,labelname_user) = read_user_data(g)\n",
    "    x_user_train=np.nan_to_num(x_user_train)\n",
    "    feature_names=summarize_features_worig(featurename_user)\n",
    "    x_user,feature_long_names=choose_sensors_longnames(x_user_train,sensor_types,feature_names)\n",
    "    y_out=s3vm(x_user,y_user_train)\n",
    "    \n",
    "    with open('dataset/semisupervised_users/{}_ssl_label.pkl'.format(fname),'wb') as f:\n",
    "            pickle.dump(y_out,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1-NearestNeighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy.ma as ma # Masked array\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Returns a standardized (0 mean, 1 variance) dataset\n",
    "def standardize(X_train):\n",
    "    mean=np.nanmean(X_train,axis=0).reshape((1,-1))# Ignores NaNs while finding the mean across rows\n",
    "    standard_dev=np.nanstd(X_train,axis=0) # Ignores NaNs while finding the standard deviation across rows\n",
    "    standard_dev_nonzero=np.where(standard_dev>0,standard_dev,1.).reshape((1,-1)) # Div zero\n",
    "    \n",
    "    X=(X_train-mean)/standard_dev_nonzero\n",
    "    return X,mean,standard_dev_nonzero   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def nearestneighbor(x,y):\n",
    "    imputed_y=np.empty_like(y)\n",
    "    for ind in range(y.shape[-1]):\n",
    "        print(\"Current column: \",ind)\n",
    "        y_col=y[:,ind]\n",
    "        \n",
    "        unique_col,counts_col=np.unique(y_col,return_counts=True)\n",
    "        print(*zip(unique_col,counts_col))\n",
    "        \n",
    "        skip_col=0\n",
    "        for i in range(len(unique_col)):\n",
    "            if(unique_col[i]==-1):\n",
    "                if counts_col[i]==len(y[:,ind]):\n",
    "                    skip_col=1\n",
    "                    print(\"Skipping column {}\".format(ind))\n",
    "        \n",
    "        if (skip_col!=1):\n",
    "            y_train_masked=ma.masked_where(y_col!=-1,y_col).mask\n",
    "            missing_indices=np.where(y_train_masked==False)[0]\n",
    "\n",
    "\n",
    "            x_train=x[y_train_masked]\n",
    "            y_col_train=y_col[y_train_masked]\n",
    "\n",
    "            knn_classifier=KNN(n_neighbors=1,weights='distance',p=2,metric='minkowski',n_jobs=-1)\n",
    "            knn_classifier.fit(x_train,y_col_train)\n",
    "\n",
    "            for i,index in enumerate(missing_indices):\n",
    "                print(\"\\t\\tWorking on index {} of {}\".format(i,len(missing_indices)))\n",
    "                predict=knn_classifier.predict(x[index,:].reshape(1, -1))\n",
    "                print(predict)\n",
    "                #print(\"\\t\\t\",knn_classifier.predict_proba(x_user[index,:].reshape(1, -1)))\n",
    "                y_col[index]=predict\n",
    "                y_train_masked=ma.masked_where(y_col!=-1,y_col).mask\n",
    "\n",
    "                knn_classifier.fit(x[y_train_masked],y_col[y_train_masked])\n",
    "                print(\"\\t\\tMissing length is now :\",len(np.where(y_train_masked==False)[0]))\n",
    "            imputed_y[:,ind]=y_col\n",
    "    unique,counts=np.unique(imputed_y,return_counts=True)\n",
    "    print(*zip(unique,counts))\n",
    "    \n",
    "    return imputed_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for g in glob.glob('dataset/Extrasensory_uuid_fl_uTAR/*'):\n",
    "    print(g)\n",
    "    fname=g.split('/')[-1].split('.')[0]\n",
    "    (x_user_train,y_user_train,missed_label_user,tstamp_user,featurename_user,labelname_user) = read_user_data(g)\n",
    "    x_user_train,_,_=standardize(x_user_train)\n",
    "    x_user_train=np.nan_to_num(x_user_train)\n",
    "    \n",
    "    feature_names=summarize_features_worig(featurename_user)\n",
    "    x_user,feature_long_names=choose_sensors_longnames(x_user_train,sensor_types,feature_names)\n",
    "    \n",
    "    y_out=nearestneighbor(x_user,y_user_train)\n",
    "    \n",
    "    with open('dataset/semisupervised_users_1NN/{}_ssl_label.pkl'.format(fname),'wb') as f:\n",
    "            pickle.dump(y_out,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,balanced_accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support,classification_report\n",
    "#from sklearn.metrics import multilabel_confusion_matrix # Only available in dev .21\n",
    "\n",
    "# Need Pytorch for multilabel classifications\n",
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "#import skorch [Scikit-learn wrapper around Pytorch so allowing for K-fold cross-validation]\n",
    "random_state=10\n",
    "np.random.seed(random_state)\n",
    "\n",
    "from utils import cm,remove_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Simple function to run using GPU when available\n",
    "def C(structure):\n",
    "    if torch.cuda.is_available():\n",
    "        device=torch.device(\"cuda\")\n",
    "        return structure.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create a mask to hide -1 nans before training and then input to a train criterion\n",
    "def mask(criterion,y_true,y_pred,mask_value=-1.):\n",
    "    mask=torch.ne(y_true,mask_value).type(torch.cuda.FloatTensor)\n",
    "    # Cast the ByteTensor from elementwise comparison to a FloatTensor\n",
    "    return criterion(torch.mul(y_pred,mask),torch.mul(y_true,mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Creating an instance weight matrix for the training labels\n",
    "def instance_weight_matrix(y_train):\n",
    "    instance_weights=np.zeros_like(y_train)\n",
    "    for l in range(len(labelname_user)):\n",
    "        temp_column=y_train[:,l]\n",
    "        count_neg=0\n",
    "        count_0=0\n",
    "        count_1=0\n",
    "        for i in range(len(temp_column)): # n^2 bincount doesn't work with arrays consisting of negative numbers\n",
    "            if (temp_column[i]==-1):\n",
    "                count_neg+=1\n",
    "            elif (temp_column[i]==0):\n",
    "                count_0+=1\n",
    "            elif (temp_column[i]==1):\n",
    "                count_1+=1\n",
    "            else:\n",
    "                raise ValueError(\"Bad Loop\")\n",
    "#         print(l,count_0,count_1)\n",
    "        if(count_0!=0):\n",
    "            weight_0=float((count_0+count_1)/count_0)\n",
    "        else:\n",
    "            weight_0=0.\n",
    "        if(count_1!=0):\n",
    "            weight_1=float((count_0+count_1)/count_1)\n",
    "        else:\n",
    "            weight_1=0.\n",
    "        if(weight_0+weight_1==0.):\n",
    "            weight_0=1.\n",
    "            weight_1=1.\n",
    "        else:\n",
    "            weight_0=weight_0/(weight_0+weight_1)\n",
    "            weight_1=weight_1/(weight_0+weight_1)\n",
    "\n",
    "        for i in range(len(temp_column)):\n",
    "            if (temp_column[i]==-1):\n",
    "                instance_weights[i,l]=0.\n",
    "            elif (temp_column[i]==0):\n",
    "                instance_weights[i,l]=weight_0\n",
    "            elif (temp_column[i]==1):\n",
    "                instance_weights[i,l]=weight_1\n",
    "    return instance_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Returns a standardized (0 mean, 1 variance) dataset\n",
    "def standardize(X_train):\n",
    "    mean=np.nanmean(X_train,axis=0).reshape((1,-1))# Ignores NaNs while finding the mean across rows\n",
    "    standard_dev=np.nanstd(X_train,axis=0) # Ignores NaNs while finding the standard deviation across rows\n",
    "    standard_dev_nonzero=np.where(standard_dev>0,standard_dev,1.).reshape((1,-1)) # Div zero\n",
    "    \n",
    "    X=(X_train-mean)/standard_dev_nonzero\n",
    "    return X,mean,standard_dev_nonzero   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Linear decreasing LR scheduler\n",
    "def linear_lr_scheduler(optimizer,epoch):\n",
    "    \"\"\"\n",
    "    LR_init=0.1, LR_final=0.01, n_epochs=40\n",
    "    Reset to n_epochs=80 (0,0.1) (79,0.01)\n",
    "    Sets the learning rate to the initial LR decayed by 1.04 every epoch\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr=param_group['lr']\n",
    "    m=-9/7900;#-3/1300\n",
    "    c=0.1\n",
    "    lr=(epoch*m)+c # Linear LR decay based on a set number of epochs\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr']=lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Adds euclidean regularization to weight matrices\n",
    "def frobenius_norm(model,loss):\n",
    "    regularizer_loss=0\n",
    "    \n",
    "    for m in model.modules():\n",
    "        if isinstance(m,nn.Linear): # Linear layer\n",
    "            frobenius_norm=torch.norm(m.weight,p='fro')\n",
    "            regularizer_loss+=frobenius_norm # Regularization over the weight matrices for linear layers\n",
    "    return loss+0.001*regularizer_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Function for the required accuracy metrics per fold\n",
    "def accuracy(fold,target_labels,y_true,y_pred):\n",
    "    y_true=y_true.detach().numpy()\n",
    "    y_pred=y_pred.detach().numpy()\n",
    "    balanced_accuracy_dict={}\n",
    "    print('*'*20)\n",
    "    print('For fold {}'.format(fold))\n",
    "    \n",
    "    # Balanced accuracy\n",
    "    for i in range(len(target_labels)):\n",
    "        true_perlabel=y_true[:,i]\n",
    "        pred_perlabel=y_pred[:,i]\n",
    "        initial_shape=true_perlabel.shape\n",
    "        \n",
    "        invalid_mask=np.where(true_perlabel==-1.)\n",
    "        valid_mask=np.where(true_perlabel!=-1.) # Create a mask\n",
    "        true_perlabel=true_perlabel[valid_mask]\n",
    "        pred_perlabel=pred_perlabel[valid_mask]\n",
    "        \n",
    "        bal_acc=balanced_accuracy_score(y_true=true_perlabel,y_pred=pred_perlabel)\n",
    "#         print('\\t Label {}:::-> Balanced Accuracy {}'.format(target_labels[i],round(bal_acc,7)))\n",
    "#         print('\\t\\t Initial length {}, Missing mask length {}, Valid mask length {}, Final length {}'\n",
    "#               .format(initial_shape[0],len(invalid_mask[0]),len(valid_mask[0]),len(true_perlabel)))\n",
    "        balanced_accuracy_dict[target_labels[i]]=round(bal_acc,5)\n",
    "    return balanced_accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# CPU/detach inference\n",
    "def I(tensor):\n",
    "    return tensor.cpu().detach().numpy() # Run on CPU, detach from variable in graph, convert to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Train function w/BCE loss, linear LR scheduler, instance weights\n",
    "def train(model,X,Y,X_test,Y_test,weights,n_epoch,batch_size,lr_init,momentum,fold):\n",
    "    \n",
    "    optimizer=optim.SGD(model.parameters(),lr=lr_init,momentum=momentum)\n",
    "\n",
    "    X=V(torch.cuda.FloatTensor(X),requires_grad=True)\n",
    "    Y=V(torch.cuda.FloatTensor(Y),requires_grad=False)\n",
    "    X_test=V(torch.cuda.FloatTensor(X_test),requires_grad=False)\n",
    "    Y_test=V(torch.cuda.FloatTensor(Y_test),requires_grad=False)\n",
    "    weights=V(torch.cuda.FloatTensor(weights),requires_grad=False)\n",
    "   \n",
    "    # Cuda-Compatible Model\n",
    "    model = C(model)\n",
    "    # Create dataloaders\n",
    "    # Dataloader creation\n",
    "    # Wrap weights for instance weight tensor along with data & label tensors s.t.\n",
    "    # it can be called properly as a dataloader in batches.\n",
    "    train_dataset=utils.TensorDataset(X,Y,weights)\n",
    "    train_loader=utils.DataLoader(dataset=train_dataset,batch_size=bs\n",
    "                                  ,shuffle=False,drop_last=False)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        linear_lr_scheduler(optimizer,epoch)\n",
    "        for i,data in enumerate(train_loader,0):\n",
    "\n",
    "            inputs,labels,weights=data\n",
    "            inputs=V(torch.cuda.FloatTensor(inputs),requires_grad=True)\n",
    "            labels=V(torch.cuda.FloatTensor(labels),requires_grad=False)\n",
    "            weights=V(torch.cuda.FloatTensor(weights),requires_grad=False)\n",
    "\n",
    "            criterion=C(nn.BCEWithLogitsLoss(weight=weights))\n",
    "            optimizer.zero_grad()   \n",
    "            sum_total=0\n",
    "\n",
    "            outputs=model(inputs)\n",
    "\n",
    "            # Zero gradients, backward pass, weight update\n",
    "#             loss=criterion(outputs,labels) \n",
    "            loss=mask(criterion=criterion,y_true=labels,y_pred=outputs,mask_value=-1)\n",
    "            loss=frobenius_norm(model,loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_total+=loss.item()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                epoch_lr=param_group['lr']\n",
    "\n",
    "            print(\"Epoch {}::Minibatch {}::LR {} --> Loss {}\".format(epoch+1,i+1,epoch_lr,sum_total/bs))\n",
    "            sum_total=0.\n",
    "        \n",
    "    \n",
    "    print(\"Training finished, Prediction\")\n",
    "    \n",
    "    model.eval() # Evaluation model\n",
    "    \n",
    "    y_pred=torch.sigmoid(model(X))>=0.5\n",
    "    fold_train_dict=accuracy(fold,labelname_user,Y.cpu(),y_pred.cpu())\n",
    "    \n",
    "    Y_test_pred=torch.sigmoid(model(X_test))>=0.5\n",
    "    fold_test_dict=accuracy(fold,labelname_user,Y_test.cpu(),Y_test_pred.cpu())\n",
    "    \n",
    "#     cm(I(Y),I(y_pred),np.asarray(labelname_user),fname+'train.png')\n",
    "#     cm(I(Y_test),I(Y_test_pred),np.asarray(labelname_user),fname+'test.png')\n",
    "    model.train() # Back to train model\n",
    "    \n",
    "#     return fold_train_dict,fold_test_dict,I(Y),I(y_pred),I(Y_test),I(Y_test_pred)\n",
    "    return fold_train_dict,fold_test_dict,I(y_pred),I(Y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Train function w/BCE loss, linear LR scheduler, instance weights\n",
    "def train_custom(model,X,Y,X_test,Y_test,weights,n_epoch,batch_size,lr_init,momentum,fold):\n",
    "    \n",
    "    optimizer=optim.Adam(model.parameters())\n",
    "\n",
    "    X=V(torch.cuda.FloatTensor(X),requires_grad=True)\n",
    "    Y=V(torch.cuda.FloatTensor(Y),requires_grad=False)\n",
    "    X_test=V(torch.cuda.FloatTensor(X_test),requires_grad=False)\n",
    "    Y_test=V(torch.cuda.FloatTensor(Y_test),requires_grad=False)\n",
    "    weights=V(torch.cuda.FloatTensor(weights),requires_grad=False)\n",
    "   \n",
    "    # Cuda-Compatible Model\n",
    "    model = C(model)\n",
    "    # Create dataloaders\n",
    "    # Dataloader creation\n",
    "    # Wrap weights for instance weight tensor along with data & label tensors s.t.\n",
    "    # it can be called properly as a dataloader in batches.\n",
    "    train_dataset=utils.TensorDataset(X,Y,weights)\n",
    "    train_loader=utils.DataLoader(dataset=train_dataset,batch_size=bs\n",
    "                                  ,shuffle=False,drop_last=False)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        for i,data in enumerate(train_loader,0):\n",
    "\n",
    "            inputs,labels,weights=data\n",
    "            inputs=V(torch.cuda.FloatTensor(inputs),requires_grad=True)\n",
    "            labels=V(torch.cuda.FloatTensor(labels),requires_grad=False)\n",
    "            weights=V(torch.cuda.FloatTensor(weights),requires_grad=False)\n",
    "\n",
    "            criterion=C(nn.BCEWithLogitsLoss(weight=weights))\n",
    "            optimizer.zero_grad()   \n",
    "            sum_total=0\n",
    "\n",
    "            outputs=model(inputs)\n",
    "\n",
    "            # Zero gradients, backward pass, weight update\n",
    "#             loss=criterion(outputs,labels) \n",
    "            loss=mask(criterion=criterion,y_true=labels,y_pred=outputs,mask_value=-1)\n",
    "            loss=frobenius_norm(model,loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_total+=loss.item()\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 epoch_lr=param_group['lr']\n",
    "\n",
    "            print(\"Epoch {}::Minibatch {}--> Loss {}\".format(epoch+1,i+1,sum_total/bs))\n",
    "            sum_total=0.\n",
    "        \n",
    "    \n",
    "    print(\"Training finished, Prediction\")\n",
    "    \n",
    "    model.eval() # Evaluation model\n",
    "    \n",
    "    y_pred=torch.sigmoid(model(X))>=0.5\n",
    "    fold_train_dict=accuracy(fold,labelname_user,Y.cpu(),y_pred.cpu())\n",
    "    \n",
    "    Y_test_pred=torch.sigmoid(model(X_test))>=0.5\n",
    "    fold_test_dict=accuracy(fold,labelname_user,Y_test.cpu(),Y_test_pred.cpu())\n",
    "    \n",
    "#     cm(I(Y),I(y_pred),np.asarray(labelname_user),fname+'train.png')\n",
    "#     cm(I(Y_test),I(Y_test_pred),np.asarray(labelname_user),fname+'test.png')\n",
    "    model.train() # Back to train model\n",
    "    \n",
    "#     return fold_train_dict,fold_test_dict,I(Y),I(y_pred),I(Y_test),I(Y_test_pred)\n",
    "    return fold_train_dict,fold_test_dict,I(y_pred),I(Y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Randomly choose sensors (features) and replace those by 0 (dropout-ish)\n",
    "def random_choice(sensor_list,feature_names,x_dataset):\n",
    "    '''\n",
    "    Sensor list if of the form:\n",
    "    sensor_list=['phone_acc','phone_gyro','phone_mag','phone_loc','phone_audio',\n",
    "'phone_app','phone_battery','phone_use','phone_callstat','phone_wifi','phone_lf',\n",
    "'phone_time']\n",
    "    '''\n",
    "    sensor_length=len(sensor_list)\n",
    "    chosen_sensors=np.random.choice(sensor_list,math.floor(sensor_length*0.8),replace=False)\n",
    "    ignored_sensors=list(set(sensor_list)-set(chosen_sensors)) \n",
    "    print(\"\\t\\t\\tIgnoring {}\".format(ignored_sensors))\n",
    "    new_summary_features=summarize_features(feature_long_names)\n",
    "    x_dataset=choose_sensors_dropout(x_dataset,chosen_sensors,new_summary_features)\n",
    "    \n",
    "    return x_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Train function w/BCE loss, linear LR scheduler, instance weights\n",
    "def train_sensordropout(model,X,Y,X_test,Y_test,weights,sensor_list,feature_names,n_epoch,batch_size,lr_init,momentum,fold):\n",
    "    \n",
    "    optimizer=optim.SGD(model.parameters(),lr=lr_init,momentum=momentum)\n",
    "\n",
    "    X=V(torch.cuda.FloatTensor(X),requires_grad=True)\n",
    "    Y=V(torch.cuda.FloatTensor(Y),requires_grad=False)\n",
    "    X_test=V(torch.cuda.FloatTensor(X_test),requires_grad=False)\n",
    "    Y_test=V(torch.cuda.FloatTensor(Y_test),requires_grad=False)\n",
    "    weights=V(torch.cuda.FloatTensor(weights),requires_grad=False)\n",
    "   \n",
    "    # Cuda-Compatible Model\n",
    "    model = C(model)\n",
    "    # Create dataloaders\n",
    "    # Dataloader creation\n",
    "    # Wrap weights for instance weight tensor along with data & label tensors s.t.\n",
    "    # it can be called properly as a dataloader in batches.\n",
    "    train_dataset=utils.TensorDataset(X,Y,weights)\n",
    "    train_loader=utils.DataLoader(dataset=train_dataset,batch_size=bs\n",
    "                                  ,shuffle=False,drop_last=False)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        linear_lr_scheduler(optimizer,epoch)\n",
    "        for i,data in enumerate(train_loader,0):\n",
    "\n",
    "            inputs,labels,weights=data\n",
    "            inputs_detached=inputs.cpu().detach().numpy()\n",
    "            \n",
    "            inputs=random_choice(sensor_list,feature_names,inputs_detached) # Sensor Dropout\n",
    "            \n",
    "            inputs=V(torch.cuda.FloatTensor(inputs),requires_grad=True)\n",
    "            labels=V(torch.cuda.FloatTensor(labels),requires_grad=False)\n",
    "            weights=V(torch.cuda.FloatTensor(weights),requires_grad=False)\n",
    "\n",
    "            criterion=C(nn.BCEWithLogitsLoss(weight=weights))\n",
    "            optimizer.zero_grad()   \n",
    "            sum_total=0\n",
    "\n",
    "            outputs=model(inputs)\n",
    "\n",
    "            # Zero gradients, backward pass, weight update\n",
    "#             loss=criterion(outputs,labels) \n",
    "            loss=mask(criterion=criterion,y_true=labels,y_pred=outputs,mask_value=-1)\n",
    "            loss=frobenius_norm(model,loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_total+=loss.item()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                epoch_lr=param_group['lr']\n",
    "\n",
    "            print(\"Epoch {}::Minibatch {}::LR {} --> Loss {}\".format(epoch+1,i+1,epoch_lr,sum_total/bs))\n",
    "            sum_total=0.\n",
    "        \n",
    "    \n",
    "    print(\"Training finished, Prediction\")\n",
    "    \n",
    "    model.eval() # Evaluation model\n",
    "    \n",
    "    y_pred=torch.sigmoid(model(X))>=0.5\n",
    "    fold_train_dict=accuracy(fold,labelname_user,Y.cpu(),y_pred.cpu())\n",
    "    \n",
    "    Y_test_pred=torch.sigmoid(model(X_test))>=0.5\n",
    "    fold_test_dict=accuracy(fold,labelname_user,Y_test.cpu(),Y_test_pred.cpu())\n",
    "    \n",
    "    model.train() # Back to train model\n",
    "    \n",
    "    return fold_train_dict,fold_test_dict,I(y_pred),I(Y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size 170, Output Size 51\n"
     ]
    }
   ],
   "source": [
    "# Defining sizes for neural networks and other global hyperparameters\n",
    "# input_size=x_train[0].shape[-1]\n",
    "input_size=170 #176 #464 #506 # 170\n",
    "hidden_size=16\n",
    "# output_size=y_train[0].shape[-1]\n",
    "output_size=51#51\n",
    "n_epoch=80\n",
    "bs=300\n",
    "lr_init=0.1\n",
    "momentum=0.5\n",
    "print('Input Size {}, Output Size {}'.format(input_size,output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearMLP(\n",
      "  (fc1): Linear(in_features=170, out_features=51, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Linear MLP no hidden layer\n",
    "class LinearMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearMLP,self).__init__()\n",
    "        self.fc1=nn.Linear(input_size,output_size)\n",
    "    def forward(self,x):\n",
    "        x=self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "model=LinearMLP()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in glob.glob('dataset/Extrasensory_uuid_fl_uTAR/*'):\n",
    "    print(\"\\n\",g)\n",
    "    fname=g.split('/')[-1].split('.')[0]\n",
    "    (x_user_train,y_user_train,missed_label_user,tstamp_user,featurename_user,labelname_user) = read_user_data(g)\n",
    "    x_user_train,_,_=standardize(x_user_train)\n",
    "    x_user_train=np.nan_to_num(x_user_train)\n",
    "    \n",
    "    feature_names=summarize_features_worig(featurename_user)\n",
    "    x_user,feature_long_names=choose_sensors_longnames(x_user_train,sensor_types,feature_names)\n",
    "    \n",
    "    weights=instance_weight_matrix(y_user_train)\n",
    "    print(\"\\t Loaded datasets\")\n",
    "    \n",
    "    model=LinearMLP() # Creating a new instance of the model every fold\n",
    "    \n",
    "    mlp0H_train_dict,mlp0H_test_dict,y_train_pred,y_test_pred=train(model,\n",
    "                                                           X=x_user,\n",
    "                                                           Y=y_user_train,\n",
    "                                                           X_test=x_user,\n",
    "                                                           Y_test=y_user_train,\n",
    "                                                           weights=weights,\n",
    "                                                           n_epoch=n_epoch,\n",
    "                                                           batch_size=bs,\n",
    "                                                           lr_init=lr_init,\n",
    "                                                           momentum=momentum,\n",
    "                                                           fold=-1)\n",
    "    print(mlp0H_test_dict)\n",
    "    print(\"Y_train_pred==Y_test_pred:\",np.array_equal(y_train_pred,y_test_pred))\n",
    "    \n",
    "    y_user_impute=copy.deepcopy(y_user_train)\n",
    "    ind_x,ind_y=np.where(y_user_train)\n",
    "    \n",
    "    for z in zip(ind_x,ind_y):\n",
    "        y_user_impute[z[0],z[1]]=y_test_pred[z[0],z[1]]\n",
    "    \n",
    "    impute_unique,impute_counts=np.unique(y_user_impute,return_counts=True)\n",
    "    print(*zip(impute_unique,impute_counts))\n",
    "    \n",
    "    with open('dataset/semisupervised_users_0H/{}_ssl_0H.pkl'.format(fname),'wb') as f:\n",
    "            pickle.dump(y_user_impute,f) #train_pred==test_pred\n",
    "    \n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size 170, Output Size 51\n"
     ]
    }
   ],
   "source": [
    "# Defining sizes for neural networks and other global hyperparameters\n",
    "# input_size=x_train[0].shape[-1]\n",
    "input_size=170 #176 #464 #506 # 170\n",
    "hidden_size=16\n",
    "# output_size=y_train[0].shape[-1]\n",
    "output_size=51#51\n",
    "n_epoch=80\n",
    "bs=300\n",
    "lr_init=0.1\n",
    "momentum=0.5\n",
    "print('Input Size {}, Output Size {}'.format(input_size,output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_1H(\n",
      "  (hidden0): Sequential(\n",
      "    (0): Linear(in_features=170, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=51, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Linear MLP 1 hidden layer\n",
    "class MLP_1H(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_1H,self).__init__()\n",
    "        self.hidden0=nn.Sequential(\n",
    "            nn.Linear(input_size,hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "        self.out=nn.Sequential(\n",
    "            nn.Linear(hidden_size,output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.hidden0(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "# Train for MLP-1 Hidden\n",
    "model=MLP_1H()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in glob.glob('dataset/Extrasensory_uuid_fl_uTAR/*'):\n",
    "    print(\"\\n\",g)\n",
    "    fname=g.split('/')[-1].split('.')[0]\n",
    "    (x_user_train,y_user_train,missed_label_user,tstamp_user,featurename_user,labelname_user) = read_user_data(g)\n",
    "    x_user_train,_,_=standardize(x_user_train)\n",
    "    x_user_train=np.nan_to_num(x_user_train)\n",
    "    \n",
    "    feature_names=summarize_features_worig(featurename_user)\n",
    "    x_user,feature_long_names=choose_sensors_longnames(x_user_train,sensor_types,feature_names)\n",
    "    \n",
    "    weights=instance_weight_matrix(y_user_train)\n",
    "    print(\"\\t Loaded datasets\")\n",
    "    \n",
    "    model=MLP_1H() # Creating a new instance of the model every fold\n",
    "    \n",
    "    mlp1H_train_dict,mlp1H_test_dict,y_train_pred,y_test_pred=train(model,\n",
    "                                                           X=x_user,\n",
    "                                                           Y=y_user_train,\n",
    "                                                           X_test=x_user,\n",
    "                                                           Y_test=y_user_train,\n",
    "                                                           weights=weights,\n",
    "                                                           n_epoch=n_epoch,\n",
    "                                                           batch_size=bs,\n",
    "                                                           lr_init=lr_init,\n",
    "                                                           momentum=momentum,\n",
    "                                                           fold=-1)\n",
    "    countlabels_user=np.sum(y_user_train,axis=0) # Column summary\n",
    "    labelname_countlabel_user=zip(labelname_user,countlabels_user) # Zip together names, counts\n",
    "    labelname_countlabel_user=sorted(labelname_countlabel_user,key=lambda row:row[-1],reverse=True)\n",
    "\n",
    "    print('Activities and counts:')\n",
    "    print(labelname_countlabel_user)\n",
    "    \n",
    "    print(mlp1H_test_dict)\n",
    "    \n",
    "    print(\"Y_train_pred==Y_test_pred:\",np.array_equal(y_train_pred,y_test_pred))\n",
    "    \n",
    "    y_user_impute=copy.deepcopy(y_user_train)\n",
    "    ind_x,ind_y=np.where(y_user_train)\n",
    "    \n",
    "    for z in zip(ind_x,ind_y):\n",
    "        y_user_impute[z[0],z[1]]=y_test_pred[z[0],z[1]]\n",
    "    \n",
    "    impute_unique,impute_counts=np.unique(y_user_impute,return_counts=True)\n",
    "    print(*zip(impute_unique,impute_counts))\n",
    "    \n",
    "    with open('dataset/semisupervised_users_1H/{}_ssl_1H.pkl'.format(fname),'wb') as f:\n",
    "            pickle.dump(y_user_impute,f) #train_pred==test_pred\n",
    "    \n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size 170, Output Size 51\n"
     ]
    }
   ],
   "source": [
    "# Defining sizes for neural networks and other global hyperparameters\n",
    "# input_size=x_train[0].shape[-1]\n",
    "input_size=170 #176 #464 #506 # 170\n",
    "hidden_size=16\n",
    "# output_size=y_train[0].shape[-1]\n",
    "output_size=51#51\n",
    "n_epoch=80\n",
    "bs=300\n",
    "lr_init=0.1\n",
    "momentum=0.5\n",
    "print('Input Size {}, Output Size {}'.format(input_size,output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_2H(\n",
      "  (hidden0): Sequential(\n",
      "    (0): Linear(in_features=170, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (hidden1): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=51, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP_2H(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_2H,self).__init__()\n",
    "        self.hidden0=nn.Sequential(\n",
    "            nn.Linear(input_size,hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "        self.hidden1=nn.Sequential(\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "        self.out=nn.Sequential(\n",
    "            nn.Linear(hidden_size,output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "# Train for MLP-2 Hidden\n",
    "model=MLP_2H()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in glob.glob('dataset/Extrasensory_uuid_fl_uTAR/*'):\n",
    "    print(\"\\n\",g)\n",
    "    fname=g.split('/')[-1].split('.')[0]\n",
    "    (x_user_train,y_user_train,missed_label_user,tstamp_user,featurename_user,labelname_user) = read_user_data(g)\n",
    "    x_user_train,_,_=standardize(x_user_train)\n",
    "    x_user_train=np.nan_to_num(x_user_train)\n",
    "    \n",
    "    feature_names=summarize_features_worig(featurename_user)\n",
    "    x_user,feature_long_names=choose_sensors_longnames(x_user_train,sensor_types,feature_names)\n",
    "    \n",
    "    weights=instance_weight_matrix(y_user_train)\n",
    "    print(\"\\t Loaded datasets\")\n",
    "    \n",
    "    model=MLP_2H() # Creating a new instance of the model every fold\n",
    "    \n",
    "    mlp2H_train_dict,mlp2H_test_dict,y_train_pred,y_test_pred=train(model,\n",
    "                                                           X=x_user,\n",
    "                                                           Y=y_user_train,\n",
    "                                                           X_test=x_user,\n",
    "                                                           Y_test=y_user_train,\n",
    "                                                           weights=weights,\n",
    "                                                           n_epoch=n_epoch,\n",
    "                                                           batch_size=bs,\n",
    "                                                           lr_init=lr_init,\n",
    "                                                           momentum=momentum,\n",
    "                                                           fold=-1)\n",
    "    countlabels_user=np.sum(y_user_train,axis=0) # Column summary\n",
    "    labelname_countlabel_user=zip(labelname_user,countlabels_user) # Zip together names, counts\n",
    "    labelname_countlabel_user=sorted(labelname_countlabel_user,key=lambda row:row[-1],reverse=True)\n",
    "\n",
    "    print('Activities and counts:')\n",
    "    print(labelname_countlabel_user)\n",
    "    \n",
    "    print(mlp2H_test_dict)\n",
    "    \n",
    "    print(\"Y_train_pred==Y_test_pred:\",np.array_equal(y_train_pred,y_test_pred))\n",
    "    \n",
    "    y_user_impute=copy.deepcopy(y_user_train)\n",
    "    ind_x,ind_y=np.where(y_user_train)\n",
    "    \n",
    "    for z in zip(ind_x,ind_y):\n",
    "        y_user_impute[z[0],z[1]]=y_test_pred[z[0],z[1]]\n",
    "    \n",
    "    impute_unique,impute_counts=np.unique(y_user_impute,return_counts=True)\n",
    "    print(*zip(impute_unique,impute_counts))\n",
    "    \n",
    "    with open('dataset/semisupervised_users_2H/{}_ssl_2H.pkl'.format(fname),'wb') as f:\n",
    "            pickle.dump(y_user_impute,f) #train_pred==test_pred\n",
    "    \n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2HDrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size 170, Output Size 51\n"
     ]
    }
   ],
   "source": [
    "# Defining sizes for neural networks and other global hyperparameters\n",
    "# input_size=x_train[0].shape[-1]\n",
    "input_size=170 #176 #464 #506 # 170\n",
    "hidden_size=16\n",
    "# output_size=y_train[0].shape[-1]\n",
    "output_size=51#51\n",
    "n_epoch=80\n",
    "bs=300\n",
    "lr_init=0.1\n",
    "momentum=0.5\n",
    "print('Input Size {}, Output Size {}'.format(input_size,output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_2HDrop(\n",
      "  (hidden0): Sequential(\n",
      "    (0): Linear(in_features=170, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.1)\n",
      "    (2): Dropout(p=0.2)\n",
      "  )\n",
      "  (hidden1): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.1)\n",
      "    (2): Dropout(p=0.2)\n",
      "  )\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=51, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP_2HDrop(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_2HDrop,self).__init__()\n",
    "        self.hidden0=nn.Sequential(\n",
    "            nn.Linear(input_size,hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Dropout(0.20)\n",
    "        )\n",
    "        self.hidden1=nn.Sequential(\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Dropout(0.20)\n",
    "        )\n",
    "        self.out=nn.Sequential(\n",
    "            nn.Linear(hidden_size,output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "model=MLP_2HDrop()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in glob.glob('dataset/Extrasensory_uuid_fl_uTAR/*'):\n",
    "    print(\"\\n\",g)\n",
    "    fname=g.split('/')[-1].split('.')[0]\n",
    "    (x_user_train,y_user_train,missed_label_user,tstamp_user,featurename_user,labelname_user) = read_user_data(g)\n",
    "    x_user_train,_,_=standardize(x_user_train)\n",
    "    x_user_train=np.nan_to_num(x_user_train)\n",
    "    \n",
    "    feature_names=summarize_features_worig(featurename_user)\n",
    "    x_user,feature_long_names=choose_sensors_longnames(x_user_train,sensor_types,feature_names)\n",
    "    \n",
    "    weights=instance_weight_matrix(y_user_train)\n",
    "    print(\"\\t Loaded datasets\")\n",
    "    \n",
    "    model=MLP_2HDrop() # Creating a new instance of the model every fold\n",
    "    \n",
    "    mlp2Hdrop_train_dict,mlp2Hdrop_test_dict,y_train_pred,y_test_pred=train_sensordropout(model,\n",
    "                                                             X=x_user,\n",
    "                                                             Y=y_user_train,\n",
    "                                                             X_test=x_user,\n",
    "                                                             Y_test=y_user_train,\n",
    "                                                             weights=weights,\n",
    "                                                             sensor_list=sensor_types,\n",
    "                                                             feature_names=feature_names,\n",
    "                                                             n_epoch=n_epoch,\n",
    "                                                             batch_size=bs,\n",
    "                                                             lr_init=lr_init,\n",
    "                                                             momentum=momentum,\n",
    "                                                             fold=-1)\n",
    "    \n",
    "    countlabels_user=np.sum(y_user_train,axis=0) # Column summary\n",
    "    labelname_countlabel_user=zip(labelname_user,countlabels_user) # Zip together names, counts\n",
    "    labelname_countlabel_user=sorted(labelname_countlabel_user,key=lambda row:row[-1],reverse=True)\n",
    "\n",
    "    print('Activities and counts:')\n",
    "    print(labelname_countlabel_user)\n",
    "    \n",
    "    print(mlp2Hdrop_test_dict)\n",
    "    \n",
    "    print(\"Y_train_pred==Y_test_pred:\",np.array_equal(y_train_pred,y_test_pred))\n",
    "    \n",
    "    y_user_impute=copy.deepcopy(y_user_train)\n",
    "    \n",
    "    ind_x,ind_y=np.where(y_user_train)\n",
    "    \n",
    "    for z in zip(ind_x,ind_y):\n",
    "        y_user_impute[z[0],z[1]]=y_test_pred[z[0],z[1]]\n",
    "    \n",
    "    impute_unique,impute_counts=np.unique(y_user_impute,return_counts=True)\n",
    "    print(*zip(impute_unique,impute_counts))\n",
    "    \n",
    "    with open('dataset/semisupervised_users_2Hdrop/{}_ssl_2Hdrop.pkl'.format(fname),'wb') as f:\n",
    "            pickle.dump(y_user_impute,f) #train_pred==test_pred\n",
    "    \n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "635.85px",
    "left": "213px",
    "right": "20px",
    "top": "203px",
    "width": "741px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
