{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Dataset-parsers-and-cleaning-functions\" data-toc-modified-id=\"Dataset-parsers-and-cleaning-functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset parsers and cleaning functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-data\" data-toc-modified-id=\"Test-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Test data</a></span></li></ul></li><li><span><a href=\"#Training-Functions\" data-toc-modified-id=\"Training-Functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training Functions</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Test-data-on-particular-sensors\" data-toc-modified-id=\"Test-data-on-particular-sensors-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Test data on particular sensors</a></span></li></ul></li><li><span><a href=\"#Creating-a-new-data-structure-for-all-valid-data-and-pickling-it\" data-toc-modified-id=\"Creating-a-new-data-structure-for-all-valid-data-and-pickling-it-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Creating a new data structure for all valid data and pickling it</a></span></li><li><span><a href=\"#Creating-and-pickling-instance-weight-matrix\" data-toc-modified-id=\"Creating-and-pickling-instance-weight-matrix-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Creating and pickling instance weight matrix</a></span></li><li><span><a href=\"#Miscellaneous-train/test-functions\" data-toc-modified-id=\"Miscellaneous-train/test-functions-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Miscellaneous train/test functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cuda-enable\" data-toc-modified-id=\"Cuda-enable-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Cuda-enable</a></span></li><li><span><a href=\"#Tackling-missing-labels-using-a-mask\" data-toc-modified-id=\"Tackling-missing-labels-using-a-mask-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Tackling missing labels using a mask</a></span></li><li><span><a href=\"#Linear-Learning-Rate-scheduler\" data-toc-modified-id=\"Linear-Learning-Rate-scheduler-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Linear Learning-Rate scheduler</a></span></li><li><span><a href=\"#Euclidean-Norm-for-weight-matrices\" data-toc-modified-id=\"Euclidean-Norm-for-weight-matrices-3.3.4\"><span class=\"toc-item-num\">3.3.4&nbsp;&nbsp;</span>Euclidean Norm for weight matrices</a></span></li><li><span><a href=\"#Accuracy-(Precision,-Recall,-F1,-Support,-Balanced-Accuracy)-metrics\" data-toc-modified-id=\"Accuracy-(Precision,-Recall,-F1,-Support,-Balanced-Accuracy)-metrics-3.3.5\"><span class=\"toc-item-num\">3.3.5&nbsp;&nbsp;</span>Accuracy (Precision, Recall, F1, Support, Balanced Accuracy) metrics</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import glob\n",
    "import pickle\n",
    "import copy\n",
    "import math\n",
    "from io import StringIO\n",
    "import importlib.machinery\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,balanced_accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support,classification_report\n",
    "#from sklearn.metrics import multilabel_confusion_matrix # Only available in dev .21\n",
    "\n",
    "# Need Pytorch for multilabel classifications\n",
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "#import skorch [Scikit-learn wrapper around Pytorch so allowing for K-fold cross-validation]\n",
    "random_state=10\n",
    "np.random.seed(random_state)\n",
    "\n",
    "from utils import cm,remove_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Data location and sample user\n",
    "prefix='dataset/Extrasensory_uuid_fl_uTAR/'\n",
    "cross_validation_user_loc='dataset/cv_5_folds/'\n",
    "user_sample='3600D531-0C55-44A7-AE95-A7A38519464E.features_labels'\n",
    "done=1 # Pickled files are created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset parsers and cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Dataset parsers for header/ body for CSVs\n",
    "def parse_header_of_csv(csv_str):\n",
    "    # Isolate the headline columns:\n",
    "    headline = csv_str[:csv_str.index('\\n')];\n",
    "    columns = headline.split(',');\n",
    "\n",
    "    # The first column should be timestamp:\n",
    "    assert columns[0] == 'timestamp';\n",
    "    # The last column should be label_source:\n",
    "    assert columns[-1] == 'label_source';\n",
    "    \n",
    "    # Search for the column of the first label:\n",
    "    for (ci,col) in enumerate(columns):\n",
    "        if col.startswith('label:'):\n",
    "            first_label_ind = ci;\n",
    "            break;\n",
    "        pass;\n",
    "\n",
    "    # Feature columns come after timestamp and before the labels:\n",
    "    feature_names = columns[1:first_label_ind];\n",
    "    # Then come the labels, till the one-before-last column:\n",
    "    label_names = columns[first_label_ind:-1];\n",
    "    for (li,label) in enumerate(label_names):\n",
    "        # In the CSV the label names appear with prefix 'label:', but we don't need it after reading the data:\n",
    "        assert label.startswith('label:');\n",
    "        label_names[li] = label.replace('label:','');\n",
    "        pass;\n",
    "    \n",
    "    return (feature_names,label_names);\n",
    "\n",
    "def parse_body_of_csv(csv_str,n_features):\n",
    "    # Read the entire CSV body into a single numeric matrix:\n",
    "    full_table = np.loadtxt(StringIO(csv_str),delimiter=',',skiprows=1);\n",
    "    \n",
    "    # Timestamp is the primary key for the records (examples):\n",
    "    timestamps = full_table[:,0].astype(int);\n",
    "    \n",
    "    # Read the sensor features:\n",
    "    X = full_table[:,1:(n_features+1)];\n",
    "    \n",
    "    # Read the binary label values, and the 'missing label' indicators:\n",
    "    trinary_labels_mat = full_table[:,(n_features+1):-1]; # This should have values of either 0., 1. or NaN\n",
    "    M = np.isnan(trinary_labels_mat); # M is the missing label matrix\n",
    "    \n",
    "    #print(\"M matrix shape:\",M.shape)\n",
    "    #print(\"Matrix: \",np.argwhere(M))\n",
    "    trinary_labels_mat[M]=-1 # Replace NaNs with -1.0 for which we then apply a mask\n",
    "    unique,counts=np.unique(trinary_labels_mat,return_counts=True)\n",
    "    print(*zip(unique,counts)) \n",
    "    \n",
    "#     Y = np.where(M,0,trinary_labels_mat) > 0.; # Y is the label matrix\n",
    "    \n",
    "    return (X,trinary_labels_mat,M,timestamps);\n",
    "\n",
    "def read_user_data(directory):\n",
    "    print('Reading {}'.format(directory.split(\"/\")[-1]))\n",
    "\n",
    "    # Read the entire csv file of the user:\n",
    "    with gzip.open(directory,'rb') as fid:\n",
    "        csv_str = fid.read();\n",
    "        csv_str = csv_str.decode(\"utf-8\")\n",
    "        pass;\n",
    "\n",
    "    (feature_names,label_names) = parse_header_of_csv(csv_str);\n",
    "    n_features = len(feature_names);\n",
    "    (X,Y,M,timestamps) = parse_body_of_csv(csv_str,n_features);\n",
    "\n",
    "    return (X,Y,M,timestamps,feature_names,label_names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Clean labels\n",
    "def clean_labels(input_label):\n",
    "    if label.endswith('_'):\n",
    "        label=label[:-1]+')'\n",
    "    label=label.replace('__',' (').replace('_',' ')\n",
    "    label=label[0]+label[1:].lower()\n",
    "    label=label.replace('i m','I\\'m')\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Get a summary of the sensor feature\n",
    "'''\n",
    "# Summarize features as we are only using phone_acc,phone_gyro,phone_mag,phone_loc,phone_audio,\n",
    "# phone_app,phone_battery,phone_use,phone_callstat,phone_wifi,phone_lf,phone_time\n",
    "# We are ignoring the use of the smartwatch features. There are definitely features that will be used\n",
    "# much more (e.g. than the phone_callstat) but we'll leave that up to the ML algorithm.\n",
    "'''\n",
    "def summarize_features(feature_list):\n",
    "    summary_feature_list=np.empty_like(feature_list)\n",
    "    for (ind,feature) in enumerate(feature_list):\n",
    "        if feature.startswith('raw_acc'):\n",
    "            summary_feature_list[ind]='phone_acc' \n",
    "        if feature.startswith('proc_gyro'):\n",
    "            summary_feature_list[ind]='phone_gyro'\n",
    "        if feature.startswith('raw_magnet'):\n",
    "            summary_feature_list[ind]='phone_mag'\n",
    "        if feature.startswith('watch_acc'):\n",
    "            summary_feature_list[ind]='watch_acc'\n",
    "        if feature.startswith('watch_heading'):\n",
    "            summary_feature_list[ind]='watch_dir'\n",
    "        if feature.startswith('location'):\n",
    "            summary_feature_list[ind]='phone_loc'\n",
    "        if feature.startswith('audio'):\n",
    "            summary_feature_list[ind]='phone_audio'\n",
    "        if feature.startswith('discrete:app_state'):\n",
    "            summary_feature_list[ind]='phone_app'\n",
    "        if feature.startswith('discrete:battery'):\n",
    "            summary_feature_list[ind]='phone_battery'\n",
    "        if feature.startswith('discrete:on'):\n",
    "            summary_feature_list[ind]='phone_use'\n",
    "        if feature.startswith('discrete:ringer'):\n",
    "            summary_feature_list[ind]='phone_callstat'\n",
    "        if feature.startswith('discrete:wifi'):\n",
    "            summary_feature_list[ind]='phone_wifi'\n",
    "        if feature.startswith('lf'):\n",
    "            summary_feature_list[ind]='phone_lf'\n",
    "        if feature.startswith('discrete:time'):\n",
    "            summary_feature_list[ind]='phone_time'\n",
    "\n",
    "    return summary_feature_list\n",
    "\n",
    "\n",
    "# Get a summary of the sensor feature along with the original label that was used\n",
    "def summarize_features_worig(feature_list):\n",
    "    summary_feature_list=np.empty((len(feature_list),2),dtype=object)\n",
    "    \n",
    "    for (ind,feature) in enumerate(feature_list):\n",
    "        if feature.startswith('raw_acc'):\n",
    "            summary_feature_list[ind,0]='phone_acc'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('proc_gyro'):\n",
    "            summary_feature_list[ind,0]='phone_gyro'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('raw_magnet'):\n",
    "            summary_feature_list[ind,0]='phone_mag'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('watch_acc'):\n",
    "            summary_feature_list[ind,0]='watch_acc'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('watch_heading'):\n",
    "            summary_feature_list[ind,0]='watch_dir'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('location'):\n",
    "            summary_feature_list[ind,0]='phone_loc'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('audio'):\n",
    "            summary_feature_list[ind,0]='phone_audio'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:app_state'):\n",
    "            summary_feature_list[ind,0]='phone_app'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:battery'):\n",
    "            summary_feature_list[ind,0]='phone_battery'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:on'):\n",
    "            summary_feature_list[ind,0]='phone_use'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:ringer'):\n",
    "            summary_feature_list[ind,0]='phone_callstat'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:wifi'):\n",
    "            summary_feature_list[ind,0]='phone_wifi'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('lf'):\n",
    "            summary_feature_list[ind,0]='phone_lf'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:time'):\n",
    "            summary_feature_list[ind,0]='phone_time'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "\n",
    "    return summary_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Custom dictionary class with help for duplicate keys\n",
    "class Customdictionary(dict):\n",
    "    def __setitem__(self,key,value):\n",
    "        try:\n",
    "            self[key]\n",
    "        except KeyError:\n",
    "            super(Customdictionary,self).__setitem__(key,[])\n",
    "        self[key].append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 3600D531-0C55-44A7-AE95-A7A38519464E.features_labels.csv.gz\n",
      "(-1.0, 148794) (0.0, 97289) (1.0, 19270)\n",
      "Data shape input for user (Len minutes/num examples, num sensors):  (5203, 225)\n",
      "Label shape for user (Len minutes, num labels):  (5203, 51) \n",
      "\n",
      "Sensor feature names:\n",
      "\n",
      "Activities and counts:\n",
      "[('LOC_home', 3040.0), ('SITTING', 481.0), ('WITH_FRIENDS', 295.0), ('PHONE_ON_TABLE', -42.0), ('LYING_DOWN', -99.0), ('OR_indoors', -102.0), ('SLEEPING', -414.0), ('WATCHING_TV', -523.0), ('EATING', -673.0), ('TALKING', -797.0), ('DRIVE_-_I_M_A_PASSENGER', -1026.0), ('OR_standing', -1051.0), ('IN_A_CAR', -1093.0), ('OR_exercise', -1273.0), ('AT_THE_GYM', -1273.0), ('SINGING', -1299.0), ('FIX_walking', -1303.0), ('SHOPPING', -1324.0), ('AT_SCHOOL', -1330.0), ('BATHING_-_SHOWER', -1350.0), ('DRESSING', -1368.0), ('DRINKING__ALCOHOL_', -1369.0), ('FIX_restaurant', -1376.0), ('IN_CLASS', -1381.0), ('IN_A_MEETING', -1408.0), ('TOILET', -1423.0), ('COOKING', -1430.0), ('ELEVATOR', -1434.0), ('PHONE_IN_POCKET', -1515.0), ('PHONE_IN_HAND', -2157.0), ('PHONE_IN_BAG', -2188.0), ('OR_outside', -2462.0), ('FIX_running', -5203.0), ('BICYCLING', -5203.0), ('LAB_WORK', -5203.0), ('LOC_main_workplace', -5203.0), ('ON_A_BUS', -5203.0), ('DRIVE_-_I_M_THE_DRIVER', -5203.0), ('STROLLING', -5203.0), ('CLEANING', -5203.0), ('DOING_LAUNDRY', -5203.0), ('WASHING_DISHES', -5203.0), ('SURFING_THE_INTERNET', -5203.0), ('AT_A_PARTY', -5203.0), ('AT_A_BAR', -5203.0), ('LOC_beach', -5203.0), ('COMPUTER_WORK', -5203.0), ('GROOMING', -5203.0), ('STAIRS_-_GOING_UP', -5203.0), ('STAIRS_-_GOING_DOWN', -5203.0), ('WITH_CO-WORKERS', -5203.0)]\n"
     ]
    }
   ],
   "source": [
    "# Reading sample data\n",
    "sample_loc='{}/{}.csv.gz'.format(prefix,user_sample)\n",
    "x_user,y_user,missedlabel_user,tstamp_user,featurename_user,labelname_user=read_user_data(sample_loc)\n",
    "\n",
    "# Dataset summaries for this user\n",
    "print('Data shape input for user (Len minutes/num examples, num sensors): ',x_user.shape) # Timestep examples, number of sensors\n",
    "print('Label shape for user (Len minutes, num labels): ',y_user.shape,'\\n') # Timestep examples, labels\n",
    "\n",
    "countlabels_user=np.sum(y_user,axis=0) # Column summary\n",
    "labelname_countlabel_user=zip(labelname_user,countlabels_user) # Zip together names, counts\n",
    "labelname_countlabel_user=sorted(labelname_countlabel_user,key=lambda row:row[-1],reverse=True)\n",
    "\n",
    "print('Sensor feature names:\\n')\n",
    "feature_names=summarize_features(featurename_user)\n",
    "    \n",
    "# for i,sensor_feature in enumerate(featurename_user):\n",
    "#     print('{} :: {} ::--> {}\\n'.format(i,feature_names[i],sensor_feature))\n",
    "\n",
    "print('Activities and counts:')\n",
    "print(labelname_countlabel_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Choosing sensor labels\n",
    "'''\n",
    "Summary sensor choices are: phone_acc,phone_gyro,phone_mag,watch_acc,watch_dir,phone_loc,phone_audio,\n",
    "phone_app,phone_battery,phone_use,phone_callstat,phone_wifi,phone_lf,phone_time\n",
    "In this project, we aren't using watch_acc,watch_dir (no smartwatch)\n",
    "'''\n",
    "\n",
    "def choose_sensors(X_train,used_sensors,summarized_feature_names):\n",
    "    used_sensor_feature_names=np.zeros(len(summarized_feature_names),dtype=bool)\n",
    "    # Creates a zero boolean vector of all possible feature names\n",
    "    for s in used_sensors:\n",
    "        used_sensor_feature_names=np.logical_or(used_sensor_feature_names,(s==summarized_feature_names))\n",
    "    X_train=X_train[:,used_sensor_feature_names]\n",
    "    return X_train\n",
    "\n",
    "def choose_sensors_dropout(X_train,used_sensors,summarized_feature_names):\n",
    "    used_sensor_feature_names=np.zeros(len(summarized_feature_names),dtype=bool)\n",
    "    data_length=len(X_train)\n",
    "    \n",
    "    # Creates a zero boolean vector of all possible feature names\n",
    "    for s in used_sensors:\n",
    "        used_sensor_feature_names=np.logical_or(used_sensor_feature_names,(s==summarized_feature_names))\n",
    "    mask=np.tile(used_sensor_feature_names,(data_length,1))\n",
    "    \n",
    "    X_train=np.multiply(X_train,mask) # Element-wise matrix multiply\n",
    "    return X_train\n",
    "\n",
    "def choose_sensors_longnames(X_train,used_sensors,long_featurenames):\n",
    "    \n",
    "    used_sensor_feature_names=np.zeros(len(long_featurenames),dtype=bool)\n",
    "    used_feature_actualnames=np.zeros(len(long_featurenames),dtype=bool)\n",
    "    # Creates a zero boolean vector of all possible feature names\n",
    "    summary_features=long_featurenames[:,0]\n",
    "    all_complete_features=long_featurenames[:,-1]\n",
    "    \n",
    "    for s in used_sensors:\n",
    "        similar=(s==summary_features)\n",
    "        \n",
    "        #used_complete_features=(all_complete_features[similar.astype(int)])\n",
    "       \n",
    "        used_sensor_feature_names=np.logical_or(used_sensor_feature_names,similar)\n",
    "        used_feature_actualnames=np.logical_or(used_feature_actualnames,similar)\n",
    "    \n",
    "    X_train=X_train[:,used_sensor_feature_names]\n",
    "    long_names=all_complete_features[used_feature_actualnames]\n",
    "    return X_train,long_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Returns a standardized (0 mean, 1 variance) dataset\n",
    "def standardize(X_train):\n",
    "    mean=np.nanmean(X_train,axis=0).reshape((1,-1))# Ignores NaNs while finding the mean across rows\n",
    "    standard_dev=np.nanstd(X_train,axis=0) # Ignores NaNs while finding the standard deviation across rows\n",
    "    standard_dev_nonzero=np.where(standard_dev>0,standard_dev,1.).reshape((1,-1)) # Div zero\n",
    "    \n",
    "    X=(X_train-mean)/standard_dev_nonzero\n",
    "    return X,mean,standard_dev_nonzero   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Sensor Types, Label Possibilities variables\n",
    "sensor_types=['phone_acc','phone_gyro','phone_mag','phone_loc','phone_audio',\n",
    "'phone_app','phone_battery','phone_use','phone_callstat','phone_wifi','phone_lf',\n",
    "'phone_time']\n",
    "label_possibilities=['LOC_home','OR_indoors','PHONE_ON_TABLE','SITTING','WITH_FRIENDS',\n",
    " 'LYING_DOWN','SLEEPING','WATCHING_TV','EATING','PHONE_IN_POCKET',\n",
    " 'TALKING','DRIVE_-_I_M_A_PASSENGER','OR_standing','IN_A_CAR',\n",
    " 'OR_exercise','AT_THE_GYM','SINGING','FIX_walking','OR_outside',\n",
    " 'SHOPPING','AT_SCHOOL','BATHING_-_SHOWER','DRESSING','DRINKING__ALCOHOL_',\n",
    " 'PHONE_IN_HAND','FIX_restaurant','IN_CLASS','PHONE_IN_BAG','IN_A_MEETING',\n",
    " 'TOILET','COOKING','ELEVATOR','FIX_running','BICYCLING','LAB_WORK',\n",
    " 'LOC_main_workplace','ON_A_BUS','DRIVE_-_I_M_THE_DRIVER','STROLLING',\n",
    " 'CLEANING','DOING_LAUNDRY','WASHING_DISHES','SURFING_THE_INTERNET',\n",
    " 'AT_A_PARTY','AT_A_BAR','LOC_beach','COMPUTER_WORK','GROOMING','STAIRS_-_GOING_UP',\n",
    " 'STAIRS_-_GOING_DOWN','WITH_CO-WORKERS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data on particular sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 3600D531-0C55-44A7-AE95-A7A38519464E.features_labels.csv.gz\n",
      "(-1.0, 148794) (0.0, 97289) (1.0, 19270)\n",
      "Data shape input for user (Len minutes/num examples, num sensors):  (5203, 225)\n",
      "Label shape for user (Len minutes, num labels):  (5203, 51) \n",
      "\n",
      "Sensor feature names:\n",
      "\n",
      "Activities and counts:\n",
      "[('LOC_home', 3040.0), ('SITTING', 481.0), ('WITH_FRIENDS', 295.0), ('PHONE_ON_TABLE', -42.0), ('LYING_DOWN', -99.0), ('OR_indoors', -102.0), ('SLEEPING', -414.0), ('WATCHING_TV', -523.0), ('EATING', -673.0), ('TALKING', -797.0), ('DRIVE_-_I_M_A_PASSENGER', -1026.0), ('OR_standing', -1051.0), ('IN_A_CAR', -1093.0), ('OR_exercise', -1273.0), ('AT_THE_GYM', -1273.0), ('SINGING', -1299.0), ('FIX_walking', -1303.0), ('SHOPPING', -1324.0), ('AT_SCHOOL', -1330.0), ('BATHING_-_SHOWER', -1350.0), ('DRESSING', -1368.0), ('DRINKING__ALCOHOL_', -1369.0), ('FIX_restaurant', -1376.0), ('IN_CLASS', -1381.0), ('IN_A_MEETING', -1408.0), ('TOILET', -1423.0), ('COOKING', -1430.0), ('ELEVATOR', -1434.0), ('PHONE_IN_POCKET', -1515.0), ('PHONE_IN_HAND', -2157.0), ('PHONE_IN_BAG', -2188.0), ('OR_outside', -2462.0), ('FIX_running', -5203.0), ('BICYCLING', -5203.0), ('LAB_WORK', -5203.0), ('LOC_main_workplace', -5203.0), ('ON_A_BUS', -5203.0), ('DRIVE_-_I_M_THE_DRIVER', -5203.0), ('STROLLING', -5203.0), ('CLEANING', -5203.0), ('DOING_LAUNDRY', -5203.0), ('WASHING_DISHES', -5203.0), ('SURFING_THE_INTERNET', -5203.0), ('AT_A_PARTY', -5203.0), ('AT_A_BAR', -5203.0), ('LOC_beach', -5203.0), ('COMPUTER_WORK', -5203.0), ('GROOMING', -5203.0), ('STAIRS_-_GOING_UP', -5203.0), ('STAIRS_-_GOING_DOWN', -5203.0), ('WITH_CO-WORKERS', -5203.0)]\n"
     ]
    }
   ],
   "source": [
    "# Reading sample data\n",
    "sample_loc='{}/{}.csv.gz'.format(prefix,user_sample)\n",
    "x_user,y_user,missedlabel_user,tstamp_user,featurename_user,labelname_user=read_user_data(sample_loc)\n",
    "\n",
    "# Dataset summaries for this user\n",
    "print('Data shape input for user (Len minutes/num examples, num sensors): ',x_user.shape) # Timestep examples, number of sensors\n",
    "print('Label shape for user (Len minutes, num labels): ',y_user.shape,'\\n') # Timestep examples, labels\n",
    "\n",
    "countlabels_user=np.sum(y_user,axis=0) # Column summary\n",
    "labelname_countlabel_user=zip(labelname_user,countlabels_user) # Zip together names, counts\n",
    "labelname_countlabel_user=sorted(labelname_countlabel_user,key=lambda row:row[-1],reverse=True)\n",
    "\n",
    "print('Sensor feature names:\\n')\n",
    "feature_names_woriginallabels=summarize_features_worig(featurename_user)\n",
    "    \n",
    "print('Activities and counts:')\n",
    "print(labelname_countlabel_user)\n",
    "\n",
    "x_train_chosen,feature_long_names=choose_sensors_longnames(x_user,sensor_types,feature_names_woriginallabels)\n",
    "# feature_long_names is original long feature name from the chosen sensor list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new data structure for all valid data and pickling it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with np.nan labels (missing labels). Zero impute missing feature entries. Standardization done at train time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "done=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kalman_loc='dataset/impute_raw/kalmanimpute_raw/'\n",
    "# arimakalman_loc='dataset/impute_raw/arimakalmanimpute_raw/'\n",
    "ssl_user_loc='dataset/semisupervised_users_2Hdrop/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Train:  2C32C23E-E30C-498A-8DD2-0EFB9150A02E\n",
      "Working on Train:  3600D531-0C55-44A7-AE95-A7A38519464E\n",
      "Working on Train:  4FC32141-E888-4BFF-8804-12559A491D8C\n",
      "Working on Train:  5119D0F8-FCA8-4184-A4EB-19421A40DE0D\n",
      "Working on Train:  5152A2DF-FAF3-4BA8-9CA9-E66B32671A53\n",
      "Working on Train:  5EF64122-B513-46AE-BCF1-E62AAC285D2C\n",
      "Working on Train:  61976C24-1C50-4355-9C49-AAE44A7D09F6\n",
      "Working on Train:  78A91A4E-4A51-4065-BDA7-94755F0BB3BB\n",
      "Working on Train:  7CE37510-56D0-4120-A1CF-0E23351428D2\n",
      "Working on Train:  7D9BB102-A612-4E2A-8E22-3159752F55D8\n",
      "Working on Train:  8023FE1A-D3B0-4E2C-A57A-9321B7FC755F\n",
      "Working on Train:  83CF687B-7CEC-434B-9FE8-00C3D5799BE6\n",
      "Working on Train:  9759096F-1119-4E19-A0AD-6F16989C7E1C\n",
      "Working on Train:  9DC38D04-E82E-4F29-AB52-B476535226F2\n",
      "Working on Train:  A5CDF89D-02A2-4EC1-89F8-F534FDABDD96\n",
      "Working on Train:  A76A5AF5-5A93-4CF2-A16E-62353BB70E8A\n",
      "Working on Train:  B09E373F-8A54-44C8-895B-0039390B859F\n",
      "Working on Train:  B9724848-C7E2-45F4-9B3F-A1F38D864495\n",
      "Working on Train:  BE3CA5A6-A561-4BBD-B7C9-5DF6805400FC\n",
      "Working on Train:  C48CE857-A0DD-4DDB-BEA5-3A25449B2153\n",
      "Working on Train:  CA820D43-E5E2-42EF-9798-BE56F776370B\n",
      "Working on Train:  CDA3BBF7-6631-45E8-85BA-EEB416B32A3C\n",
      "Working on Train:  CF722AA9-2533-4E51-9FEB-9EAC84EE9AAC\n",
      "Working on Train:  D7D20E2E-FC78-405D-B346-DBD3FD8FC92B\n",
      "Working on Train:  E65577C1-8D5D-4F70-AF23-B3ADB9D3DBA3\n",
      "Working on Train:  ECECC2AB-D32F-4F90-B74C-E12A1C69BBE2\n",
      "Working on Train:  FDAA70A1-42A3-4E3F-9AE3-3FDA412E03BF\n",
      "Working on Train:  33A85C34-CFE4-4732-9E73-0A7AC861B27A\n",
      "Working on Train:  40E170A7-607B-4578-AF04-F021C3B0384A\n",
      "Working on Train:  481F4DD2-7689-43B9-A2AA-C8772227162B\n",
      "Working on Train:  4E98F91F-4654-42EF-B908-A3389443F2E7\n",
      "Working on Train:  59818CD2-24D7-4D32-B133-24C2FE3801E5\n",
      "Working on Train:  59EEFAE0-DEB0-4FFF-9250-54D2A03D0CF2\n",
      "Working on Train:  61359772-D8D8-480D-B623-7C636EAD0C81\n",
      "Working on Train:  665514DE-49DC-421F-8DCB-145D0B2609AD\n",
      "Working on Train:  74B86067-5D4B-43CF-82CF-341B76BEA0F4\n",
      "Working on Train:  797D145F-3858-4A7F-A7C2-A4EB721E133C\n",
      "Working on Train:  806289BC-AD52-4CC1-806C-0CDB14D65EB6\n",
      "Working on Train:  81536B0A-8DBF-4D8A-AC24-9543E2E4C8E0\n",
      "Working on Train:  86A4F379-B305-473D-9D83-FC7D800180EF\n",
      "Working on Train:  96A358A0-FFF2-4239-B93E-C7425B901B47\n",
      "Working on Train:  99B204C0-DD5C-4BB7-83E8-A37281B8D769\n",
      "Working on Train:  A5A30F76-581E-4757-97A2-957553A2C6AA\n",
      "Working on Train:  A7599A50-24AE-46A6-8EA6-2576F1011D81\n",
      "Working on Train:  B7F9D634-263E-4A97-87F9-6FFB4DDCB36C\n",
      "Working on Train:  BEF6C611-50DA-4971-A040-87FB979F3FC1\n",
      "Working on Train:  CCAF77F0-FABB-4F2F-9E24-D56AD0C5A82F\n",
      "Working on Train:  F50235E0-DD67-4F2A-B00B-1F31ADA998B9\n",
      "Working on Test:  00EABED2-271D-49D8-B599-1D4A09240601\n",
      "Working on Test:  098A72A5-E3E5-4F54-A152-BBDA0DF7B694\n",
      "Working on Test:  0A986513-7828-4D53-AA1F-E02D6DF9561B\n",
      "Working on Test:  11B5EC4D-4133-4289-B475-4E737182A406\n",
      "Working on Test:  136562B6-95B2-483D-88DC-065F28409FD2\n",
      "Working on Test:  1538C99F-BA1E-4EFB-A949-6C7C47701B20\n",
      "Working on Test:  24E40C4C-A349-4F9F-93AB-01D00FB994AF\n",
      "Working on Test:  0BFC35E2-4817-4865-BFA7-764742302A2D\n",
      "Working on Test:  0E6184E1-90C0-48EE-B25A-F1ECB7B9714E\n",
      "Working on Test:  1155FF54-63D3-4AB2-9863-8385D0BD0A13\n",
      "Working on Test:  1DBB0F6F-1F81-4A50-9DF4-CD62ACFA4842\n",
      "Working on Test:  27E04243-B138-4F40-A164-F40B60165CF3\n",
      "Done for fold 0\n",
      "Working on Train:  0BFC35E2-4817-4865-BFA7-764742302A2D\n",
      "Working on Train:  0E6184E1-90C0-48EE-B25A-F1ECB7B9714E\n",
      "Working on Train:  1155FF54-63D3-4AB2-9863-8385D0BD0A13\n",
      "Working on Train:  1DBB0F6F-1F81-4A50-9DF4-CD62ACFA4842\n",
      "Working on Train:  27E04243-B138-4F40-A164-F40B60165CF3\n",
      "Working on Train:  59EEFAE0-DEB0-4FFF-9250-54D2A03D0CF2\n",
      "Working on Train:  61359772-D8D8-480D-B623-7C636EAD0C81\n",
      "Working on Train:  665514DE-49DC-421F-8DCB-145D0B2609AD\n",
      "Working on Train:  74B86067-5D4B-43CF-82CF-341B76BEA0F4\n",
      "Working on Train:  797D145F-3858-4A7F-A7C2-A4EB721E133C\n",
      "Working on Train:  806289BC-AD52-4CC1-806C-0CDB14D65EB6\n",
      "Working on Train:  81536B0A-8DBF-4D8A-AC24-9543E2E4C8E0\n",
      "Working on Train:  86A4F379-B305-473D-9D83-FC7D800180EF\n",
      "Working on Train:  96A358A0-FFF2-4239-B93E-C7425B901B47\n",
      "Working on Train:  99B204C0-DD5C-4BB7-83E8-A37281B8D769\n",
      "Working on Train:  A5A30F76-581E-4757-97A2-957553A2C6AA\n",
      "Working on Train:  A7599A50-24AE-46A6-8EA6-2576F1011D81\n",
      "Working on Train:  B7F9D634-263E-4A97-87F9-6FFB4DDCB36C\n",
      "Working on Train:  BEF6C611-50DA-4971-A040-87FB979F3FC1\n",
      "Working on Train:  CCAF77F0-FABB-4F2F-9E24-D56AD0C5A82F\n",
      "Working on Train:  F50235E0-DD67-4F2A-B00B-1F31ADA998B9\n",
      "Working on Train:  00EABED2-271D-49D8-B599-1D4A09240601\n",
      "Working on Train:  098A72A5-E3E5-4F54-A152-BBDA0DF7B694\n",
      "Working on Train:  0A986513-7828-4D53-AA1F-E02D6DF9561B\n",
      "Working on Train:  11B5EC4D-4133-4289-B475-4E737182A406\n",
      "Working on Train:  136562B6-95B2-483D-88DC-065F28409FD2\n",
      "Working on Train:  1538C99F-BA1E-4EFB-A949-6C7C47701B20\n",
      "Working on Train:  24E40C4C-A349-4F9F-93AB-01D00FB994AF\n",
      "Working on Train:  78A91A4E-4A51-4065-BDA7-94755F0BB3BB\n",
      "Working on Train:  7CE37510-56D0-4120-A1CF-0E23351428D2\n",
      "Working on Train:  7D9BB102-A612-4E2A-8E22-3159752F55D8\n",
      "Working on Train:  8023FE1A-D3B0-4E2C-A57A-9321B7FC755F\n",
      "Working on Train:  83CF687B-7CEC-434B-9FE8-00C3D5799BE6\n",
      "Working on Train:  9759096F-1119-4E19-A0AD-6F16989C7E1C\n",
      "Working on Train:  9DC38D04-E82E-4F29-AB52-B476535226F2\n",
      "Working on Train:  A5CDF89D-02A2-4EC1-89F8-F534FDABDD96\n",
      "Working on Train:  A76A5AF5-5A93-4CF2-A16E-62353BB70E8A\n",
      "Working on Train:  B09E373F-8A54-44C8-895B-0039390B859F\n",
      "Working on Train:  B9724848-C7E2-45F4-9B3F-A1F38D864495\n",
      "Working on Train:  BE3CA5A6-A561-4BBD-B7C9-5DF6805400FC\n",
      "Working on Train:  C48CE857-A0DD-4DDB-BEA5-3A25449B2153\n",
      "Working on Train:  CA820D43-E5E2-42EF-9798-BE56F776370B\n",
      "Working on Train:  CDA3BBF7-6631-45E8-85BA-EEB416B32A3C\n",
      "Working on Train:  CF722AA9-2533-4E51-9FEB-9EAC84EE9AAC\n",
      "Working on Train:  D7D20E2E-FC78-405D-B346-DBD3FD8FC92B\n",
      "Working on Train:  E65577C1-8D5D-4F70-AF23-B3ADB9D3DBA3\n",
      "Working on Train:  ECECC2AB-D32F-4F90-B74C-E12A1C69BBE2\n",
      "Working on Train:  FDAA70A1-42A3-4E3F-9AE3-3FDA412E03BF\n",
      "Working on Test:  33A85C34-CFE4-4732-9E73-0A7AC861B27A\n",
      "Working on Test:  40E170A7-607B-4578-AF04-F021C3B0384A\n",
      "Working on Test:  481F4DD2-7689-43B9-A2AA-C8772227162B\n",
      "Working on Test:  4E98F91F-4654-42EF-B908-A3389443F2E7\n",
      "Working on Test:  59818CD2-24D7-4D32-B133-24C2FE3801E5\n",
      "Working on Test:  2C32C23E-E30C-498A-8DD2-0EFB9150A02E\n",
      "Working on Test:  3600D531-0C55-44A7-AE95-A7A38519464E\n",
      "Working on Test:  4FC32141-E888-4BFF-8804-12559A491D8C\n",
      "Working on Test:  5119D0F8-FCA8-4184-A4EB-19421A40DE0D\n",
      "Working on Test:  5152A2DF-FAF3-4BA8-9CA9-E66B32671A53\n",
      "Working on Test:  5EF64122-B513-46AE-BCF1-E62AAC285D2C\n",
      "Working on Test:  61976C24-1C50-4355-9C49-AAE44A7D09F6\n",
      "Done for fold 1\n",
      "Working on Train:  0BFC35E2-4817-4865-BFA7-764742302A2D\n",
      "Working on Train:  0E6184E1-90C0-48EE-B25A-F1ECB7B9714E\n",
      "Working on Train:  1155FF54-63D3-4AB2-9863-8385D0BD0A13\n",
      "Working on Train:  1DBB0F6F-1F81-4A50-9DF4-CD62ACFA4842\n",
      "Working on Train:  27E04243-B138-4F40-A164-F40B60165CF3\n",
      "Working on Train:  33A85C34-CFE4-4732-9E73-0A7AC861B27A\n",
      "Working on Train:  40E170A7-607B-4578-AF04-F021C3B0384A\n",
      "Working on Train:  481F4DD2-7689-43B9-A2AA-C8772227162B\n",
      "Working on Train:  4E98F91F-4654-42EF-B908-A3389443F2E7\n",
      "Working on Train:  59818CD2-24D7-4D32-B133-24C2FE3801E5\n",
      "Working on Train:  806289BC-AD52-4CC1-806C-0CDB14D65EB6\n",
      "Working on Train:  81536B0A-8DBF-4D8A-AC24-9543E2E4C8E0\n",
      "Working on Train:  86A4F379-B305-473D-9D83-FC7D800180EF\n",
      "Working on Train:  96A358A0-FFF2-4239-B93E-C7425B901B47\n",
      "Working on Train:  99B204C0-DD5C-4BB7-83E8-A37281B8D769\n",
      "Working on Train:  A5A30F76-581E-4757-97A2-957553A2C6AA\n",
      "Working on Train:  A7599A50-24AE-46A6-8EA6-2576F1011D81\n",
      "Working on Train:  B7F9D634-263E-4A97-87F9-6FFB4DDCB36C\n",
      "Working on Train:  BEF6C611-50DA-4971-A040-87FB979F3FC1\n",
      "Working on Train:  CCAF77F0-FABB-4F2F-9E24-D56AD0C5A82F\n",
      "Working on Train:  F50235E0-DD67-4F2A-B00B-1F31ADA998B9\n",
      "Working on Train:  00EABED2-271D-49D8-B599-1D4A09240601\n",
      "Working on Train:  098A72A5-E3E5-4F54-A152-BBDA0DF7B694\n",
      "Working on Train:  0A986513-7828-4D53-AA1F-E02D6DF9561B\n",
      "Working on Train:  11B5EC4D-4133-4289-B475-4E737182A406\n",
      "Working on Train:  136562B6-95B2-483D-88DC-065F28409FD2\n",
      "Working on Train:  1538C99F-BA1E-4EFB-A949-6C7C47701B20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Train:  24E40C4C-A349-4F9F-93AB-01D00FB994AF\n",
      "Working on Train:  2C32C23E-E30C-498A-8DD2-0EFB9150A02E\n",
      "Working on Train:  3600D531-0C55-44A7-AE95-A7A38519464E\n",
      "Working on Train:  4FC32141-E888-4BFF-8804-12559A491D8C\n",
      "Working on Train:  5119D0F8-FCA8-4184-A4EB-19421A40DE0D\n",
      "Working on Train:  5152A2DF-FAF3-4BA8-9CA9-E66B32671A53\n",
      "Working on Train:  5EF64122-B513-46AE-BCF1-E62AAC285D2C\n",
      "Working on Train:  61976C24-1C50-4355-9C49-AAE44A7D09F6\n",
      "Working on Train:  A5CDF89D-02A2-4EC1-89F8-F534FDABDD96\n",
      "Working on Train:  A76A5AF5-5A93-4CF2-A16E-62353BB70E8A\n",
      "Working on Train:  B09E373F-8A54-44C8-895B-0039390B859F\n",
      "Working on Train:  B9724848-C7E2-45F4-9B3F-A1F38D864495\n",
      "Working on Train:  BE3CA5A6-A561-4BBD-B7C9-5DF6805400FC\n",
      "Working on Train:  C48CE857-A0DD-4DDB-BEA5-3A25449B2153\n",
      "Working on Train:  CA820D43-E5E2-42EF-9798-BE56F776370B\n",
      "Working on Train:  CDA3BBF7-6631-45E8-85BA-EEB416B32A3C\n",
      "Working on Train:  CF722AA9-2533-4E51-9FEB-9EAC84EE9AAC\n",
      "Working on Train:  D7D20E2E-FC78-405D-B346-DBD3FD8FC92B\n",
      "Working on Train:  E65577C1-8D5D-4F70-AF23-B3ADB9D3DBA3\n",
      "Working on Train:  ECECC2AB-D32F-4F90-B74C-E12A1C69BBE2\n",
      "Working on Train:  FDAA70A1-42A3-4E3F-9AE3-3FDA412E03BF\n",
      "Working on Test:  59EEFAE0-DEB0-4FFF-9250-54D2A03D0CF2\n",
      "Working on Test:  61359772-D8D8-480D-B623-7C636EAD0C81\n",
      "Working on Test:  665514DE-49DC-421F-8DCB-145D0B2609AD\n",
      "Working on Test:  74B86067-5D4B-43CF-82CF-341B76BEA0F4\n",
      "Working on Test:  797D145F-3858-4A7F-A7C2-A4EB721E133C\n",
      "Working on Test:  78A91A4E-4A51-4065-BDA7-94755F0BB3BB\n",
      "Working on Test:  7CE37510-56D0-4120-A1CF-0E23351428D2\n",
      "Working on Test:  7D9BB102-A612-4E2A-8E22-3159752F55D8\n",
      "Working on Test:  8023FE1A-D3B0-4E2C-A57A-9321B7FC755F\n",
      "Working on Test:  83CF687B-7CEC-434B-9FE8-00C3D5799BE6\n",
      "Working on Test:  9759096F-1119-4E19-A0AD-6F16989C7E1C\n",
      "Working on Test:  9DC38D04-E82E-4F29-AB52-B476535226F2\n",
      "Done for fold 2\n",
      "Working on Train:  00EABED2-271D-49D8-B599-1D4A09240601\n",
      "Working on Train:  098A72A5-E3E5-4F54-A152-BBDA0DF7B694\n",
      "Working on Train:  0A986513-7828-4D53-AA1F-E02D6DF9561B\n",
      "Working on Train:  11B5EC4D-4133-4289-B475-4E737182A406\n",
      "Working on Train:  136562B6-95B2-483D-88DC-065F28409FD2\n",
      "Working on Train:  1538C99F-BA1E-4EFB-A949-6C7C47701B20\n",
      "Working on Train:  24E40C4C-A349-4F9F-93AB-01D00FB994AF\n",
      "Working on Train:  2C32C23E-E30C-498A-8DD2-0EFB9150A02E\n",
      "Working on Train:  3600D531-0C55-44A7-AE95-A7A38519464E\n",
      "Working on Train:  4FC32141-E888-4BFF-8804-12559A491D8C\n",
      "Working on Train:  5119D0F8-FCA8-4184-A4EB-19421A40DE0D\n",
      "Working on Train:  5152A2DF-FAF3-4BA8-9CA9-E66B32671A53\n",
      "Working on Train:  5EF64122-B513-46AE-BCF1-E62AAC285D2C\n",
      "Working on Train:  61976C24-1C50-4355-9C49-AAE44A7D09F6\n",
      "Working on Train:  78A91A4E-4A51-4065-BDA7-94755F0BB3BB\n",
      "Working on Train:  7CE37510-56D0-4120-A1CF-0E23351428D2\n",
      "Working on Train:  7D9BB102-A612-4E2A-8E22-3159752F55D8\n",
      "Working on Train:  8023FE1A-D3B0-4E2C-A57A-9321B7FC755F\n",
      "Working on Train:  83CF687B-7CEC-434B-9FE8-00C3D5799BE6\n",
      "Working on Train:  9759096F-1119-4E19-A0AD-6F16989C7E1C\n",
      "Working on Train:  9DC38D04-E82E-4F29-AB52-B476535226F2\n",
      "Working on Train:  CDA3BBF7-6631-45E8-85BA-EEB416B32A3C\n",
      "Working on Train:  CF722AA9-2533-4E51-9FEB-9EAC84EE9AAC\n",
      "Working on Train:  D7D20E2E-FC78-405D-B346-DBD3FD8FC92B\n",
      "Working on Train:  E65577C1-8D5D-4F70-AF23-B3ADB9D3DBA3\n",
      "Working on Train:  ECECC2AB-D32F-4F90-B74C-E12A1C69BBE2\n",
      "Working on Train:  FDAA70A1-42A3-4E3F-9AE3-3FDA412E03BF\n",
      "Working on Train:  0BFC35E2-4817-4865-BFA7-764742302A2D\n",
      "Working on Train:  0E6184E1-90C0-48EE-B25A-F1ECB7B9714E\n",
      "Working on Train:  1155FF54-63D3-4AB2-9863-8385D0BD0A13\n",
      "Working on Train:  1DBB0F6F-1F81-4A50-9DF4-CD62ACFA4842\n",
      "Working on Train:  27E04243-B138-4F40-A164-F40B60165CF3\n",
      "Working on Train:  33A85C34-CFE4-4732-9E73-0A7AC861B27A\n",
      "Working on Train:  40E170A7-607B-4578-AF04-F021C3B0384A\n",
      "Working on Train:  481F4DD2-7689-43B9-A2AA-C8772227162B\n",
      "Working on Train:  4E98F91F-4654-42EF-B908-A3389443F2E7\n",
      "Working on Train:  59818CD2-24D7-4D32-B133-24C2FE3801E5\n",
      "Working on Train:  59EEFAE0-DEB0-4FFF-9250-54D2A03D0CF2\n",
      "Working on Train:  61359772-D8D8-480D-B623-7C636EAD0C81\n",
      "Working on Train:  665514DE-49DC-421F-8DCB-145D0B2609AD\n",
      "Working on Train:  74B86067-5D4B-43CF-82CF-341B76BEA0F4\n",
      "Working on Train:  797D145F-3858-4A7F-A7C2-A4EB721E133C\n",
      "Working on Train:  A5A30F76-581E-4757-97A2-957553A2C6AA\n",
      "Working on Train:  A7599A50-24AE-46A6-8EA6-2576F1011D81\n",
      "Working on Train:  B7F9D634-263E-4A97-87F9-6FFB4DDCB36C\n",
      "Working on Train:  BEF6C611-50DA-4971-A040-87FB979F3FC1\n",
      "Working on Train:  CCAF77F0-FABB-4F2F-9E24-D56AD0C5A82F\n",
      "Working on Train:  F50235E0-DD67-4F2A-B00B-1F31ADA998B9\n",
      "Working on Test:  806289BC-AD52-4CC1-806C-0CDB14D65EB6\n",
      "Working on Test:  81536B0A-8DBF-4D8A-AC24-9543E2E4C8E0\n",
      "Working on Test:  86A4F379-B305-473D-9D83-FC7D800180EF\n",
      "Working on Test:  96A358A0-FFF2-4239-B93E-C7425B901B47\n",
      "Working on Test:  99B204C0-DD5C-4BB7-83E8-A37281B8D769\n",
      "Working on Test:  A5CDF89D-02A2-4EC1-89F8-F534FDABDD96\n",
      "Working on Test:  A76A5AF5-5A93-4CF2-A16E-62353BB70E8A\n",
      "Working on Test:  B09E373F-8A54-44C8-895B-0039390B859F\n",
      "Working on Test:  B9724848-C7E2-45F4-9B3F-A1F38D864495\n",
      "Working on Test:  BE3CA5A6-A561-4BBD-B7C9-5DF6805400FC\n",
      "Working on Test:  C48CE857-A0DD-4DDB-BEA5-3A25449B2153\n",
      "Working on Test:  CA820D43-E5E2-42EF-9798-BE56F776370B\n",
      "Done for fold 3\n",
      "Working on Train:  00EABED2-271D-49D8-B599-1D4A09240601\n",
      "Working on Train:  098A72A5-E3E5-4F54-A152-BBDA0DF7B694\n",
      "Working on Train:  0A986513-7828-4D53-AA1F-E02D6DF9561B\n",
      "Working on Train:  11B5EC4D-4133-4289-B475-4E737182A406\n",
      "Working on Train:  136562B6-95B2-483D-88DC-065F28409FD2\n",
      "Working on Train:  1538C99F-BA1E-4EFB-A949-6C7C47701B20\n",
      "Working on Train:  24E40C4C-A349-4F9F-93AB-01D00FB994AF\n",
      "Working on Train:  2C32C23E-E30C-498A-8DD2-0EFB9150A02E\n",
      "Working on Train:  3600D531-0C55-44A7-AE95-A7A38519464E\n",
      "Working on Train:  4FC32141-E888-4BFF-8804-12559A491D8C\n",
      "Working on Train:  5119D0F8-FCA8-4184-A4EB-19421A40DE0D\n",
      "Working on Train:  5152A2DF-FAF3-4BA8-9CA9-E66B32671A53\n",
      "Working on Train:  5EF64122-B513-46AE-BCF1-E62AAC285D2C\n",
      "Working on Train:  61976C24-1C50-4355-9C49-AAE44A7D09F6\n",
      "Working on Train:  78A91A4E-4A51-4065-BDA7-94755F0BB3BB\n",
      "Working on Train:  7CE37510-56D0-4120-A1CF-0E23351428D2\n",
      "Working on Train:  7D9BB102-A612-4E2A-8E22-3159752F55D8\n",
      "Working on Train:  8023FE1A-D3B0-4E2C-A57A-9321B7FC755F\n",
      "Working on Train:  83CF687B-7CEC-434B-9FE8-00C3D5799BE6\n",
      "Working on Train:  9759096F-1119-4E19-A0AD-6F16989C7E1C\n",
      "Working on Train:  9DC38D04-E82E-4F29-AB52-B476535226F2\n",
      "Working on Train:  A5CDF89D-02A2-4EC1-89F8-F534FDABDD96\n",
      "Working on Train:  A76A5AF5-5A93-4CF2-A16E-62353BB70E8A\n",
      "Working on Train:  B09E373F-8A54-44C8-895B-0039390B859F\n",
      "Working on Train:  B9724848-C7E2-45F4-9B3F-A1F38D864495\n",
      "Working on Train:  BE3CA5A6-A561-4BBD-B7C9-5DF6805400FC\n",
      "Working on Train:  C48CE857-A0DD-4DDB-BEA5-3A25449B2153\n",
      "Working on Train:  CA820D43-E5E2-42EF-9798-BE56F776370B\n",
      "Working on Train:  0BFC35E2-4817-4865-BFA7-764742302A2D\n",
      "Working on Train:  0E6184E1-90C0-48EE-B25A-F1ECB7B9714E\n",
      "Working on Train:  1155FF54-63D3-4AB2-9863-8385D0BD0A13\n",
      "Working on Train:  1DBB0F6F-1F81-4A50-9DF4-CD62ACFA4842\n",
      "Working on Train:  27E04243-B138-4F40-A164-F40B60165CF3\n",
      "Working on Train:  33A85C34-CFE4-4732-9E73-0A7AC861B27A\n",
      "Working on Train:  40E170A7-607B-4578-AF04-F021C3B0384A\n",
      "Working on Train:  481F4DD2-7689-43B9-A2AA-C8772227162B\n",
      "Working on Train:  4E98F91F-4654-42EF-B908-A3389443F2E7\n",
      "Working on Train:  59818CD2-24D7-4D32-B133-24C2FE3801E5\n",
      "Working on Train:  59EEFAE0-DEB0-4FFF-9250-54D2A03D0CF2\n",
      "Working on Train:  61359772-D8D8-480D-B623-7C636EAD0C81\n",
      "Working on Train:  665514DE-49DC-421F-8DCB-145D0B2609AD\n",
      "Working on Train:  74B86067-5D4B-43CF-82CF-341B76BEA0F4\n",
      "Working on Train:  797D145F-3858-4A7F-A7C2-A4EB721E133C\n",
      "Working on Train:  806289BC-AD52-4CC1-806C-0CDB14D65EB6\n",
      "Working on Train:  81536B0A-8DBF-4D8A-AC24-9543E2E4C8E0\n",
      "Working on Train:  86A4F379-B305-473D-9D83-FC7D800180EF\n",
      "Working on Train:  96A358A0-FFF2-4239-B93E-C7425B901B47\n",
      "Working on Train:  99B204C0-DD5C-4BB7-83E8-A37281B8D769\n",
      "Working on Test:  A5A30F76-581E-4757-97A2-957553A2C6AA\n",
      "Working on Test:  A7599A50-24AE-46A6-8EA6-2576F1011D81\n",
      "Working on Test:  B7F9D634-263E-4A97-87F9-6FFB4DDCB36C\n",
      "Working on Test:  BEF6C611-50DA-4971-A040-87FB979F3FC1\n",
      "Working on Test:  CCAF77F0-FABB-4F2F-9E24-D56AD0C5A82F\n",
      "Working on Test:  F50235E0-DD67-4F2A-B00B-1F31ADA998B9\n",
      "Working on Test:  CDA3BBF7-6631-45E8-85BA-EEB416B32A3C\n",
      "Working on Test:  CF722AA9-2533-4E51-9FEB-9EAC84EE9AAC\n",
      "Working on Test:  D7D20E2E-FC78-405D-B346-DBD3FD8FC92B\n",
      "Working on Test:  E65577C1-8D5D-4F70-AF23-B3ADB9D3DBA3\n",
      "Working on Test:  ECECC2AB-D32F-4F90-B74C-E12A1C69BBE2\n",
      "Working on Test:  FDAA70A1-42A3-4E3F-9AE3-3FDA412E03BF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for fold 4\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Skipping cell if the data files were already created previously \n",
    "if done!=1:\n",
    "    # Reading data in the directory (Stacked)\n",
    "    \n",
    "    #M_train_t=np.empty((0,51))\n",
    "    #M_test_t=np.empty((0,51))\n",
    "    for fold_n in [0,1,2,3,4]:\n",
    "        #X_train_t=np.empty((0,170))\n",
    "        Y_train_t=np.empty((0,51))\n",
    "        #X_test_t=np.empty((0,170))\n",
    "        Y_test_t=np.empty((0,51))\n",
    "        train = glob.glob(cross_validation_user_loc+'fold_%d_train_*_uuids.txt'%fold_n)\n",
    "        test = glob.glob(cross_validation_user_loc+'fold_%d_test_*_uuids.txt'%fold_n)\n",
    "        for tr in train:\n",
    "            with open(tr,'r') as file:\n",
    "                for line in file:\n",
    "                    line = line.replace(\"\\n\",\"\")\n",
    "                    print(\"Working on Train: \",line)\n",
    "\n",
    "                    with open(ssl_user_loc+line+'_ssl_2Hdrop.pkl','rb') as f:\n",
    "                        y_ssl=pickle.load(f)\n",
    "                    f.close()\n",
    "                    \n",
    "                    y_ssl[y_ssl < 1e-1] = 0\n",
    "                    unique,counts=np.unique(y_ssl,return_counts=True)\n",
    "                    if(len(unique)>3):\n",
    "                        print(*zip(unique,counts),\"\\n\")\n",
    "                    #print(y_ssl.shape)\n",
    "                    Y_train_t=np.vstack((Y_train_t,y_ssl))\n",
    "        \n",
    "        for te in test:\n",
    "            with open(te,'r') as file:\n",
    "                for line in file:\n",
    "                    line = line.replace(\"\\n\",\"\")\n",
    "                    print(\"Working on Test: \",line)\n",
    "                    \n",
    "                    with open(ssl_user_loc+line+'_ssl_2Hdrop.pkl','rb') as f:\n",
    "                        y_ssl_t=pickle.load(f)\n",
    "                    f.close()\n",
    "                    y_ssl_t[y_ssl_t < 1e-1] = 0\n",
    "                    unique,counts=np.unique(y_ssl_t,return_counts=True)\n",
    "                    if(len(unique)>3):\n",
    "                        print(*zip(unique,counts),\"\\n\")\n",
    "                    \n",
    "                    Y_test_t=np.vstack((Y_test_t,y_ssl_t))\n",
    "        with open('dataset/pickled/semisupervised_2Hdrop/ytrain_{}.pkl'.format(fold_n),'wb') as f:\n",
    "            pickle.dump(Y_train_t,f)\n",
    "#         with open('dataset/pickled/orig_logicimputelabel/xtest_{}.pkl'.format(fold_n),'wb') as f:\n",
    "#             pickle.dump(X_test_t,f)\n",
    "        with open('dataset/pickled/semisupervised_2Hdrop/ytest_{}.pkl'.format(fold_n),'wb') as f:\n",
    "            pickle.dump(Y_test_t,f)\n",
    "        print(\"Done for fold {}\".format(fold_n))\n",
    "    print (\"DONE\") \n",
    "else:\n",
    "    print(\"Skipping step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Skipping cell if the data files were already created previously \n",
    "if done!=1:\n",
    "    # Reading data in the directory (Stacked)\n",
    "    \n",
    "    #M_train_t=np.empty((0,51))\n",
    "    #M_test_t=np.empty((0,51))\n",
    "    for fold_n in [0,1,2,3,4]:\n",
    "        X_train_t=np.empty((0,170))\n",
    "        Y_train_t=np.empty((0,51))\n",
    "        X_test_t=np.empty((0,170))\n",
    "        Y_test_t=np.empty((0,51))\n",
    "        train = glob.glob(cross_validation_user_loc+'fold_%d_train_*_uuids.txt'%fold_n)\n",
    "        test = glob.glob(cross_validation_user_loc+'fold_%d_test_*_uuids.txt'%fold_n)\n",
    "        for tr in train:\n",
    "            with open(tr,'r') as file:\n",
    "                for line in file:\n",
    "                    line = line.replace(\"\\n\",\"\")\n",
    "                    print(\"Working on Train: \",line)\n",
    "                    \n",
    "                    (x_user_train,y_user_train,missed_label_user,tstamp_user,featurename_user,labelname_user) = read_user_data(prefix+line+'.features_labels.csv.gz')\n",
    "#                     x_sh=x_user_train.shape\n",
    "#                     y_sh=y_user_train.shape\n",
    "                    \n",
    "                    x_user_train=np.nan_to_num(x_user_train)\n",
    "                    \n",
    "                    \n",
    "                    # New method of imputation\n",
    "#                     tstamp_user_df=[pd.Timestamp(datetime.datetime.fromtimestamp(x).strftime(\"%x %X\")) for x in tstamp_user]\n",
    "#                     x_user_train_df=pd.DataFrame(x_user_train,columns=featurename_user)\n",
    "#                     x_user_train_df.insert(0,\"Datetime\",tstamp_user_df,True) \n",
    "#                     x_user_train_df=x_user_train_df.set_index('Datetime')\n",
    "#                     x_user_train_df.interpolate(limit_direction='forward',method='time')\n",
    "#                     x_user_train_df.interpolate()\n",
    "#                     x_user_train_df.fillna(0)\n",
    "                    # End new method of imputation\n",
    "    \n",
    "#                     x_user_train=x_user_train_df.values # Date-time column is an index (doesn't have to be ignored)\n",
    "#                     x_user_train=np.nan_to_num(x_user_train)\n",
    "                    #x_user_train=choose_sensors(x_user_train,used_sensors=sensor_types,summarized_feature_names=feature_names)\n",
    "                    #x_user_train=IterativeImputer().fit_transform(x_user_train)\n",
    "#                     x_user_train=KNN(k=3).fit_transform(x_user_train)\n",
    "#                     x_user_train=SoftImpute().fit_transform(x_user_train)\n",
    "\n",
    "#                     with open(arimakalman_loc+line+'_x.pkl','rb') as f:\n",
    "#                         x_user_train=pickle.load(f).astype(np.float)\n",
    "#                     with open(arimakalman_loc+line+'_y.pkl','rb') as f:\n",
    "#                         y_user_train=pickle.load(f).astype(np.float)\n",
    "                    #y_df=pd.read_csv(yimpute_loc+line+'.csv',header=0,index_col=0)\n",
    "                    #y_user_train=y_df.values\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                    #x_user_train=np.nan_to_num(x_user_train)\n",
    "                    # Remove outliers\n",
    "#                     df_x_user=pd.DataFrame(x_user_train)\n",
    "#                     df_y_user=pd.DataFrame(y_user_train,columns=labelname_user)\n",
    "#                     df_x_trim,df_y_trim=remove_outliers(df_x=df_x_user,df_y=df_y_user,method='iqrwhiskers')\n",
    "                    # End remove outlier\n",
    "\n",
    "                    #X_train_t=np.vstack((X_train_t,x_user_train)) # Removing the first index columns\n",
    "                    y_ssl=s3vm(x_user_train,y_user_train)\n",
    "                    Y_train_t=np.vstack((Y_train_t,y_ssl))\n",
    "        \n",
    "#         X_train_t,mean,dev=standardize(X_train_t)\n",
    "#         assert len(X_train_t)==len(Y_train_t)\n",
    "#         print('\\nTraining: Fold::{} X::{} ,Y::{}'.format(fold_n,X_train_t.shape,Y_train_t.shape))\n",
    "\n",
    "        for te in test:\n",
    "            with open(te,'r') as file:\n",
    "                for line in file:\n",
    "                    line = line.replace(\"\\n\",\"\")\n",
    "                    print(\"Working on Test: \",line)\n",
    "                    (x_user_test,y_user_test,missed_label_user,tstamp_user,featurename_user,labelname_user) = read_user_data(prefix+line+'.features_labels.csv.gz')\n",
    "#                     x_sh=x_user_test.shape\n",
    "#                     y_sh=y_user_test.shape\n",
    "                    \n",
    "                    x_user_test=np.nan_to_num(x_user_test)\n",
    "                    \n",
    "\n",
    "                    # New method of imputation    \n",
    "#                     tstamp_user_df=[pd.Timestamp(datetime.datetime.fromtimestamp(x).strftime(\"%x %X\")) for x in tstamp_user]\n",
    "#                     x_user_test_df=pd.DataFrame(x_user_test,columns=featurename_user)\n",
    "#                     x_user_test_df.insert(0,\"Datetime\",tstamp_user_df,True) \n",
    "#                     x_user_test_df=x_user_test_df.set_index('Datetime')\n",
    "#                     x_user_test_df.interpolate(limit_direction='forward',method='time')\n",
    "#                     x_user_test_df.interpolate()\n",
    "#                     x_user_test_df.fillna(0)\n",
    "                    # End new method of imputation\n",
    "                    \n",
    "#                     x_user_test=x_user_test_df.values\n",
    "                    \n",
    "                    x_user_test=choose_sensors(x_user_test,used_sensors=sensor_types,summarized_feature_names=feature_names)\n",
    "#                     x_user_test=IterativeImputer().transform(x_user_test)\n",
    "#                     x_user_test=KNN(k=3).fit_transform(x_user_test)\n",
    "#                     x_user_test=SoftImpute().fit_transform(x_user_test)\n",
    "#                     with open(arimakalman_loc+line+'_x.pkl','rb') as f:\n",
    "#                         x_user_test=pickle.load(f).astype(np.float)\n",
    "#                     with open(arimakalman_loc+line+'_y.pkl','rb') as f:\n",
    "#                         y_user_test=pickle.load(f).astype(np.float)\n",
    "#                     y_df=pd.read_csv(yimpute_loc+line+'.csv',header=0,index_col=0)\n",
    "#                     y_user_test=y_df.values\n",
    "#                     x_user_test=np.nan_to_num(x_user_test)\n",
    "                    # Remove outliers\n",
    "#                     df_x_user=pd.DataFrame(x_user_test)\n",
    "#                     df_y_user=pd.DataFrame(y_user_test,columns=labelname_user)\n",
    "#                     df_x_trim,df_y_trim=remove_outliers(df_x=df_x_user,df_y=df_y_user,method='iqrwhiskers')\n",
    "                    # End remove outlier\n",
    "                    y_ssl=s3vm(x_user_test,y_user_test)\n",
    "                    Y_test_t=np.vstack((Y_test_t,y_ssl))\n",
    "#                     X_test_t=np.vstack((X_test_t,x_user_test)) # Removing the first index columns\n",
    "#                     Y_test_t=np.vstack((Y_test_t,y_user_test))\n",
    "            \n",
    "#         X_test_t=(X_test_t-mean)/dev\n",
    "\n",
    "#         assert len(X_test_t)==len(Y_test_t)\n",
    "#         print('\\nTesting: Fold::{} X::{} ,Y::{}'.format(fold_n,X_test_t.shape,Y_test_t.shape))\n",
    "        \n",
    "#         print(\"Pickling data files\")\n",
    "        # Split datasets\n",
    "#         with open('dataset/pickled/orig_logicimputelabel/xtrain_{}.pkl'.format(fold_n),'wb') as f:\n",
    "#             pickle.dump(X_train_t,f)\n",
    "        with open('dataset/pickled/semisupervised_labels/ytrain_{}.pkl'.format(fold_n),'wb') as f:\n",
    "            pickle.dump(Y_train_t,f)\n",
    "#         with open('dataset/pickled/orig_logicimputelabel/xtest_{}.pkl'.format(fold_n),'wb') as f:\n",
    "#             pickle.dump(X_test_t,f)\n",
    "        with open('dataset/pickled/semisupervised_labels/ytest_{}.pkl'.format(fold_n),'wb') as f:\n",
    "            pickle.dump(Y_test_t,f)\n",
    "        print(\"Done for fold {}\".format(fold_n))\n",
    "    print (\"DONE\") \n",
    "else:\n",
    "    print(\"Skipping step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and pickling instance weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Creating an instance weight matrix for the training labels\n",
    "def instance_weight_matrix(y_train):\n",
    "    instance_weights=np.zeros_like(y_train)\n",
    "    for l in range(len(labelname_user)):\n",
    "        temp_column=y_train[:,l]\n",
    "        count_neg=0\n",
    "        count_0=0\n",
    "        count_1=0\n",
    "        for i in range(len(temp_column)): # n^2 bincount doesn't work with arrays consisting of negative numbers\n",
    "            if (temp_column[i]==-1):\n",
    "                count_neg+=1\n",
    "            elif (temp_column[i]==0):\n",
    "                count_0+=1\n",
    "            elif (temp_column[i]==1):\n",
    "                count_1+=1\n",
    "            else:\n",
    "                print(temp_column[i])\n",
    "                raise ValueError(\"Bad Loop\")\n",
    "#         print(l,count_0,count_1)\n",
    "        if(count_0!=0):\n",
    "            weight_0=float((count_0+count_1)/count_0)\n",
    "        else:\n",
    "            weight_0=0.\n",
    "        if(count_1!=0):\n",
    "            weight_1=float((count_0+count_1)/count_1)\n",
    "        else:\n",
    "            weight_1=0.\n",
    "        if(weight_0+weight_1==0.):\n",
    "            weight_0=1.\n",
    "            weight_1=1.\n",
    "        else:\n",
    "            weight_0=weight_0/(weight_0+weight_1)\n",
    "            weight_1=weight_1/(weight_0+weight_1)\n",
    "\n",
    "        for i in range(len(temp_column)):\n",
    "            if (temp_column[i]==-1):\n",
    "                instance_weights[i,l]=0.\n",
    "            elif (temp_column[i]==0):\n",
    "                instance_weights[i,l]=weight_0\n",
    "            elif (temp_column[i]==1):\n",
    "                instance_weights[i,l]=weight_1\n",
    "    return instance_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Skipping cell if the data files were already created previously \n",
    "if done!=1:\n",
    "    weights = dict()\n",
    "    for fold_n in [0,1,2,3,4]:\n",
    "        with open('dataset/pickled/semisupervised_2Hdrop/ytrain_{}.pkl'.format(fold_n),'rb') as f:\n",
    "            y_train=pickle.load(f)\n",
    "            #y_train=unclassified_labels(y_train)\n",
    "\n",
    "        weights[fold_n] = instance_weight_matrix(y_train)\n",
    "\n",
    "    for fold_n in [0,1,2,3,4]:\n",
    "        with open('dataset/pickled/semisupervised_2Hdrop/weights_{}.pkl'.format(fold_n),'wb') as f:\n",
    "            pickle.dump(weights[fold_n],f)\n",
    "else:\n",
    "    print(\"Skipping step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous train/test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda-enable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Simple function to run using GPU when available\n",
    "def C(structure):\n",
    "    if torch.cuda.is_available():\n",
    "        device=torch.device(\"cuda\")\n",
    "        return structure.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tackling missing labels using a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create a mask to hide -1 nans before training and then input to a train criterion\n",
    "def mask(criterion,y_true,y_pred,mask_value=-1.):\n",
    "    mask=torch.ne(y_true,mask_value).type(torch.cuda.FloatTensor)\n",
    "    # Cast the ByteTensor from elementwise comparison to a FloatTensor\n",
    "    return criterion(torch.mul(y_pred,mask),torch.mul(y_true,mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Learning-Rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Linear decreasing LR scheduler\n",
    "def linear_lr_scheduler(optimizer,epoch):\n",
    "    \"\"\"\n",
    "    LR_init=0.1, LR_final=0.01, n_epochs=40\n",
    "    Sets the learning rate to the initial LR decayed by 1.04 every epoch\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr=param_group['lr']\n",
    "    m=-3/1300\n",
    "    c=0.1\n",
    "    lr=(epoch*m)+c # Linear LR decay based on a set number of epochs\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr']=lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear decreasing LR scheduler\n",
    "def linear_lr_scheduler_2(optimizer,epoch):\n",
    "    \"\"\"\n",
    "    LR_init=0.1, LR_final=0.01, n_epochs=40\n",
    "    Sets the learning rate to the initial LR decayed by 1.04 every epoch\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr=param_group['lr']\n",
    "    m=-33/1300000\n",
    "    c=1333/1300000\n",
    "    lr=(epoch*m)+c # Linear LR decay based on a set number of epochs\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr']=lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Euclidean Norm for weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Adds euclidean regularization to weight matrices\n",
    "def frobenius_norm(model,loss):\n",
    "    regularizer_loss=0\n",
    "    \n",
    "    for m in model.modules():\n",
    "        if isinstance(m,nn.Linear): # Linear layer\n",
    "            frobenius_norm=torch.norm(m.weight,p='fro')\n",
    "            regularizer_loss+=frobenius_norm # Regularization over the weight matrices for linear layers\n",
    "    return loss+0.001*regularizer_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy (Precision, Recall, F1, Support, Balanced Accuracy) metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function for the required accuracy metrics per fold\n",
    "def accuracy(fold,target_labels,y_true,y_pred):\n",
    "    y_true=y_true.detach().numpy()\n",
    "    y_pred=y_pred.detach().numpy()\n",
    "    balanced_accuracy_dict={}\n",
    "    print('*'*20)\n",
    "    print('For fold {}'.format(fold))\n",
    "    \n",
    "    # Balanced accuracy\n",
    "    for i in range(len(target_labels)):\n",
    "        true_perlabel=y_true[:,i]\n",
    "        pred_perlabel=y_pred[:,i]\n",
    "        initial_shape=true_perlabel.shape\n",
    "        \n",
    "        invalid_mask=np.where(true_perlabel==-1.)\n",
    "        valid_mask=np.where(true_perlabel!=-1.) # Create a mask\n",
    "        true_perlabel=true_perlabel[valid_mask]\n",
    "        pred_perlabel=pred_perlabel[valid_mask]\n",
    "        \n",
    "        bal_acc=balanced_accuracy_score(y_true=true_perlabel,y_pred=pred_perlabel)\n",
    "        print('\\t Label {}:::-> Balanced Accuracy {}'.format(target_labels[i],round(bal_acc,7)))\n",
    "        print('\\t\\t Initial length {}, Missing mask length {}, Valid mask length {}, Final length {}'\n",
    "              .format(initial_shape[0],len(invalid_mask[0]),len(valid_mask[0]),len(true_perlabel)))\n",
    "        balanced_accuracy_dict[target_labels[i]]=round(bal_acc,5)\n",
    "    return balanced_accuracy_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12.2333px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "353.983px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "930.85px",
    "left": "1197.28px",
    "right": "41.7167px",
    "top": "104px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
