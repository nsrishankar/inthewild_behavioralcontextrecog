{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Dataset-parsers-and-cleaning-functions\" data-toc-modified-id=\"Dataset-parsers-and-cleaning-functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset parsers and cleaning functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-data\" data-toc-modified-id=\"Test-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Test data</a></span></li></ul></li><li><span><a href=\"#Training-Functions\" data-toc-modified-id=\"Training-Functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-a-new-data-structure-for-all-valid-data-and-pickling-it\" data-toc-modified-id=\"Creating-a-new-data-structure-for-all-valid-data-and-pickling-it-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Creating a new data structure for all valid data and pickling it</a></span></li><li><span><a href=\"#Creating-and-pickling-instance-weight-matrix\" data-toc-modified-id=\"Creating-and-pickling-instance-weight-matrix-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Creating and pickling instance weight matrix</a></span></li><li><span><a href=\"#Loading-data-and-instance-weight-matrix\" data-toc-modified-id=\"Loading-data-and-instance-weight-matrix-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Loading data and instance weight matrix</a></span></li><li><span><a href=\"#Miscellaneous-train/test-functions\" data-toc-modified-id=\"Miscellaneous-train/test-functions-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Miscellaneous train/test functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cuda-enable\" data-toc-modified-id=\"Cuda-enable-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Cuda-enable</a></span></li><li><span><a href=\"#Tackling-missing-labels-using-a-mask\" data-toc-modified-id=\"Tackling-missing-labels-using-a-mask-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Tackling missing labels using a mask</a></span></li><li><span><a href=\"#Linear-Learning-Rate-scheduler\" data-toc-modified-id=\"Linear-Learning-Rate-scheduler-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>Linear Learning-Rate scheduler</a></span></li><li><span><a href=\"#Euclidean-Norm-for-weight-matrices\" data-toc-modified-id=\"Euclidean-Norm-for-weight-matrices-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span>Euclidean Norm for weight matrices</a></span></li><li><span><a href=\"#Accuracy-(Precision,-Recall,-F1,-Support,-Balanced-Accuracy)-metrics\" data-toc-modified-id=\"Accuracy-(Precision,-Recall,-F1,-Support,-Balanced-Accuracy)-metrics-3.4.5\"><span class=\"toc-item-num\">3.4.5&nbsp;&nbsp;</span>Accuracy (Precision, Recall, F1, Support, Balanced Accuracy) metrics</a></span></li><li><span><a href=\"#K-Fold-cross-validation\" data-toc-modified-id=\"K-Fold-cross-validation-3.4.6\"><span class=\"toc-item-num\">3.4.6&nbsp;&nbsp;</span>K-Fold cross validation</a></span></li><li><span><a href=\"#Train/Fit-function\" data-toc-modified-id=\"Train/Fit-function-3.4.7\"><span class=\"toc-item-num\">3.4.7&nbsp;&nbsp;</span>Train/Fit function</a></span></li></ul></li></ul></li><li><span><a href=\"#Multi-Class-Classifier:-Learning-and-Evaluation\" data-toc-modified-id=\"Multi-Class-Classifier:-Learning-and-Evaluation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Multi Class Classifier: Learning and Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Global-Hyperparameter-variables\" data-toc-modified-id=\"Global-Hyperparameter-variables-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Global Hyperparameter variables</a></span></li><li><span><a href=\"#Multi-Layer-Perceptron-(0-Hidden-Layers)\" data-toc-modified-id=\"Multi-Layer-Perceptron-(0-Hidden-Layers)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Multi-Layer Perceptron (0 Hidden Layers)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Balanced-Accuracy-outputs\" data-toc-modified-id=\"Balanced-Accuracy-outputs-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Balanced Accuracy outputs</a></span></li><li><span><a href=\"#Saving-data\" data-toc-modified-id=\"Saving-data-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Saving data</a></span></li></ul></li><li><span><a href=\"#Multi-Layer-Perceptron-(1-Hidden-Layer)\" data-toc-modified-id=\"Multi-Layer-Perceptron-(1-Hidden-Layer)-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Multi-Layer Perceptron (1 Hidden Layer)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Balanced-Accuracy-outputs\" data-toc-modified-id=\"Balanced-Accuracy-outputs-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Balanced Accuracy outputs</a></span></li><li><span><a href=\"#Saving-data\" data-toc-modified-id=\"Saving-data-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Saving data</a></span></li></ul></li><li><span><a href=\"#Multi-Layer-Perceptron-(2-Hidden-Layers)\" data-toc-modified-id=\"Multi-Layer-Perceptron-(2-Hidden-Layers)-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Multi-Layer Perceptron (2 Hidden Layers)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Balanced-Accuracy-outputs\" data-toc-modified-id=\"Balanced-Accuracy-outputs-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Balanced Accuracy outputs</a></span></li><li><span><a href=\"#Balanced-Accuracy-outputs\" data-toc-modified-id=\"Balanced-Accuracy-outputs-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Balanced Accuracy outputs</a></span></li></ul></li><li><span><a href=\"#Multi-Layer-Perceptron-(2-Hidden-Layers,-with-Dropout)\" data-toc-modified-id=\"Multi-Layer-Perceptron-(2-Hidden-Layers,-with-Dropout)-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Multi-Layer Perceptron (2 Hidden Layers, with Dropout)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import glob\n",
    "import pickle\n",
    "import copy\n",
    "from io import StringIO\n",
    "import importlib.machinery\n",
    "\n",
    "from sklearn.model_selection import train_test_split as TT_split\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier as OvR\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,balanced_accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support,classification_report\n",
    "#from sklearn.metrics import multilabel_confusion_matrix # Only available in dev .21\n",
    "\n",
    "# Need Pytorch for multilabel classifications\n",
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "#import skorch [Scikit-learn wrapper around Pytorch so allowing for K-fold cross-validation]\n",
    "from sklearn.model_selection import KFold\n",
    "random_state=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Data location and sample user\n",
    "prefix='dataset/Extrasensory_uuid_fl_uTAR/'\n",
    "cross_validation_user_loc='dataset/cv_5_folds/'\n",
    "user_sample='3600D531-0C55-44A7-AE95-A7A38519464E.features_labels'\n",
    "done=0 # Pickled files are created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset parsers and cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Dataset parsers for header/ body for CSVs\n",
    "def parse_header_of_csv(csv_str):\n",
    "    # Isolate the headline columns:\n",
    "    headline = csv_str[:csv_str.index('\\n')];\n",
    "    columns = headline.split(',');\n",
    "\n",
    "    # The first column should be timestamp:\n",
    "    assert columns[0] == 'timestamp';\n",
    "    # The last column should be label_source:\n",
    "    assert columns[-1] == 'label_source';\n",
    "    \n",
    "    # Search for the column of the first label:\n",
    "    for (ci,col) in enumerate(columns):\n",
    "        if col.startswith('label:'):\n",
    "            first_label_ind = ci;\n",
    "            break;\n",
    "        pass;\n",
    "\n",
    "    # Feature columns come after timestamp and before the labels:\n",
    "    feature_names = columns[1:first_label_ind];\n",
    "    # Then come the labels, till the one-before-last column:\n",
    "    label_names = columns[first_label_ind:-1];\n",
    "    for (li,label) in enumerate(label_names):\n",
    "        # In the CSV the label names appear with prefix 'label:', but we don't need it after reading the data:\n",
    "        assert label.startswith('label:');\n",
    "        label_names[li] = label.replace('label:','');\n",
    "        pass;\n",
    "    \n",
    "    return (feature_names,label_names);\n",
    "\n",
    "def parse_body_of_csv(csv_str,n_features):\n",
    "    # Read the entire CSV body into a single numeric matrix:\n",
    "    full_table = np.loadtxt(StringIO(csv_str),delimiter=',',skiprows=1);\n",
    "    \n",
    "    # Timestamp is the primary key for the records (examples):\n",
    "    timestamps = full_table[:,0].astype(int);\n",
    "    \n",
    "    # Read the sensor features:\n",
    "    X = full_table[:,1:(n_features+1)];\n",
    "    \n",
    "    # Read the binary label values, and the 'missing label' indicators:\n",
    "    trinary_labels_mat = full_table[:,(n_features+1):-1]; # This should have values of either 0., 1. or NaN\n",
    "    M = np.isnan(trinary_labels_mat); # M is the missing label matrix\n",
    "    \n",
    "    #print(\"M matrix shape:\",M.shape)\n",
    "    #print(\"Matrix: \",np.argwhere(M))\n",
    "    \n",
    "    Y = np.where(M,0,trinary_labels_mat) > 0.; # Y is the label matrix\n",
    "    \n",
    "    return (X,Y,M,timestamps);\n",
    "\n",
    "def read_user_data(directory):\n",
    "    print('Reading {}'.format(directory.split(\"/\")[-1]))\n",
    "\n",
    "    # Read the entire csv file of the user:\n",
    "    with gzip.open(directory,'rb') as fid:\n",
    "        csv_str = fid.read();\n",
    "        csv_str = csv_str.decode(\"utf-8\")\n",
    "        pass;\n",
    "\n",
    "    (feature_names,label_names) = parse_header_of_csv(csv_str);\n",
    "    n_features = len(feature_names);\n",
    "    (X,Y,M,timestamps) = parse_body_of_csv(csv_str,n_features);\n",
    "\n",
    "    return (X,Y,M,timestamps,feature_names,label_names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Clean labels\n",
    "def clean_labels(input_label):\n",
    "    if label.endswith('_'):\n",
    "        label=label[:-1]+')'\n",
    "    label=label.replace('__',' (').replace('_',' ')\n",
    "    label=label[0]+label[1:].lower()\n",
    "    label=label.replace('i m','I\\'m')\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Get a summary of the sensor feature\n",
    "'''\n",
    "# Summarize features as we are only using phone_acc,phone_gyro,phone_mag,phone_loc,phone_audio,\n",
    "# phone_app,phone_battery,phone_use,phone_callstat,phone_wifi,phone_lf,phone_time\n",
    "# We are ignoring the use of the smartwatch features. There are definitely features that will be used\n",
    "# much more (e.g. than the phone_callstat) but we'll leave that up to the ML algorithm.\n",
    "'''\n",
    "def summarize_features(feature_list):\n",
    "    summary_feature_list=np.empty_like(feature_list)\n",
    "    for (ind,feature) in enumerate(feature_list):\n",
    "        if feature.startswith('raw_acc'):\n",
    "            summary_feature_list[ind]='phone_acc' \n",
    "        if feature.startswith('proc_gyro'):\n",
    "            summary_feature_list[ind]='phone_gyro'\n",
    "        if feature.startswith('raw_magnet'):\n",
    "            summary_feature_list[ind]='phone_mag'\n",
    "        if feature.startswith('watch_acc'):\n",
    "            summary_feature_list[ind]='watch_acc'\n",
    "        if feature.startswith('watch_heading'):\n",
    "            summary_feature_list[ind]='watch_dir'\n",
    "        if feature.startswith('location'):\n",
    "            summary_feature_list[ind]='phone_loc'\n",
    "        if feature.startswith('audio_naive'):\n",
    "            summary_feature_list[ind]='phone_audio'\n",
    "        if feature.startswith('discrete:app_state'):\n",
    "            summary_feature_list[ind]='phone_app'\n",
    "        if feature.startswith('discrete:battery'):\n",
    "            summary_feature_list[ind]='phone_battery'\n",
    "        if feature.startswith('discrete:on'):\n",
    "            summary_feature_list[ind]='phone_use'\n",
    "        if feature.startswith('discrete:ringer'):\n",
    "            summary_feature_list[ind]='phone_callstat'\n",
    "        if feature.startswith('discrete:wifi'):\n",
    "            summary_feature_list[ind]='phone_wifi'\n",
    "        if feature.startswith('lf'):\n",
    "            summary_feature_list[ind]='phone_lf'\n",
    "        if feature.startswith('discrete:time'):\n",
    "            summary_feature_list[ind]='phone_time'\n",
    "\n",
    "    return summary_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Custom dictionary class with help for duplicate keys\n",
    "class Customdictionary(dict):\n",
    "    def __setitem__(self,key,value):\n",
    "        try:\n",
    "            self[key]\n",
    "        except KeyError:\n",
    "            super(Customdictionary,self).__setitem__(key,[])\n",
    "        self[key].append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 3600D531-0C55-44A7-AE95-A7A38519464E.features_labels.csv.gz\n",
      "Data shape input for user (Len minutes/num examples, num sensors):  (5203, 225)\n",
      "Label shape for user (Len minutes, num labels):  (5203, 51) \n",
      "\n",
      "Sensor feature names:\n",
      "\n",
      "0 :: phone_acc ::--> raw_acc:magnitude_stats:mean\n",
      "\n",
      "1 :: phone_acc ::--> raw_acc:magnitude_stats:std\n",
      "\n",
      "2 :: phone_acc ::--> raw_acc:magnitude_stats:moment3\n",
      "\n",
      "3 :: phone_acc ::--> raw_acc:magnitude_stats:moment4\n",
      "\n",
      "4 :: phone_acc ::--> raw_acc:magnitude_stats:percentile25\n",
      "\n",
      "5 :: phone_acc ::--> raw_acc:magnitude_stats:percentile50\n",
      "\n",
      "6 :: phone_acc ::--> raw_acc:magnitude_stats:percentile75\n",
      "\n",
      "7 :: phone_acc ::--> raw_acc:magnitude_stats:value_entropy\n",
      "\n",
      "8 :: phone_acc ::--> raw_acc:magnitude_stats:time_entropy\n",
      "\n",
      "9 :: phone_acc ::--> raw_acc:magnitude_spectrum:log_energy_band0\n",
      "\n",
      "10 :: phone_acc ::--> raw_acc:magnitude_spectrum:log_energy_band1\n",
      "\n",
      "11 :: phone_acc ::--> raw_acc:magnitude_spectrum:log_energy_band2\n",
      "\n",
      "12 :: phone_acc ::--> raw_acc:magnitude_spectrum:log_energy_band3\n",
      "\n",
      "13 :: phone_acc ::--> raw_acc:magnitude_spectrum:log_energy_band4\n",
      "\n",
      "14 :: phone_acc ::--> raw_acc:magnitude_spectrum:spectral_entropy\n",
      "\n",
      "15 :: phone_acc ::--> raw_acc:magnitude_autocorrelation:period\n",
      "\n",
      "16 :: phone_acc ::--> raw_acc:magnitude_autocorrelation:normalized_ac\n",
      "\n",
      "17 :: phone_acc ::--> raw_acc:3d:mean_x\n",
      "\n",
      "18 :: phone_acc ::--> raw_acc:3d:mean_y\n",
      "\n",
      "19 :: phone_acc ::--> raw_acc:3d:mean_z\n",
      "\n",
      "20 :: phone_acc ::--> raw_acc:3d:std_x\n",
      "\n",
      "21 :: phone_acc ::--> raw_acc:3d:std_y\n",
      "\n",
      "22 :: phone_acc ::--> raw_acc:3d:std_z\n",
      "\n",
      "23 :: phone_acc ::--> raw_acc:3d:ro_xy\n",
      "\n",
      "24 :: phone_acc ::--> raw_acc:3d:ro_xz\n",
      "\n",
      "25 :: phone_acc ::--> raw_acc:3d:ro_yz\n",
      "\n",
      "26 :: phone_gyro ::--> proc_gyro:magnitude_stats:mean\n",
      "\n",
      "27 :: phone_gyro ::--> proc_gyro:magnitude_stats:std\n",
      "\n",
      "28 :: phone_gyro ::--> proc_gyro:magnitude_stats:moment3\n",
      "\n",
      "29 :: phone_gyro ::--> proc_gyro:magnitude_stats:moment4\n",
      "\n",
      "30 :: phone_gyro ::--> proc_gyro:magnitude_stats:percentile25\n",
      "\n",
      "31 :: phone_gyro ::--> proc_gyro:magnitude_stats:percentile50\n",
      "\n",
      "32 :: phone_gyro ::--> proc_gyro:magnitude_stats:percentile75\n",
      "\n",
      "33 :: phone_gyro ::--> proc_gyro:magnitude_stats:value_entropy\n",
      "\n",
      "34 :: phone_gyro ::--> proc_gyro:magnitude_stats:time_entropy\n",
      "\n",
      "35 :: phone_gyro ::--> proc_gyro:magnitude_spectrum:log_energy_band0\n",
      "\n",
      "36 :: phone_gyro ::--> proc_gyro:magnitude_spectrum:log_energy_band1\n",
      "\n",
      "37 :: phone_gyro ::--> proc_gyro:magnitude_spectrum:log_energy_band2\n",
      "\n",
      "38 :: phone_gyro ::--> proc_gyro:magnitude_spectrum:log_energy_band3\n",
      "\n",
      "39 :: phone_gyro ::--> proc_gyro:magnitude_spectrum:log_energy_band4\n",
      "\n",
      "40 :: phone_gyro ::--> proc_gyro:magnitude_spectrum:spectral_entropy\n",
      "\n",
      "41 :: phone_gyro ::--> proc_gyro:magnitude_autocorrelation:period\n",
      "\n",
      "42 :: phone_gyro ::--> proc_gyro:magnitude_autocorrelation:normalized_ac\n",
      "\n",
      "43 :: phone_gyro ::--> proc_gyro:3d:mean_x\n",
      "\n",
      "44 :: phone_gyro ::--> proc_gyro:3d:mean_y\n",
      "\n",
      "45 :: phone_gyro ::--> proc_gyro:3d:mean_z\n",
      "\n",
      "46 :: phone_gyro ::--> proc_gyro:3d:std_x\n",
      "\n",
      "47 :: phone_gyro ::--> proc_gyro:3d:std_y\n",
      "\n",
      "48 :: phone_gyro ::--> proc_gyro:3d:std_z\n",
      "\n",
      "49 :: phone_gyro ::--> proc_gyro:3d:ro_xy\n",
      "\n",
      "50 :: phone_gyro ::--> proc_gyro:3d:ro_xz\n",
      "\n",
      "51 :: phone_gyro ::--> proc_gyro:3d:ro_yz\n",
      "\n",
      "52 :: phone_mag ::--> raw_magnet:magnitude_stats:mean\n",
      "\n",
      "53 :: phone_mag ::--> raw_magnet:magnitude_stats:std\n",
      "\n",
      "54 :: phone_mag ::--> raw_magnet:magnitude_stats:moment3\n",
      "\n",
      "55 :: phone_mag ::--> raw_magnet:magnitude_stats:moment4\n",
      "\n",
      "56 :: phone_mag ::--> raw_magnet:magnitude_stats:percentile25\n",
      "\n",
      "57 :: phone_mag ::--> raw_magnet:magnitude_stats:percentile50\n",
      "\n",
      "58 :: phone_mag ::--> raw_magnet:magnitude_stats:percentile75\n",
      "\n",
      "59 :: phone_mag ::--> raw_magnet:magnitude_stats:value_entropy\n",
      "\n",
      "60 :: phone_mag ::--> raw_magnet:magnitude_stats:time_entropy\n",
      "\n",
      "61 :: phone_mag ::--> raw_magnet:magnitude_spectrum:log_energy_band0\n",
      "\n",
      "62 :: phone_mag ::--> raw_magnet:magnitude_spectrum:log_energy_band1\n",
      "\n",
      "63 :: phone_mag ::--> raw_magnet:magnitude_spectrum:log_energy_band2\n",
      "\n",
      "64 :: phone_mag ::--> raw_magnet:magnitude_spectrum:log_energy_band3\n",
      "\n",
      "65 :: phone_mag ::--> raw_magnet:magnitude_spectrum:log_energy_band4\n",
      "\n",
      "66 :: phone_mag ::--> raw_magnet:magnitude_spectrum:spectral_entropy\n",
      "\n",
      "67 :: phone_mag ::--> raw_magnet:magnitude_autocorrelation:period\n",
      "\n",
      "68 :: phone_mag ::--> raw_magnet:magnitude_autocorrelation:normalized_ac\n",
      "\n",
      "69 :: phone_mag ::--> raw_magnet:3d:mean_x\n",
      "\n",
      "70 :: phone_mag ::--> raw_magnet:3d:mean_y\n",
      "\n",
      "71 :: phone_mag ::--> raw_magnet:3d:mean_z\n",
      "\n",
      "72 :: phone_mag ::--> raw_magnet:3d:std_x\n",
      "\n",
      "73 :: phone_mag ::--> raw_magnet:3d:std_y\n",
      "\n",
      "74 :: phone_mag ::--> raw_magnet:3d:std_z\n",
      "\n",
      "75 :: phone_mag ::--> raw_magnet:3d:ro_xy\n",
      "\n",
      "76 :: phone_mag ::--> raw_magnet:3d:ro_xz\n",
      "\n",
      "77 :: phone_mag ::--> raw_magnet:3d:ro_yz\n",
      "\n",
      "78 :: phone_mag ::--> raw_magnet:avr_cosine_similarity_lag_range0\n",
      "\n",
      "79 :: phone_mag ::--> raw_magnet:avr_cosine_similarity_lag_range1\n",
      "\n",
      "80 :: phone_mag ::--> raw_magnet:avr_cosine_similarity_lag_range2\n",
      "\n",
      "81 :: phone_mag ::--> raw_magnet:avr_cosine_similarity_lag_range3\n",
      "\n",
      "82 :: phone_mag ::--> raw_magnet:avr_cosine_similarity_lag_range4\n",
      "\n",
      "83 :: watch_acc ::--> watch_acceleration:magnitude_stats:mean\n",
      "\n",
      "84 :: watch_acc ::--> watch_acceleration:magnitude_stats:std\n",
      "\n",
      "85 :: watch_acc ::--> watch_acceleration:magnitude_stats:moment3\n",
      "\n",
      "86 :: watch_acc ::--> watch_acceleration:magnitude_stats:moment4\n",
      "\n",
      "87 :: watch_acc ::--> watch_acceleration:magnitude_stats:percentile25\n",
      "\n",
      "88 :: watch_acc ::--> watch_acceleration:magnitude_stats:percentile50\n",
      "\n",
      "89 :: watch_acc ::--> watch_acceleration:magnitude_stats:percentile75\n",
      "\n",
      "90 :: watch_acc ::--> watch_acceleration:magnitude_stats:value_entropy\n",
      "\n",
      "91 :: watch_acc ::--> watch_acceleration:magnitude_stats:time_entropy\n",
      "\n",
      "92 :: watch_acc ::--> watch_acceleration:magnitude_spectrum:log_energy_band0\n",
      "\n",
      "93 :: watch_acc ::--> watch_acceleration:magnitude_spectrum:log_energy_band1\n",
      "\n",
      "94 :: watch_acc ::--> watch_acceleration:magnitude_spectrum:log_energy_band2\n",
      "\n",
      "95 :: watch_acc ::--> watch_acceleration:magnitude_spectrum:log_energy_band3\n",
      "\n",
      "96 :: watch_acc ::--> watch_acceleration:magnitude_spectrum:log_energy_band4\n",
      "\n",
      "97 :: watch_acc ::--> watch_acceleration:magnitude_spectrum:spectral_entropy\n",
      "\n",
      "98 :: watch_acc ::--> watch_acceleration:magnitude_autocorrelation:period\n",
      "\n",
      "99 :: watch_acc ::--> watch_acceleration:magnitude_autocorrelation:normalized_ac\n",
      "\n",
      "100 :: watch_acc ::--> watch_acceleration:3d:mean_x\n",
      "\n",
      "101 :: watch_acc ::--> watch_acceleration:3d:mean_y\n",
      "\n",
      "102 :: watch_acc ::--> watch_acceleration:3d:mean_z\n",
      "\n",
      "103 :: watch_acc ::--> watch_acceleration:3d:std_x\n",
      "\n",
      "104 :: watch_acc ::--> watch_acceleration:3d:std_y\n",
      "\n",
      "105 :: watch_acc ::--> watch_acceleration:3d:std_z\n",
      "\n",
      "106 :: watch_acc ::--> watch_acceleration:3d:ro_xy\n",
      "\n",
      "107 :: watch_acc ::--> watch_acceleration:3d:ro_xz\n",
      "\n",
      "108 :: watch_acc ::--> watch_acceleration:3d:ro_yz\n",
      "\n",
      "109 :: watch_acc ::--> watch_acceleration:spectrum:x_log_energy_band0\n",
      "\n",
      "110 :: watch_acc ::--> watch_acceleration:spectrum:x_log_energy_band1\n",
      "\n",
      "111 :: watch_acc ::--> watch_acceleration:spectrum:x_log_energy_band2\n",
      "\n",
      "112 :: watch_acc ::--> watch_acceleration:spectrum:x_log_energy_band3\n",
      "\n",
      "113 :: watch_acc ::--> watch_acceleration:spectrum:x_log_energy_band4\n",
      "\n",
      "114 :: watch_acc ::--> watch_acceleration:spectrum:y_log_energy_band0\n",
      "\n",
      "115 :: watch_acc ::--> watch_acceleration:spectrum:y_log_energy_band1\n",
      "\n",
      "116 :: watch_acc ::--> watch_acceleration:spectrum:y_log_energy_band2\n",
      "\n",
      "117 :: watch_acc ::--> watch_acceleration:spectrum:y_log_energy_band3\n",
      "\n",
      "118 :: watch_acc ::--> watch_acceleration:spectrum:y_log_energy_band4\n",
      "\n",
      "119 :: watch_acc ::--> watch_acceleration:spectrum:z_log_energy_band0\n",
      "\n",
      "120 :: watch_acc ::--> watch_acceleration:spectrum:z_log_energy_band1\n",
      "\n",
      "121 :: watch_acc ::--> watch_acceleration:spectrum:z_log_energy_band2\n",
      "\n",
      "122 :: watch_acc ::--> watch_acceleration:spectrum:z_log_energy_band3\n",
      "\n",
      "123 :: watch_acc ::--> watch_acceleration:spectrum:z_log_energy_band4\n",
      "\n",
      "124 :: watch_acc ::--> watch_acceleration:relative_directions:avr_cosine_similarity_lag_range0\n",
      "\n",
      "125 :: watch_acc ::--> watch_acceleration:relative_directions:avr_cosine_similarity_lag_range1\n",
      "\n",
      "126 :: watch_acc ::--> watch_acceleration:relative_directions:avr_cosine_similarity_lag_range2\n",
      "\n",
      "127 :: watch_acc ::--> watch_acceleration:relative_directions:avr_cosine_similarity_lag_range3\n",
      "\n",
      "128 :: watch_acc ::--> watch_acceleration:relative_directions:avr_cosine_similarity_lag_range4\n",
      "\n",
      "129 :: watch_dir ::--> watch_heading:mean_cos\n",
      "\n",
      "130 :: watch_dir ::--> watch_heading:std_cos\n",
      "\n",
      "131 :: watch_dir ::--> watch_heading:mom3_cos\n",
      "\n",
      "132 :: watch_dir ::--> watch_heading:mom4_cos\n",
      "\n",
      "133 :: watch_dir ::--> watch_heading:mean_sin\n",
      "\n",
      "134 :: watch_dir ::--> watch_heading:std_sin\n",
      "\n",
      "135 :: watch_dir ::--> watch_heading:mom3_sin\n",
      "\n",
      "136 :: watch_dir ::--> watch_heading:mom4_sin\n",
      "\n",
      "137 :: watch_dir ::--> watch_heading:entropy_8bins\n",
      "\n",
      "138 :: phone_loc ::--> location:num_valid_updates\n",
      "\n",
      "139 :: phone_loc ::--> location:log_latitude_range\n",
      "\n",
      "140 :: phone_loc ::--> location:log_longitude_range\n",
      "\n",
      "141 :: phone_loc ::--> location:min_altitude\n",
      "\n",
      "142 :: phone_loc ::--> location:max_altitude\n",
      "\n",
      "143 :: phone_loc ::--> location:min_speed\n",
      "\n",
      "144 :: phone_loc ::--> location:max_speed\n",
      "\n",
      "145 :: phone_loc ::--> location:best_horizontal_accuracy\n",
      "\n",
      "146 :: phone_loc ::--> location:best_vertical_accuracy\n",
      "\n",
      "147 :: phone_loc ::--> location:diameter\n",
      "\n",
      "148 :: phone_loc ::--> location:log_diameter\n",
      "\n",
      "149 :: phone_loc ::--> location_quick_features:std_lat\n",
      "\n",
      "150 :: phone_loc ::--> location_quick_features:std_long\n",
      "\n",
      "151 :: phone_loc ::--> location_quick_features:lat_change\n",
      "\n",
      "152 :: phone_loc ::--> location_quick_features:long_change\n",
      "\n",
      "153 :: phone_loc ::--> location_quick_features:mean_abs_lat_deriv\n",
      "\n",
      "154 :: phone_loc ::--> location_quick_features:mean_abs_long_deriv\n",
      "\n",
      "155 :: phone_audio ::--> audio_naive:mfcc0:mean\n",
      "\n",
      "156 :: phone_audio ::--> audio_naive:mfcc1:mean\n",
      "\n",
      "157 :: phone_audio ::--> audio_naive:mfcc2:mean\n",
      "\n",
      "158 :: phone_audio ::--> audio_naive:mfcc3:mean\n",
      "\n",
      "159 :: phone_audio ::--> audio_naive:mfcc4:mean\n",
      "\n",
      "160 :: phone_audio ::--> audio_naive:mfcc5:mean\n",
      "\n",
      "161 :: phone_audio ::--> audio_naive:mfcc6:mean\n",
      "\n",
      "162 :: phone_audio ::--> audio_naive:mfcc7:mean\n",
      "\n",
      "163 :: phone_audio ::--> audio_naive:mfcc8:mean\n",
      "\n",
      "164 :: phone_audio ::--> audio_naive:mfcc9:mean\n",
      "\n",
      "165 :: phone_audio ::--> audio_naive:mfcc10:mean\n",
      "\n",
      "166 :: phone_audio ::--> audio_naive:mfcc11:mean\n",
      "\n",
      "167 :: phone_audio ::--> audio_naive:mfcc12:mean\n",
      "\n",
      "168 :: phone_audio ::--> audio_naive:mfcc0:std\n",
      "\n",
      "169 :: phone_audio ::--> audio_naive:mfcc1:std\n",
      "\n",
      "170 :: phone_audio ::--> audio_naive:mfcc2:std\n",
      "\n",
      "171 :: phone_audio ::--> audio_naive:mfcc3:std\n",
      "\n",
      "172 :: phone_audio ::--> audio_naive:mfcc4:std\n",
      "\n",
      "173 :: phone_audio ::--> audio_naive:mfcc5:std\n",
      "\n",
      "174 :: phone_audio ::--> audio_naive:mfcc6:std\n",
      "\n",
      "175 :: phone_audio ::--> audio_naive:mfcc7:std\n",
      "\n",
      "176 :: phone_audio ::--> audio_naive:mfcc8:std\n",
      "\n",
      "177 :: phone_audio ::--> audio_naive:mfcc9:std\n",
      "\n",
      "178 :: phone_audio ::--> audio_naive:mfcc10:std\n",
      "\n",
      "179 :: phone_audio ::--> audio_naive:mfcc11:std\n",
      "\n",
      "180 :: phone_audio ::--> audio_naive:mfcc12:std\n",
      "\n",
      "181 ::  ::--> audio_properties:max_abs_value\n",
      "\n",
      "182 ::  ::--> audio_properties:normalization_multiplier\n",
      "\n",
      "183 :: phone_app ::--> discrete:app_state:is_active\n",
      "\n",
      "184 :: phone_app ::--> discrete:app_state:is_inactive\n",
      "\n",
      "185 :: phone_app ::--> discrete:app_state:is_background\n",
      "\n",
      "186 :: phone_app ::--> discrete:app_state:missing\n",
      "\n",
      "187 :: phone_battery ::--> discrete:battery_plugged:is_ac\n",
      "\n",
      "188 :: phone_battery ::--> discrete:battery_plugged:is_usb\n",
      "\n",
      "189 :: phone_battery ::--> discrete:battery_plugged:is_wireless\n",
      "\n",
      "190 :: phone_battery ::--> discrete:battery_plugged:missing\n",
      "\n",
      "191 :: phone_battery ::--> discrete:battery_state:is_unknown\n",
      "\n",
      "192 :: phone_battery ::--> discrete:battery_state:is_unplugged\n",
      "\n",
      "193 :: phone_battery ::--> discrete:battery_state:is_not_charging\n",
      "\n",
      "194 :: phone_battery ::--> discrete:battery_state:is_discharging\n",
      "\n",
      "195 :: phone_battery ::--> discrete:battery_state:is_charging\n",
      "\n",
      "196 :: phone_battery ::--> discrete:battery_state:is_full\n",
      "\n",
      "197 :: phone_battery ::--> discrete:battery_state:missing\n",
      "\n",
      "198 :: phone_use ::--> discrete:on_the_phone:is_False\n",
      "\n",
      "199 :: phone_use ::--> discrete:on_the_phone:is_True\n",
      "\n",
      "200 :: phone_use ::--> discrete:on_the_phone:missing\n",
      "\n",
      "201 :: phone_callstat ::--> discrete:ringer_mode:is_normal\n",
      "\n",
      "202 :: phone_callstat ::--> discrete:ringer_mode:is_silent_no_vibrate\n",
      "\n",
      "203 :: phone_callstat ::--> discrete:ringer_mode:is_silent_with_vibrate\n",
      "\n",
      "204 :: phone_callstat ::--> discrete:ringer_mode:missing\n",
      "\n",
      "205 :: phone_wifi ::--> discrete:wifi_status:is_not_reachable\n",
      "\n",
      "206 :: phone_wifi ::--> discrete:wifi_status:is_reachable_via_wifi\n",
      "\n",
      "207 :: phone_wifi ::--> discrete:wifi_status:is_reachable_via_wwan\n",
      "\n",
      "208 :: phone_wifi ::--> discrete:wifi_status:missing\n",
      "\n",
      "209 :: phone_lf ::--> lf_measurements:light\n",
      "\n",
      "210 :: phone_lf ::--> lf_measurements:pressure\n",
      "\n",
      "211 :: phone_lf ::--> lf_measurements:proximity_cm\n",
      "\n",
      "212 :: phone_lf ::--> lf_measurements:proximity\n",
      "\n",
      "213 :: phone_lf ::--> lf_measurements:relative_humidity\n",
      "\n",
      "214 :: phone_lf ::--> lf_measurements:battery_level\n",
      "\n",
      "215 :: phone_lf ::--> lf_measurements:screen_brightness\n",
      "\n",
      "216 :: phone_lf ::--> lf_measurements:temperature_ambient\n",
      "\n",
      "217 :: phone_time ::--> discrete:time_of_day:between0and6\n",
      "\n",
      "218 :: phone_time ::--> discrete:time_of_day:between3and9\n",
      "\n",
      "219 :: phone_time ::--> discrete:time_of_day:between6and12\n",
      "\n",
      "220 :: phone_time ::--> discrete:time_of_day:between9and15\n",
      "\n",
      "221 :: phone_time ::--> discrete:time_of_day:between12and18\n",
      "\n",
      "222 :: phone_time ::--> discrete:time_of_day:between15and21\n",
      "\n",
      "223 :: phone_time ::--> discrete:time_of_day:between18and24\n",
      "\n",
      "224 :: phone_time ::--> discrete:time_of_day:between21and3\n",
      "\n",
      "Activities and counts:\n",
      "[('LOC_home', 3040), ('OR_indoors', 2487), ('PHONE_ON_TABLE', 2179), ('SITTING', 1916), ('WITH_FRIENDS', 1730), ('LYING_DOWN', 1336), ('SLEEPING', 1021), ('WATCHING_TV', 912), ('EATING', 762), ('PHONE_IN_POCKET', 706), ('TALKING', 638), ('DRIVE_-_I_M_A_PASSENGER', 409), ('OR_standing', 384), ('IN_A_CAR', 342), ('OR_exercise', 162), ('AT_THE_GYM', 162), ('SINGING', 136), ('FIX_walking', 132), ('OR_outside', 127), ('SHOPPING', 111), ('AT_SCHOOL', 105), ('BATHING_-_SHOWER', 85), ('DRESSING', 67), ('DRINKING__ALCOHOL_', 66), ('PHONE_IN_HAND', 64), ('FIX_restaurant', 59), ('IN_CLASS', 54), ('PHONE_IN_BAG', 33), ('IN_A_MEETING', 27), ('TOILET', 12), ('COOKING', 5), ('ELEVATOR', 1), ('FIX_running', 0), ('BICYCLING', 0), ('LAB_WORK', 0), ('LOC_main_workplace', 0), ('ON_A_BUS', 0), ('DRIVE_-_I_M_THE_DRIVER', 0), ('STROLLING', 0), ('CLEANING', 0), ('DOING_LAUNDRY', 0), ('WASHING_DISHES', 0), ('SURFING_THE_INTERNET', 0), ('AT_A_PARTY', 0), ('AT_A_BAR', 0), ('LOC_beach', 0), ('COMPUTER_WORK', 0), ('GROOMING', 0), ('STAIRS_-_GOING_UP', 0), ('STAIRS_-_GOING_DOWN', 0), ('WITH_CO-WORKERS', 0)]\n"
     ]
    }
   ],
   "source": [
    "# Reading sample data\n",
    "sample_loc='{}/{}.csv.gz'.format(prefix,user_sample)\n",
    "x_user,y_user,missedlabel_user,tstamp_user,featurename_user,labelname_user=read_user_data(sample_loc)\n",
    "\n",
    "# Dataset summaries for this user\n",
    "print('Data shape input for user (Len minutes/num examples, num sensors): ',x_user.shape) # Timestep examples, number of sensors\n",
    "print('Label shape for user (Len minutes, num labels): ',y_user.shape,'\\n') # Timestep examples, labels\n",
    "\n",
    "countlabels_user=np.sum(y_user,axis=0) # Column summary\n",
    "labelname_countlabel_user=zip(labelname_user,countlabels_user) # Zip together names, counts\n",
    "labelname_countlabel_user=sorted(labelname_countlabel_user,key=lambda row:row[-1],reverse=True)\n",
    "\n",
    "print('Sensor feature names:\\n')\n",
    "feature_names=summarize_features(featurename_user)\n",
    "    \n",
    "for i,sensor_feature in enumerate(featurename_user):\n",
    "    print('{} :: {} ::--> {}\\n'.format(i,feature_names[i],sensor_feature))\n",
    "\n",
    "print('Activities and counts:')\n",
    "print(labelname_countlabel_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "    ISSUE: There are some labels (e.g. Phone location:bag etc.) that some users have not filled out for any timestep and shows up as np.nan. The label sum above was a check to see if the same label wasn't filled out for other users (hence would have a count of zero) and would let the label being completely removed. The lowest count was (Elevator:200) which doesn't help.\n",
    "    I cannot do blindly remove rows because a particular label wasn't filled out for any timestep for a user. For single label case, this is fine...but for a multi-label case, this will mean that other valid labels are ignored. The only option that I have so far is to naively convert all nans in the labels to zeros. This could mean a loss of accuracy (the user might have been doing the task in the label but have omitted annotating it, and so we are incorrectly training a feature vector....but there is no choice so far.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Choosing sensor labels\n",
    "'''\n",
    "Summary sensor choices are: phone_acc,phone_gyro,phone_mag,watch_acc,watch_dir,phone_loc,phone_audio,\n",
    "phone_app,phone_battery,phone_use,phone_callstat,phone_wifi,phone_lf,phone_time\n",
    "In this project, we aren't using watch_acc,watch_dir (no smartwatch)\n",
    "'''\n",
    "\n",
    "def choose_sensors(X_train,used_sensors,summarized_feature_names):\n",
    "    used_sensor_feature_names=np.zeros(len(summarized_feature_names),dtype=bool)\n",
    "    # Creates a zero boolean vector of all possible feature names\n",
    "    for s in used_sensors:\n",
    "        used_sensor_feature_names=np.logical_or(used_sensor_feature_names,(s==summarized_feature_names))\n",
    "    X_train=X_train[:,used_sensor_feature_names]\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Returns a standardized (0 mean, 1 variance) dataset\n",
    "def standardize(X_train):\n",
    "    mean=np.nanmean(X_train,axis=0).reshape((1,-1))# Ignores NaNs while finding the mean across rows\n",
    "    standard_dev=np.nanstd(X_train,axis=0) # Ignores NaNs while finding the standard deviation across rows\n",
    "    standard_dev_nonzero=np.where(standard_dev>0,standard_dev,1.).reshape((1,-1)) # Div zero\n",
    "    \n",
    "    X=(X_train-mean)/standard_dev_nonzero\n",
    "    return X,mean,standard_dev_nonzero   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Sensor Types, Label Possibilities variables\n",
    "sensor_types=['phone_acc','phone_gyro','phone_mag','phone_loc','phone_audio',\n",
    "'phone_app','phone_battery','phone_use','phone_callstat','phone_wifi','phone_lf','phone_time']\n",
    "label_possibilities=['LOC_home','OR_indoors','PHONE_ON_TABLE','SITTING','WITH_FRIENDS',\n",
    " 'LYING_DOWN','SLEEPING','WATCHING_TV','EATING','PHONE_IN_POCKET',\n",
    " 'TALKING','DRIVE_-_I_M_A_PASSENGER','OR_standing','IN_A_CAR',\n",
    " 'OR_exercise','AT_THE_GYM','SINGING','FIX_walking','OR_outside',\n",
    " 'SHOPPING','AT_SCHOOL','BATHING_-_SHOWER','DRESSING','DRINKING__ALCOHOL_',\n",
    " 'PHONE_IN_HAND','FIX_restaurant','IN_CLASS','PHONE_IN_BAG','IN_A_MEETING',\n",
    " 'TOILET','COOKING','ELEVATOR','FIX_running','BICYCLING','LAB_WORK',\n",
    " 'LOC_main_workplace','ON_A_BUS','DRIVE_-_I_M_THE_DRIVER','STROLLING',\n",
    " 'CLEANING','DOING_LAUNDRY','WASHING_DISHES','SURFING_THE_INTERNET',\n",
    " 'AT_A_PARTY','AT_A_BAR','LOC_beach','COMPUTER_WORK','GROOMING','STAIRS_-_GOING_UP',\n",
    " 'STAIRS_-_GOING_DOWN','WITH_CO-WORKERS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new data structure for all valid data and pickling it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with np.nan labels (missing labels). Zero impute missing feature entries. Standardization done at train time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 3600D531-0C55-44A7-AE95-A7A38519464E.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(5203, 225), after:(5203, 225)\n",
      "Y_shape before removing invalid labels:(5203, 51), after:(5203, 51)\n",
      "\t Per User Training examples:3642, Testing examples:1561\n",
      "Reading 8023FE1A-D3B0-4E2C-A57A-9321B7FC755F.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(9189, 225), after:(9189, 225)\n",
      "Y_shape before removing invalid labels:(9189, 51), after:(9189, 51)\n",
      "\t Per User Training examples:6432, Testing examples:2757\n",
      "Reading 86A4F379-B305-473D-9D83-FC7D800180EF.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(10738, 225), after:(10738, 225)\n",
      "Y_shape before removing invalid labels:(10738, 51), after:(10738, 51)\n",
      "\t Per User Training examples:7516, Testing examples:3222\n",
      "Reading 1538C99F-BA1E-4EFB-A949-6C7C47701B20.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(6549, 225), after:(6549, 225)\n",
      "Y_shape before removing invalid labels:(6549, 51), after:(6549, 51)\n",
      "\t Per User Training examples:4584, Testing examples:1965\n",
      "Reading 11B5EC4D-4133-4289-B475-4E737182A406.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(8845, 225), after:(8845, 225)\n",
      "Y_shape before removing invalid labels:(8845, 51), after:(8845, 51)\n",
      "\t Per User Training examples:6191, Testing examples:2654\n",
      "Reading 74B86067-5D4B-43CF-82CF-341B76BEA0F4.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(7298, 225), after:(7298, 225)\n",
      "Y_shape before removing invalid labels:(7298, 51), after:(7298, 51)\n",
      "\t Per User Training examples:5108, Testing examples:2190\n",
      "Reading 4FC32141-E888-4BFF-8804-12559A491D8C.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(4979, 225), after:(4979, 225)\n",
      "Y_shape before removing invalid labels:(4979, 51), after:(4979, 51)\n",
      "\t Per User Training examples:3485, Testing examples:1494\n",
      "Reading B9724848-C7E2-45F4-9B3F-A1F38D864495.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(7626, 225), after:(7626, 225)\n",
      "Y_shape before removing invalid labels:(7626, 51), after:(7626, 51)\n",
      "\t Per User Training examples:5338, Testing examples:2288\n",
      "Reading A76A5AF5-5A93-4CF2-A16E-62353BB70E8A.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(7520, 225), after:(7520, 225)\n",
      "Y_shape before removing invalid labels:(7520, 51), after:(7520, 51)\n",
      "\t Per User Training examples:5264, Testing examples:2256\n",
      "Reading 96A358A0-FFF2-4239-B93E-C7425B901B47.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(5819, 225), after:(5819, 225)\n",
      "Y_shape before removing invalid labels:(5819, 51), after:(5819, 51)\n",
      "\t Per User Training examples:4073, Testing examples:1746\n",
      "Reading 665514DE-49DC-421F-8DCB-145D0B2609AD.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(9167, 225), after:(9167, 225)\n",
      "Y_shape before removing invalid labels:(9167, 51), after:(9167, 51)\n",
      "\t Per User Training examples:6416, Testing examples:2751\n",
      "Reading BE3CA5A6-A561-4BBD-B7C9-5DF6805400FC.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(8309, 225), after:(8309, 225)\n",
      "Y_shape before removing invalid labels:(8309, 51), after:(8309, 51)\n",
      "\t Per User Training examples:5816, Testing examples:2493\n",
      "Reading A5A30F76-581E-4757-97A2-957553A2C6AA.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(1667, 225), after:(1667, 225)\n",
      "Y_shape before removing invalid labels:(1667, 51), after:(1667, 51)\n",
      "\t Per User Training examples:1166, Testing examples:501\n",
      "Reading 27E04243-B138-4F40-A164-F40B60165CF3.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(4927, 225), after:(4927, 225)\n",
      "Y_shape before removing invalid labels:(4927, 51), after:(4927, 51)\n",
      "\t Per User Training examples:3448, Testing examples:1479\n",
      "Reading 0E6184E1-90C0-48EE-B25A-F1ECB7B9714E.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(7521, 225), after:(7521, 225)\n",
      "Y_shape before removing invalid labels:(7521, 51), after:(7521, 51)\n",
      "\t Per User Training examples:5264, Testing examples:2257\n",
      "Reading FDAA70A1-42A3-4E3F-9AE3-3FDA412E03BF.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(4973, 225), after:(4973, 225)\n",
      "Y_shape before removing invalid labels:(4973, 51), after:(4973, 51)\n",
      "\t Per User Training examples:3481, Testing examples:1492\n",
      "Reading BEF6C611-50DA-4971-A040-87FB979F3FC1.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(3451, 225), after:(3451, 225)\n",
      "Y_shape before removing invalid labels:(3451, 51), after:(3451, 51)\n",
      "\t Per User Training examples:2415, Testing examples:1036\n",
      "Reading 098A72A5-E3E5-4F54-A152-BBDA0DF7B694.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(6813, 225), after:(6813, 225)\n",
      "Y_shape before removing invalid labels:(6813, 51), after:(6813, 51)\n",
      "\t Per User Training examples:4769, Testing examples:2044\n",
      "Reading 78A91A4E-4A51-4065-BDA7-94755F0BB3BB.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(11996, 225), after:(11996, 225)\n",
      "Y_shape before removing invalid labels:(11996, 51), after:(11996, 51)\n",
      "\t Per User Training examples:8397, Testing examples:3599\n",
      "Reading B7F9D634-263E-4A97-87F9-6FFB4DDCB36C.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(9383, 225), after:(9383, 225)\n",
      "Y_shape before removing invalid labels:(9383, 51), after:(9383, 51)\n",
      "\t Per User Training examples:6568, Testing examples:2815\n",
      "Reading 9DC38D04-E82E-4F29-AB52-B476535226F2.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(9686, 225), after:(9686, 225)\n",
      "Y_shape before removing invalid labels:(9686, 51), after:(9686, 51)\n",
      "\t Per User Training examples:6780, Testing examples:2906\n",
      "Reading 81536B0A-8DBF-4D8A-AC24-9543E2E4C8E0.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(6407, 225), after:(6407, 225)\n",
      "Y_shape before removing invalid labels:(6407, 51), after:(6407, 51)\n",
      "\t Per User Training examples:4484, Testing examples:1923\n",
      "Reading 5EF64122-B513-46AE-BCF1-E62AAC285D2C.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(3911, 225), after:(3911, 225)\n",
      "Y_shape before removing invalid labels:(3911, 51), after:(3911, 51)\n",
      "\t Per User Training examples:2737, Testing examples:1174\n",
      "Reading C48CE857-A0DD-4DDB-BEA5-3A25449B2153.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(5092, 225), after:(5092, 225)\n",
      "Y_shape before removing invalid labels:(5092, 51), after:(5092, 51)\n",
      "\t Per User Training examples:3564, Testing examples:1528\n",
      "Reading CDA3BBF7-6631-45E8-85BA-EEB416B32A3C.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(2860, 225), after:(2860, 225)\n",
      "Y_shape before removing invalid labels:(2860, 51), after:(2860, 51)\n",
      "\t Per User Training examples:2002, Testing examples:858\n",
      "Reading 61976C24-1C50-4355-9C49-AAE44A7D09F6.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(8730, 225), after:(8730, 225)\n",
      "Y_shape before removing invalid labels:(8730, 51), after:(8730, 51)\n",
      "\t Per User Training examples:6111, Testing examples:2619\n",
      "Reading 59EEFAE0-DEB0-4FFF-9250-54D2A03D0CF2.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(7542, 225), after:(7542, 225)\n",
      "Y_shape before removing invalid labels:(7542, 51), after:(7542, 51)\n",
      "\t Per User Training examples:5279, Testing examples:2263\n",
      "Reading 33A85C34-CFE4-4732-9E73-0A7AC861B27A.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(6172, 225), after:(6172, 225)\n",
      "Y_shape before removing invalid labels:(6172, 51), after:(6172, 51)\n",
      "\t Per User Training examples:4320, Testing examples:1852\n",
      "Reading 61359772-D8D8-480D-B623-7C636EAD0C81.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(6079, 225), after:(6079, 225)\n",
      "Y_shape before removing invalid labels:(6079, 51), after:(6079, 51)\n",
      "\t Per User Training examples:4255, Testing examples:1824\n",
      "Reading A7599A50-24AE-46A6-8EA6-2576F1011D81.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(3898, 225), after:(3898, 225)\n",
      "Y_shape before removing invalid labels:(3898, 51), after:(3898, 51)\n",
      "\t Per User Training examples:2728, Testing examples:1170\n",
      "Reading 806289BC-AD52-4CC1-806C-0CDB14D65EB6.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(9242, 225), after:(9242, 225)\n",
      "Y_shape before removing invalid labels:(9242, 51), after:(9242, 51)\n",
      "\t Per User Training examples:6469, Testing examples:2773\n",
      "Reading 0BFC35E2-4817-4865-BFA7-764742302A2D.features_labels.csv.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape before removing invalid labels:(3108, 225), after:(3108, 225)\n",
      "Y_shape before removing invalid labels:(3108, 51), after:(3108, 51)\n",
      "\t Per User Training examples:2175, Testing examples:933\n",
      "Reading F50235E0-DD67-4F2A-B00B-1F31ADA998B9.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(2266, 225), after:(2266, 225)\n",
      "Y_shape before removing invalid labels:(2266, 51), after:(2266, 51)\n",
      "\t Per User Training examples:1586, Testing examples:680\n",
      "Reading 1155FF54-63D3-4AB2-9863-8385D0BD0A13.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(2685, 225), after:(2685, 225)\n",
      "Y_shape before removing invalid labels:(2685, 51), after:(2685, 51)\n",
      "\t Per User Training examples:1879, Testing examples:806\n",
      "Reading 9759096F-1119-4E19-A0AD-6F16989C7E1C.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(9959, 225), after:(9959, 225)\n",
      "Y_shape before removing invalid labels:(9959, 51), after:(9959, 51)\n",
      "\t Per User Training examples:6971, Testing examples:2988\n",
      "Reading 5152A2DF-FAF3-4BA8-9CA9-E66B32671A53.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(6617, 225), after:(6617, 225)\n",
      "Y_shape before removing invalid labels:(6617, 51), after:(6617, 51)\n",
      "\t Per User Training examples:4631, Testing examples:1986\n",
      "Reading 7CE37510-56D0-4120-A1CF-0E23351428D2.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(9761, 225), after:(9761, 225)\n",
      "Y_shape before removing invalid labels:(9761, 51), after:(9761, 51)\n",
      "\t Per User Training examples:6832, Testing examples:2929\n",
      "Reading 59818CD2-24D7-4D32-B133-24C2FE3801E5.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(5947, 225), after:(5947, 225)\n",
      "Y_shape before removing invalid labels:(5947, 51), after:(5947, 51)\n",
      "\t Per User Training examples:4162, Testing examples:1785\n",
      "Reading CA820D43-E5E2-42EF-9798-BE56F776370B.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(7865, 225), after:(7865, 225)\n",
      "Y_shape before removing invalid labels:(7865, 51), after:(7865, 51)\n",
      "\t Per User Training examples:5505, Testing examples:2360\n",
      "Reading 00EABED2-271D-49D8-B599-1D4A09240601.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(2287, 225), after:(2287, 225)\n",
      "Y_shape before removing invalid labels:(2287, 51), after:(2287, 51)\n",
      "\t Per User Training examples:1600, Testing examples:687\n",
      "Reading 40E170A7-607B-4578-AF04-F021C3B0384A.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(7649, 225), after:(7649, 225)\n",
      "Y_shape before removing invalid labels:(7649, 51), after:(7649, 51)\n",
      "\t Per User Training examples:5354, Testing examples:2295\n",
      "Reading 797D145F-3858-4A7F-A7C2-A4EB721E133C.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(3593, 225), after:(3593, 225)\n",
      "Y_shape before removing invalid labels:(3593, 51), after:(3593, 51)\n",
      "\t Per User Training examples:2515, Testing examples:1078\n",
      "Reading ECECC2AB-D32F-4F90-B74C-E12A1C69BBE2.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(3530, 225), after:(3530, 225)\n",
      "Y_shape before removing invalid labels:(3530, 51), after:(3530, 51)\n",
      "\t Per User Training examples:2471, Testing examples:1059\n",
      "Reading 2C32C23E-E30C-498A-8DD2-0EFB9150A02E.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(8516, 225), after:(8516, 225)\n",
      "Y_shape before removing invalid labels:(8516, 51), after:(8516, 51)\n",
      "\t Per User Training examples:5961, Testing examples:2555\n",
      "Reading CCAF77F0-FABB-4F2F-9E24-D56AD0C5A82F.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(8472, 225), after:(8472, 225)\n",
      "Y_shape before removing invalid labels:(8472, 51), after:(8472, 51)\n",
      "\t Per User Training examples:5930, Testing examples:2542\n",
      "Reading 5119D0F8-FCA8-4184-A4EB-19421A40DE0D.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(6617, 225), after:(6617, 225)\n",
      "Y_shape before removing invalid labels:(6617, 51), after:(6617, 51)\n",
      "\t Per User Training examples:4631, Testing examples:1986\n",
      "Reading D7D20E2E-FC78-405D-B346-DBD3FD8FC92B.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(6210, 225), after:(6210, 225)\n",
      "Y_shape before removing invalid labels:(6210, 51), after:(6210, 51)\n",
      "\t Per User Training examples:4347, Testing examples:1863\n",
      "Reading B09E373F-8A54-44C8-895B-0039390B859F.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(8134, 225), after:(8134, 225)\n",
      "Y_shape before removing invalid labels:(8134, 51), after:(8134, 51)\n",
      "\t Per User Training examples:5693, Testing examples:2441\n",
      "Reading 136562B6-95B2-483D-88DC-065F28409FD2.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(6218, 225), after:(6218, 225)\n",
      "Y_shape before removing invalid labels:(6218, 51), after:(6218, 51)\n",
      "\t Per User Training examples:4352, Testing examples:1866\n",
      "Reading 99B204C0-DD5C-4BB7-83E8-A37281B8D769.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(6038, 225), after:(6038, 225)\n",
      "Y_shape before removing invalid labels:(6038, 51), after:(6038, 51)\n",
      "\t Per User Training examples:4226, Testing examples:1812\n",
      "Reading 83CF687B-7CEC-434B-9FE8-00C3D5799BE6.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(9539, 225), after:(9539, 225)\n",
      "Y_shape before removing invalid labels:(9539, 51), after:(9539, 51)\n",
      "\t Per User Training examples:6677, Testing examples:2862\n",
      "Reading E65577C1-8D5D-4F70-AF23-B3ADB9D3DBA3.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(3441, 225), after:(3441, 225)\n",
      "Y_shape before removing invalid labels:(3441, 51), after:(3441, 51)\n",
      "\t Per User Training examples:2408, Testing examples:1033\n",
      "Reading 0A986513-7828-4D53-AA1F-E02D6DF9561B.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(3960, 225), after:(3960, 225)\n",
      "Y_shape before removing invalid labels:(3960, 51), after:(3960, 51)\n",
      "\t Per User Training examples:2772, Testing examples:1188\n",
      "Reading A5CDF89D-02A2-4EC1-89F8-F534FDABDD96.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(6040, 225), after:(6040, 225)\n",
      "Y_shape before removing invalid labels:(6040, 51), after:(6040, 51)\n",
      "\t Per User Training examples:4228, Testing examples:1812\n",
      "Reading 7D9BB102-A612-4E2A-8E22-3159752F55D8.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(1600, 225), after:(1600, 225)\n",
      "Y_shape before removing invalid labels:(1600, 51), after:(1600, 51)\n",
      "\t Per User Training examples:1120, Testing examples:480\n",
      "Reading 24E40C4C-A349-4F9F-93AB-01D00FB994AF.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(4771, 225), after:(4771, 225)\n",
      "Y_shape before removing invalid labels:(4771, 51), after:(4771, 51)\n",
      "\t Per User Training examples:3339, Testing examples:1432\n",
      "Reading 1DBB0F6F-1F81-4A50-9DF4-CD62ACFA4842.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(7375, 225), after:(7375, 225)\n",
      "Y_shape before removing invalid labels:(7375, 51), after:(7375, 51)\n",
      "\t Per User Training examples:5162, Testing examples:2213\n",
      "Reading 481F4DD2-7689-43B9-A2AA-C8772227162B.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(6691, 225), after:(6691, 225)\n",
      "Y_shape before removing invalid labels:(6691, 51), after:(6691, 51)\n",
      "\t Per User Training examples:4683, Testing examples:2008\n",
      "Reading CF722AA9-2533-4E51-9FEB-9EAC84EE9AAC.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(3615, 225), after:(3615, 225)\n",
      "Y_shape before removing invalid labels:(3615, 51), after:(3615, 51)\n",
      "\t Per User Training examples:2530, Testing examples:1085\n",
      "Reading 4E98F91F-4654-42EF-B908-A3389443F2E7.features_labels.csv.gz\n",
      "X_shape before removing invalid labels:(3250, 225), after:(3250, 225)\n",
      "Y_shape before removing invalid labels:(3250, 51), after:(3250, 51)\n",
      "\t Per User Training examples:2275, Testing examples:975\n",
      "\n",
      "Training: X::(264117, 168) ,Y::(264117, 51)\n",
      "Testing: X::(113229, 168) ,Y::(113229, 51)\n",
      "Pickling data files\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Skipping cell if the data files were already created previously \n",
    "if not done:\n",
    "    # Reading data in the directory (Stacked)\n",
    "    X_combined=np.empty((0,168))\n",
    "    Y_combined=np.empty((0,51))\n",
    "    X_train_t=np.empty((0,168))\n",
    "    Y_train_t=np.empty((0,51))\n",
    "    X_test_t=np.empty((0,168))\n",
    "    Y_test_t=np.empty((0,51))\n",
    "    #M_train_t=np.empty((0,51))\n",
    "    #M_test_t=np.empty((0,51))\n",
    "\n",
    "    for u_file in glob.glob('{}/*.csv.gz'.format(prefix)):\n",
    "            x_user,y_user,missed_label_user,tstamp_user,featurename_user,labelname_user=read_user_data(u_file)\n",
    "            x_sh=x_user.shape\n",
    "            y_sh=y_user.shape\n",
    "            # Removing invalid labels, imputing missing features before splitting\n",
    "            #missed_label_user=missed_label_user.astype(int) # Convert Boolean to int array\n",
    "            #missed_label_user=np.sum(missed_label_user,axis=1)# Sum across columns creating a n_row*1 vector\n",
    "            # If the value for a particular row ==0, no features are missing : Can use that row\n",
    "            #use_labels=np.logical_not(missed_label_user)\n",
    "            #x_user=x_user[use_labels,:]\n",
    "            #y_user=np.nan_to_num(y_user) # Blind way to replace NAN labels in y_train/y_test to 0\n",
    "            y_user[np.isnan(y_user)]=-1 # NEW METHOD: REPLACE NANS WITH -1 INSTEAD OF A FAKE FALSE DATA\n",
    "            # Assuming that if the user hasn't bothered with that label, it means that it wasn't too applicable.\n",
    "            x_user=np.nan_to_num(x_user)\n",
    "            #y_user=y_user[use_labels,:]\n",
    "\n",
    "            copy_x_user=copy.deepcopy(x_user)\n",
    "            # Stacking combined data\n",
    "            temp_x_user=choose_sensors(copy_x_user,used_sensors=sensor_types,summarized_feature_names=feature_names)\n",
    "            X_combined=np.vstack((X_combined,temp_x_user))\n",
    "            Y_combined=np.vstack((Y_combined,y_user))\n",
    "\n",
    "            print('X_shape before removing invalid labels:{}, after:{}'.format(x_sh,x_user.shape))\n",
    "            print('Y_shape before removing invalid labels:{}, after:{}'.format(y_sh,y_user.shape))\n",
    "\n",
    "            # Split each user data into train-test splits .70-.30 as in literature\n",
    "            x_train_u,x_test_u,y_train_u,y_test_u=TT_split(x_user,y_user,test_size=0.30,random_state=random_state)\n",
    "            #m_train,m_test=TT_split(missed_label_user,test_size=0.30,random_state=random_state)\n",
    "\n",
    "            # Removing smart watch features\n",
    "            x_train_u=choose_sensors(x_train_u,used_sensors=sensor_types,summarized_feature_names=feature_names)\n",
    "            x_test_u=choose_sensors(x_test_u,used_sensors=sensor_types,summarized_feature_names=feature_names)\n",
    "\n",
    "            # Stacking data. Will be changed for K-Fold cross-validation\n",
    "            X_train_t=np.vstack((X_train_t,x_train_u))\n",
    "            Y_train_t=np.vstack((Y_train_t,y_train_u))\n",
    "            X_test_t=np.vstack((X_test_t,x_test_u))\n",
    "            Y_test_t=np.vstack((Y_test_t,y_test_u))\n",
    "\n",
    "            print('\\t Per User Training examples:{}, Testing examples:{}'.\n",
    "                  format(y_train_u.shape[0],y_test_u.shape[0]))\n",
    "    assert len(X_train_t)==len(Y_train_t)\n",
    "    assert len(X_test_t)==len(Y_test_t)\n",
    "\n",
    "    print('\\nTraining: X::{} ,Y::{}'.format(X_train_t.shape,Y_train_t.shape))\n",
    "    print('Testing: X::{} ,Y::{}'.format(X_test_t.shape,Y_test_t.shape))\n",
    "\n",
    "    X_combined,_,_=standardize(X_combined) # Standardizing all the training examples/features  \n",
    "    print(\"Pickling data files\")\n",
    "    with open('dataset/pickled/x_combined.pkl','wb') as f:\n",
    "        pickle.dump(X_combined,f)\n",
    "    with open('dataset/pickled/y_combined.pkl','wb') as f:\n",
    "        pickle.dump(Y_combined,f)\n",
    "    # Split datasets\n",
    "    with open('dataset/pickled/x_train.pkl','wb') as f:\n",
    "        pickle.dump(X_train_t,f)\n",
    "    with open('dataset/pickled/y_train.pkl','wb') as f:\n",
    "        pickle.dump(Y_train_t,f)\n",
    "    with open('dataset/pickled/x_test.pkl','wb') as f:\n",
    "        pickle.dump(X_test_t,f)\n",
    "    with open('dataset/pickled/y_test.pkl','wb') as f:\n",
    "        pickle.dump(Y_test_t,f)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Skipping- data files already pickled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and pickling instance weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Skipping cell if the data files were already created previously \n",
    "if not done:\n",
    "    # Creating an instance weight matrix for the training labels\n",
    "    instance_weights=np.zeros_like(Y_combined)\n",
    "    for l in range(len(labelname_user)):\n",
    "        temp_column=Y_train_t[:,l]\n",
    "        count_neg=0\n",
    "        count_0=0\n",
    "        count_1=0\n",
    "        for i in range(len(temp_column)): # n^2 bincount doesn't work with arrays consisting of negative numbers\n",
    "            if (temp_column[i]==-1):\n",
    "                count_neg+=1\n",
    "            elif (temp_column[i]==0):\n",
    "                count_0+=1\n",
    "            elif (temp_column[i]==1):\n",
    "                count_1+=1\n",
    "            else:\n",
    "                raise ValueError(\"Bad Loop\")\n",
    "        for i in range(len(temp_column)):\n",
    "            if (temp_column[i]==-1):\n",
    "                instance_weights[i,l]=0.\n",
    "            elif (temp_column[i]==0):\n",
    "                instance_weights[i,l]=float((count_neg+count_0+count_1)/count_0)\n",
    "            elif (temp_column[i]==1):\n",
    "                instance_weights[i,l]=float((count_neg+count_0+count_1)/count_1)\n",
    "    with open('dataset/pickled/instance_weights.pkl','wb') as f:\n",
    "        pickle.dump(instance_weights,f)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Skipping- instance weight matrix already pickled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous train/test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda-enable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Simple function to run using GPU when available\n",
    "def C(structure):\n",
    "    if torch.cuda.is_available():\n",
    "        device=torch.device(\"cuda\")\n",
    "        return structure.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tackling missing labels using a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create a mask to hide -1 nans before training and then input to a train criterion\n",
    "def mask(criterion,y_true,y_pred,mask_value):\n",
    "    mask=torch.ne(y_true,mask_value).type(torch.cuda.FloatTensor)\n",
    "    # Cast the ByteTensor from elementwise comparison to a FloatTensor\n",
    "    return criterion(torch.mul(y_pred,mask),torch.mul(y_true,mask).type(torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Learning-Rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Linear decreasing LR scheduler\n",
    "def linear_lr_scheduler(optimizer,epoch):\n",
    "    \"\"\"\n",
    "    LR_init=0.1, LR_final=0.01, n_epochs=40\n",
    "    Sets the learning rate to the initial LR decayed by 1.04 every epoch\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr=param_group['lr']\n",
    "    m=-2.25e-3\n",
    "    c=0.1\n",
    "    lr=(epoch*m)+c # Linear LR decay based on a set number of epochs\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr']=lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Norm for weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Adds euclidean regularization to weight matrices\n",
    "def frobenius_norm(model,loss):\n",
    "    regularizer_loss=0\n",
    "    \n",
    "    for m in model.modules():\n",
    "        if isinstance(m,nn.Linear): # Linear layer\n",
    "            frobenius_norm=torch.norm(m.weight,p='fro')\n",
    "            regularizer_loss+=frobenius_norm # Regularization over the weight matrices for linear layers\n",
    "    return loss+0.001*regularizer_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy (Precision, Recall, F1, Support, Balanced Accuracy) metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function for the required accuracy metrics per fold\n",
    "def accuracy(fold,target_labels,y_true,y_pred):\n",
    "    balanced_accuracy_dict={}\n",
    "    print('*'*20)\n",
    "    print('For fold {}'.format(fold))\n",
    "    # Precision, Recall, F1, Support\n",
    "    clf_report=classification_report(y_true=y_true,y_pred=y_pred,\n",
    "                                      target_names=target_labels,output_dict=True)\n",
    "    # Balanced accuracy\n",
    "    for i in range(len(target_labels)):\n",
    "        true_perlabel=y_true[:,i]\n",
    "        pred_perlabel=y_pred[:,i]\n",
    "        bal_acc=balanced_accuracy_score(y_true=true_perlabel,y_pred=pred_perlabel)\n",
    "        print('\\t Label {}:::-> Balanced Accuracy {}'.format(target_labels[i],round(bal_acc,7)))\n",
    "        balanced_accuracy_dict[target_labels[i]]=round(bal_acc,5)\n",
    "    return balanced_accuracy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 5-fold cross-validation\n",
    "def kfold_validation(model,X,Y,instance_weights,n_folds,n_epochs,lr_init,momentum):\n",
    "    # X, Y here are the complete datasets\n",
    "    kfolds=KFold(n_folds,shuffle=False,random_state=random_state).split(X) # Make n_folds\n",
    "    \n",
    "    fold_train_dict={} # Initialize dicts for train balanced accuracy- for each fold\n",
    "    fold_valid_dict={} # Initialize dicts for valid balanced accuracy- for each fold\n",
    "    \n",
    "    for fold, (train_index,val_index) in enumerate(kfolds): # Going through the folds\n",
    "        X_train=X[train_index]\n",
    "        y_train=Y[train_index]\n",
    "        X_valid=X[val_index]\n",
    "        y_valid=Y[val_index]\n",
    "\n",
    "         # Separate instance weight matrix based on the train index\n",
    "        instance_weight_matrix=torch.cuda.FloatTensor(instance_weights[train_index])\n",
    "        # Convert the training data to Variables\n",
    "        X_train=V(torch.cuda.FloatTensor(X_train),requires_grad=True)\n",
    "        y_train=V(torch.cuda.FloatTensor(y_train),requires_grad=False)\n",
    "        X_valid=V(torch.cuda.FloatTensor(X_valid),requires_grad=False)\n",
    "        y_valid=V(torch.cuda.FloatTensor(y_valid),requires_grad=False)\n",
    "        \n",
    "        #model_train=copy.deepcopy(model) # Deepcopy of model\n",
    "        \n",
    "        # Cuda compatible\n",
    "        model=C(model)\n",
    "#         X_train=C(X_train)\n",
    "#         y_train=C(y_train)\n",
    "#         X_valid=C(X_valid)\n",
    "#         y_valid=C(y_valid)\n",
    "        \n",
    "        # Train the network\n",
    "        train(model,X_train,y_train,weights=instance_weight_matrix,\n",
    "            n_epoch=n_epochs,batch_size=bs,lr_init=lr_init,momentum=momentum)\n",
    "        \n",
    "        eval_bool=1\n",
    "        if eval_bool: # Basic methodology to ensure gradients aren't updated\n",
    "            model.eval() # Set to evaluation mode\n",
    "            # On training set\n",
    "            y_train_pred=torch.sigmoid(model(X_train))>=0.5\n",
    "            train_dict_out=accuracy(fold,labelname_user,y_train.cpu(),y_train_pred.cpu())\n",
    "            fold_train_dict[str(fold)]=train_dict_out\n",
    "            # On valid set\n",
    "            y_valid_pred=torch.sigmoid(model(X_valid))>=0.5\n",
    "            valid_dict_out=accuracy(fold,labelname_user,y_valid.cpu(),y_valid_pred.cpu())\n",
    "            fold_valid_dict[str(fold)]=valid_dict_out\n",
    "        eval_bool=0\n",
    "        model.train() # Set to train mode\n",
    "        \n",
    "    return fold_train_dict,fold_valid_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Train function w/BCE loss, linear LR scheduler, instance weights\n",
    "def train(model,X,Y,weights,n_epoch,batch_size,lr_init,momentum):\n",
    "    optimizer=optim.SGD(model.parameters(),lr=lr_init,momentum=momentum)\n",
    "    #criterion=C(nn.BCEWithLogitsLoss())\n",
    "    \n",
    "    # Create dataloaders\n",
    "    # Dataloader creation\n",
    "    # Wrap weights for instance weight tensor along with data & label tensors s.t.\n",
    "    # it can be called properly as a dataloader in batches.\n",
    "    train_dataset=utils.TensorDataset(X,Y,weights)\n",
    "    train_loader=utils.DataLoader(dataset=train_dataset,batch_size=bs\n",
    "                                  ,shuffle=False,drop_last=False)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        done=1\n",
    "        for i,data in enumerate(train_loader,0):\n",
    "            \n",
    "            inputs,labels,weights=data\n",
    "            inputs=V(torch.cuda.FloatTensor(inputs),requires_grad=True)\n",
    "            labels=V(torch.cuda.FloatTensor(labels),requires_grad=False)\n",
    "            weights=V(torch.cuda.FloatTensor(weights))\n",
    "            \n",
    "            criterion=C(nn.BCEWithLogitsLoss(weight=weights))\n",
    "            optimizer.zero_grad()   \n",
    "            sum_total=0\n",
    "            \n",
    "            outputs=model(inputs)\n",
    "\n",
    "            # Zero gradients, backward pass, weight update\n",
    "\n",
    "            if done:\n",
    "                linear_lr_scheduler(optimizer,epoch) # Reduce LR once every epoch\n",
    "                done=0\n",
    "            \n",
    "            loss=criterion(outputs,labels)\n",
    "            #loss=mask(criterion=criterion,y_true=labels,y_pred=output,mask_value=-1)\n",
    "            loss=frobenius_norm(model,loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_total+=loss.item()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                epoch_lr=param_group['lr']\n",
    "                if i%bs==0: # Every minibatch\n",
    "                    print(\"Epoch {}::Minibatch {}::LR {} --> Loss {}\".format(epoch+1,i+1,epoch_lr,sum_total/bs))\n",
    "                    sum_total=0.\n",
    "        #train_predictions=torch.sigmoid(outputs)>=0.5\n",
    "        #accuracy=torc.eq(Y,train_predictions.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Class Classifier: Learning and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Hyperparameter variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sizes for neural networks and other global hyperparameters\n",
    "input_size=X_train_t.shape[-1]\n",
    "hidden_size=16\n",
    "output_size=Y_train_t.shape[-1]\n",
    "n_fold=5\n",
    "n_epoch=40\n",
    "bs=300\n",
    "lr_init=0.1\n",
    "momentum=0.5\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (0 Hidden Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearMLP(\n",
      "  (fc1): Linear(in_features=168, out_features=51, bias=True)\n",
      ")\n",
      "Epoch 1::Minibatch 1::LR 0.1 --> Loss 0.0031396003564198812\n",
      "Epoch 1::Minibatch 301::LR 0.1 --> Loss 0.0037474576632181805\n",
      "Epoch 1::Minibatch 601::LR 0.1 --> Loss 0.001890755295753479\n",
      "Epoch 1::Minibatch 901::LR 0.1 --> Loss 1.8874630331993103e-05\n",
      "Epoch 2::Minibatch 1::LR 0.09775 --> Loss 0.001647792160511017\n",
      "Epoch 2::Minibatch 301::LR 0.09775 --> Loss 0.0025827427705128986\n",
      "Epoch 2::Minibatch 601::LR 0.09775 --> Loss 0.0012849735220273335\n",
      "Epoch 2::Minibatch 901::LR 0.09775 --> Loss 2.015506848692894e-05\n",
      "Epoch 3::Minibatch 1::LR 0.0955 --> Loss 0.0013247607151667276\n",
      "Epoch 3::Minibatch 301::LR 0.0955 --> Loss 0.002299908995628357\n",
      "Epoch 3::Minibatch 601::LR 0.0955 --> Loss 0.001042847235997518\n",
      "Epoch 3::Minibatch 901::LR 0.0955 --> Loss 2.100345523407062e-05\n",
      "Epoch 4::Minibatch 1::LR 0.09325 --> Loss 0.0012416628003120423\n",
      "Epoch 4::Minibatch 301::LR 0.09325 --> Loss 0.002180828054745992\n",
      "Epoch 4::Minibatch 601::LR 0.09325 --> Loss 0.0009065717458724976\n",
      "Epoch 4::Minibatch 901::LR 0.09325 --> Loss 2.159758781393369e-05\n",
      "Epoch 5::Minibatch 1::LR 0.09100000000000001 --> Loss 0.0012076645096143087\n",
      "Epoch 5::Minibatch 301::LR 0.09100000000000001 --> Loss 0.0021257275342941285\n",
      "Epoch 5::Minibatch 601::LR 0.09100000000000001 --> Loss 0.000819394439458847\n",
      "Epoch 5::Minibatch 901::LR 0.09100000000000001 --> Loss 2.2053344485660392e-05\n",
      "Epoch 6::Minibatch 1::LR 0.08875000000000001 --> Loss 0.0011882385611534118\n",
      "Epoch 6::Minibatch 301::LR 0.08875000000000001 --> Loss 0.00209409236907959\n",
      "Epoch 6::Minibatch 601::LR 0.08875000000000001 --> Loss 0.0007597277561823527\n",
      "Epoch 6::Minibatch 901::LR 0.08875000000000001 --> Loss 2.2424013974765935e-05\n",
      "Epoch 7::Minibatch 1::LR 0.08650000000000001 --> Loss 0.001174994707107544\n",
      "Epoch 7::Minibatch 301::LR 0.08650000000000001 --> Loss 0.0020710472265879315\n",
      "Epoch 7::Minibatch 601::LR 0.08650000000000001 --> Loss 0.0007169391214847565\n",
      "Epoch 7::Minibatch 901::LR 0.08650000000000001 --> Loss 2.2736658963064352e-05\n",
      "Epoch 8::Minibatch 1::LR 0.08425 --> Loss 0.0011649842063585917\n",
      "Epoch 8::Minibatch 301::LR 0.08425 --> Loss 0.0020518525441487632\n",
      "Epoch 8::Minibatch 601::LR 0.08425 --> Loss 0.0006851328412691752\n",
      "Epoch 8::Minibatch 901::LR 0.08425 --> Loss 2.3007108829915523e-05\n",
      "Epoch 9::Minibatch 1::LR 0.082 --> Loss 0.001156809429327647\n",
      "Epoch 9::Minibatch 301::LR 0.082 --> Loss 0.002034752170244853\n",
      "Epoch 9::Minibatch 601::LR 0.082 --> Loss 0.0006607719262441\n",
      "Epoch 9::Minibatch 901::LR 0.082 --> Loss 2.3245358218749363e-05\n",
      "Epoch 10::Minibatch 1::LR 0.07975000000000002 --> Loss 0.0011497834324836731\n",
      "Epoch 10::Minibatch 301::LR 0.07975000000000002 --> Loss 0.0020191297928492226\n",
      "Epoch 10::Minibatch 601::LR 0.07975000000000002 --> Loss 0.0006416517992814382\n",
      "Epoch 10::Minibatch 901::LR 0.07975000000000002 --> Loss 2.3457965192695458e-05\n",
      "Epoch 11::Minibatch 1::LR 0.07750000000000001 --> Loss 0.0011435188849767048\n",
      "Epoch 11::Minibatch 301::LR 0.07750000000000001 --> Loss 0.0020046857992808025\n",
      "Epoch 11::Minibatch 601::LR 0.07750000000000001 --> Loss 0.0006263426939646403\n",
      "Epoch 11::Minibatch 901::LR 0.07750000000000001 --> Loss 2.3649434248606363e-05\n",
      "Epoch 12::Minibatch 1::LR 0.07525000000000001 --> Loss 0.0011378506819407145\n",
      "Epoch 12::Minibatch 301::LR 0.07525000000000001 --> Loss 0.0019913315773010254\n",
      "Epoch 12::Minibatch 601::LR 0.07525000000000001 --> Loss 0.0006138866146405537\n",
      "Epoch 12::Minibatch 901::LR 0.07525000000000001 --> Loss 2.3823012597858905e-05\n",
      "Epoch 13::Minibatch 1::LR 0.07300000000000001 --> Loss 0.0011326247453689576\n",
      "Epoch 13::Minibatch 301::LR 0.07300000000000001 --> Loss 0.0019789713621139525\n",
      "Epoch 13::Minibatch 601::LR 0.07300000000000001 --> Loss 0.0006036176780859629\n",
      "Epoch 13::Minibatch 901::LR 0.07300000000000001 --> Loss 2.3981227229038875e-05\n",
      "Epoch 14::Minibatch 1::LR 0.07075000000000001 --> Loss 0.0011277279257774352\n",
      "Epoch 14::Minibatch 301::LR 0.07075000000000001 --> Loss 0.0019675676027933755\n",
      "Epoch 14::Minibatch 601::LR 0.07075000000000001 --> Loss 0.0005950636665026347\n",
      "Epoch 14::Minibatch 901::LR 0.07075000000000001 --> Loss 2.412601994971434e-05\n",
      "Epoch 15::Minibatch 1::LR 0.0685 --> Loss 0.0011230629682540893\n",
      "Epoch 15::Minibatch 301::LR 0.0685 --> Loss 0.0019570362567901613\n",
      "Epoch 15::Minibatch 601::LR 0.0685 --> Loss 0.0005878795683383942\n",
      "Epoch 15::Minibatch 901::LR 0.0685 --> Loss 2.425899108250936e-05\n",
      "Epoch 16::Minibatch 1::LR 0.06625 --> Loss 0.001118595004081726\n",
      "Epoch 16::Minibatch 301::LR 0.06625 --> Loss 0.0019473344087600707\n",
      "Epoch 16::Minibatch 601::LR 0.06625 --> Loss 0.0005818118155002594\n",
      "Epoch 16::Minibatch 901::LR 0.06625 --> Loss 2.438141033053398e-05\n",
      "Epoch 17::Minibatch 1::LR 0.064 --> Loss 0.0011142855882644654\n",
      "Epoch 17::Minibatch 301::LR 0.064 --> Loss 0.0019383790095647176\n",
      "Epoch 17::Minibatch 601::LR 0.064 --> Loss 0.0005766661465168\n",
      "Epoch 17::Minibatch 901::LR 0.064 --> Loss 2.4494345610340436e-05\n",
      "Epoch 18::Minibatch 1::LR 0.061750000000000006 --> Loss 0.0011101373036702473\n",
      "Epoch 18::Minibatch 301::LR 0.061750000000000006 --> Loss 0.0019301241636276245\n",
      "Epoch 18::Minibatch 601::LR 0.061750000000000006 --> Loss 0.0005722945928573609\n",
      "Epoch 18::Minibatch 901::LR 0.061750000000000006 --> Loss 2.4598658395310243e-05\n",
      "Epoch 19::Minibatch 1::LR 0.05950000000000001 --> Loss 0.0011061153809229533\n",
      "Epoch 19::Minibatch 301::LR 0.05950000000000001 --> Loss 0.0019224892059961955\n",
      "Epoch 19::Minibatch 601::LR 0.05950000000000001 --> Loss 0.0005685792863368988\n",
      "Epoch 19::Minibatch 901::LR 0.05950000000000001 --> Loss 2.4695113922158877e-05\n",
      "Epoch 20::Minibatch 1::LR 0.05725000000000001 --> Loss 0.001102229356765747\n",
      "Epoch 20::Minibatch 301::LR 0.05725000000000001 --> Loss 0.0019154347976048788\n",
      "Epoch 20::Minibatch 601::LR 0.05725000000000001 --> Loss 0.0005654301246007284\n",
      "Epoch 20::Minibatch 901::LR 0.05725000000000001 --> Loss 2.4784340833624204e-05\n",
      "Epoch 21::Minibatch 1::LR 0.05500000000000001 --> Loss 0.0010984456539154053\n",
      "Epoch 21::Minibatch 301::LR 0.05500000000000001 --> Loss 0.001908893585205078\n",
      "Epoch 21::Minibatch 601::LR 0.05500000000000001 --> Loss 0.0005627732475598653\n",
      "Epoch 21::Minibatch 901::LR 0.05500000000000001 --> Loss 2.4866887057820954e-05\n",
      "Epoch 22::Minibatch 1::LR 0.05275000000000001 --> Loss 0.0010947684446970622\n",
      "Epoch 22::Minibatch 301::LR 0.05275000000000001 --> Loss 0.0019028258323669433\n",
      "Epoch 22::Minibatch 601::LR 0.05275000000000001 --> Loss 0.0005605504910151164\n",
      "Epoch 22::Minibatch 901::LR 0.05275000000000001 --> Loss 2.494323377807935e-05\n",
      "Epoch 23::Minibatch 1::LR 0.05050000000000001 --> Loss 0.001091171403725942\n",
      "Epoch 23::Minibatch 301::LR 0.05050000000000001 --> Loss 0.0018971772988637288\n",
      "Epoch 23::Minibatch 601::LR 0.05050000000000001 --> Loss 0.0005587138235569\n",
      "Epoch 23::Minibatch 901::LR 0.05050000000000001 --> Loss 2.501380629837513e-05\n",
      "Epoch 24::Minibatch 1::LR 0.04825000000000001 --> Loss 0.0010876540342966716\n",
      "Epoch 24::Minibatch 301::LR 0.04825000000000001 --> Loss 0.0018919124205907187\n",
      "Epoch 24::Minibatch 601::LR 0.04825000000000001 --> Loss 0.0005572251478830973\n",
      "Epoch 24::Minibatch 901::LR 0.04825000000000001 --> Loss 2.5078946103652317e-05\n",
      "Epoch 25::Minibatch 1::LR 0.04600000000000001 --> Loss 0.0010842007398605346\n",
      "Epoch 25::Minibatch 301::LR 0.04600000000000001 --> Loss 0.001886988083521525\n",
      "Epoch 25::Minibatch 601::LR 0.04600000000000001 --> Loss 0.0005560519794623058\n",
      "Epoch 25::Minibatch 901::LR 0.04600000000000001 --> Loss 2.5138983813424904e-05\n",
      "Epoch 26::Minibatch 1::LR 0.04375000000000001 --> Loss 0.0010808159907658894\n",
      "Epoch 26::Minibatch 301::LR 0.04375000000000001 --> Loss 0.0018823728958765665\n",
      "Epoch 26::Minibatch 601::LR 0.04375000000000001 --> Loss 0.0005551670988400777\n",
      "Epoch 26::Minibatch 901::LR 0.04375000000000001 --> Loss 2.5194215898712475e-05\n",
      "Epoch 27::Minibatch 1::LR 0.04150000000000001 --> Loss 0.0010774901509284973\n",
      "Epoch 27::Minibatch 301::LR 0.04150000000000001 --> Loss 0.0018780352671941122\n",
      "Epoch 27::Minibatch 601::LR 0.04150000000000001 --> Loss 0.0005545494457085927\n",
      "Epoch 27::Minibatch 901::LR 0.04150000000000001 --> Loss 2.5244886055588724e-05\n",
      "Epoch 28::Minibatch 1::LR 0.03925000000000001 --> Loss 0.0010742167631785074\n",
      "Epoch 28::Minibatch 301::LR 0.03925000000000001 --> Loss 0.0018739495674769083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28::Minibatch 601::LR 0.03925000000000001 --> Loss 0.0005541793008645375\n",
      "Epoch 28::Minibatch 901::LR 0.03925000000000001 --> Loss 2.5291225562493006e-05\n",
      "Epoch 29::Minibatch 1::LR 0.037000000000000005 --> Loss 0.0010710020860036215\n",
      "Epoch 29::Minibatch 301::LR 0.037000000000000005 --> Loss 0.0018700953324635823\n",
      "Epoch 29::Minibatch 601::LR 0.037000000000000005 --> Loss 0.0005540407697359721\n",
      "Epoch 29::Minibatch 901::LR 0.037000000000000005 --> Loss 2.533344396700462e-05\n",
      "Epoch 30::Minibatch 1::LR 0.03475000000000002 --> Loss 0.001067836582660675\n",
      "Epoch 30::Minibatch 301::LR 0.03475000000000002 --> Loss 0.0018664437532424926\n",
      "Epoch 30::Minibatch 601::LR 0.03475000000000002 --> Loss 0.000554119348526001\n",
      "Epoch 30::Minibatch 901::LR 0.03475000000000002 --> Loss 2.5371736846864223e-05\n",
      "Epoch 31::Minibatch 1::LR 0.032500000000000015 --> Loss 0.0010647230346997579\n",
      "Epoch 31::Minibatch 301::LR 0.032500000000000015 --> Loss 0.0018629831075668334\n",
      "Epoch 31::Minibatch 601::LR 0.032500000000000015 --> Loss 0.0005544032653172811\n",
      "Epoch 31::Minibatch 901::LR 0.032500000000000015 --> Loss 2.5406273392339546e-05\n",
      "Epoch 32::Minibatch 1::LR 0.030250000000000013 --> Loss 0.0010616614421208698\n",
      "Epoch 32::Minibatch 301::LR 0.030250000000000013 --> Loss 0.0018596998850504557\n",
      "Epoch 32::Minibatch 601::LR 0.030250000000000013 --> Loss 0.0005548781156539917\n",
      "Epoch 32::Minibatch 901::LR 0.030250000000000013 --> Loss 2.543720416724682e-05\n",
      "Epoch 33::Minibatch 1::LR 0.02800000000000001 --> Loss 0.0010586504141489664\n",
      "Epoch 33::Minibatch 301::LR 0.02800000000000001 --> Loss 0.0018565821647644042\n",
      "Epoch 33::Minibatch 601::LR 0.02800000000000001 --> Loss 0.0005555326243241628\n",
      "Epoch 33::Minibatch 901::LR 0.02800000000000001 --> Loss 2.5464696809649466e-05\n",
      "Epoch 34::Minibatch 1::LR 0.02575000000000001 --> Loss 0.0010556900501251221\n",
      "Epoch 34::Minibatch 301::LR 0.02575000000000001 --> Loss 0.0018536255757013956\n",
      "Epoch 34::Minibatch 601::LR 0.02575000000000001 --> Loss 0.0005563533802827199\n",
      "Epoch 34::Minibatch 901::LR 0.02575000000000001 --> Loss 2.548888325691223e-05\n",
      "Epoch 35::Minibatch 1::LR 0.023500000000000007 --> Loss 0.0010527827342351277\n",
      "Epoch 35::Minibatch 301::LR 0.023500000000000007 --> Loss 0.0018508299191792807\n",
      "Epoch 35::Minibatch 601::LR 0.023500000000000007 --> Loss 0.0005573287109533945\n",
      "Epoch 35::Minibatch 901::LR 0.023500000000000007 --> Loss 2.5509906311829885e-05\n",
      "Epoch 36::Minibatch 1::LR 0.021250000000000005 --> Loss 0.0010499319434165955\n",
      "Epoch 36::Minibatch 301::LR 0.021250000000000005 --> Loss 0.0018481934070587159\n",
      "Epoch 36::Minibatch 601::LR 0.021250000000000005 --> Loss 0.0005584429701169332\n",
      "Epoch 36::Minibatch 901::LR 0.021250000000000005 --> Loss 2.5527911881605783e-05\n",
      "Epoch 37::Minibatch 1::LR 0.019000000000000017 --> Loss 0.0010471413532892862\n",
      "Epoch 37::Minibatch 301::LR 0.019000000000000017 --> Loss 0.0018457196156183879\n",
      "Epoch 37::Minibatch 601::LR 0.019000000000000017 --> Loss 0.0005596782763799032\n",
      "Epoch 37::Minibatch 901::LR 0.019000000000000017 --> Loss 2.5543028799196083e-05\n",
      "Epoch 38::Minibatch 1::LR 0.016750000000000015 --> Loss 0.0010444204012552897\n",
      "Epoch 38::Minibatch 301::LR 0.016750000000000015 --> Loss 0.0018434067567189535\n",
      "Epoch 38::Minibatch 601::LR 0.016750000000000015 --> Loss 0.0005610159039497375\n",
      "Epoch 38::Minibatch 901::LR 0.016750000000000015 --> Loss 2.555538744976123e-05\n",
      "Epoch 39::Minibatch 1::LR 0.014500000000000013 --> Loss 0.001041776140530904\n",
      "Epoch 39::Minibatch 301::LR 0.014500000000000013 --> Loss 0.0018412633736928305\n",
      "Epoch 39::Minibatch 601::LR 0.014500000000000013 --> Loss 0.0005624301234881084\n",
      "Epoch 39::Minibatch 901::LR 0.014500000000000013 --> Loss 2.5565153919160366e-05\n",
      "Epoch 40::Minibatch 1::LR 0.012250000000000011 --> Loss 0.001039226253827413\n",
      "Epoch 40::Minibatch 301::LR 0.012250000000000011 --> Loss 0.0018392809232076009\n",
      "Epoch 40::Minibatch 601::LR 0.012250000000000011 --> Loss 0.0005638871093591054\n",
      "Epoch 40::Minibatch 901::LR 0.012250000000000011 --> Loss 2.5572466353575388e-05\n",
      "********************\n",
      "For fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsrishankar/Desktop/Self_study/custom_dl_env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nsrishankar/Desktop/Self_study/custom_dl_env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Label LYING_DOWN:::-> Balanced Accuracy 0.830141\n",
      "\t Label SITTING:::-> Balanced Accuracy 0.7173173\n",
      "\t Label FIX_walking:::-> Balanced Accuracy 0.5948963\n",
      "\t Label FIX_running:::-> Balanced Accuracy 0.5054233\n",
      "\t Label BICYCLING:::-> Balanced Accuracy 0.548427\n",
      "\t Label SLEEPING:::-> Balanced Accuracy 0.7890097\n",
      "\t Label LAB_WORK:::-> Balanced Accuracy 0.5505571\n",
      "\t Label IN_CLASS:::-> Balanced Accuracy 0.5335475\n",
      "\t Label IN_A_MEETING:::-> Balanced Accuracy 0.5115785\n",
      "\t Label LOC_main_workplace:::-> Balanced Accuracy 0.6625263\n",
      "\t Label OR_indoors:::-> Balanced Accuracy 0.7080801\n",
      "\t Label OR_outside:::-> Balanced Accuracy 0.5149387\n",
      "\t Label IN_A_CAR:::-> Balanced Accuracy 0.5251241\n",
      "\t Label ON_A_BUS:::-> Balanced Accuracy 0.4997885\n",
      "\t Label DRIVE_-_I_M_THE_DRIVER:::-> Balanced Accuracy 0.6033217\n",
      "\t Label DRIVE_-_I_M_A_PASSENGER:::-> Balanced Accuracy 0.5070209\n",
      "\t Label LOC_home:::-> Balanced Accuracy 0.7395226\n",
      "\t Label FIX_restaurant:::-> Balanced Accuracy 0.4999717\n",
      "\t Label PHONE_IN_POCKET:::-> Balanced Accuracy 0.5369641\n",
      "\t Label OR_exercise:::-> Balanced Accuracy 0.5487978\n",
      "\t Label COOKING:::-> Balanced Accuracy 0.4999832\n",
      "\t Label SHOPPING:::-> Balanced Accuracy 0.499995\n",
      "\t Label STROLLING:::-> Balanced Accuracy 0.4999552\n",
      "\t Label DRINKING__ALCOHOL_:::-> Balanced Accuracy 0.499985\n",
      "\t Label BATHING_-_SHOWER:::-> Balanced Accuracy 0.496917\n",
      "\t Label CLEANING:::-> Balanced Accuracy 0.4999666\n",
      "\t Label DOING_LAUNDRY:::-> Balanced Accuracy 0.499995\n",
      "\t Label WASHING_DISHES:::-> Balanced Accuracy 0.499985\n",
      "\t Label WATCHING_TV:::-> Balanced Accuracy 0.510476\n",
      "\t Label SURFING_THE_INTERNET:::-> Balanced Accuracy 0.5401448\n",
      "\t Label AT_A_PARTY:::-> Balanced Accuracy 0.5005311\n",
      "\t Label AT_A_BAR:::-> Balanced Accuracy 0.4999834\n",
      "\t Label LOC_beach:::-> Balanced Accuracy 0.499995\n",
      "\t Label SINGING:::-> Balanced Accuracy 0.499995\n",
      "\t Label TALKING:::-> Balanced Accuracy 0.5263939\n",
      "\t Label COMPUTER_WORK:::-> Balanced Accuracy 0.6034234\n",
      "\t Label EATING:::-> Balanced Accuracy 0.5000438\n",
      "\t Label TOILET:::-> Balanced Accuracy 0.4993129\n",
      "\t Label GROOMING:::-> Balanced Accuracy 0.4999583\n",
      "\t Label DRESSING:::-> Balanced Accuracy 0.4999768\n",
      "\t Label AT_THE_GYM:::-> Balanced Accuracy 0.4999967\n",
      "\t Label STAIRS_-_GOING_UP:::-> Balanced Accuracy 0.4999934\n",
      "\t Label STAIRS_-_GOING_DOWN:::-> Balanced Accuracy 0.4999934\n",
      "\t Label ELEVATOR:::-> Balanced Accuracy 0.4999934\n",
      "\t Label OR_standing:::-> Balanced Accuracy 0.5005022\n",
      "\t Label AT_SCHOOL:::-> Balanced Accuracy 0.5566021\n",
      "\t Label PHONE_IN_HAND:::-> Balanced Accuracy 0.5021136\n",
      "\t Label PHONE_IN_BAG:::-> Balanced Accuracy 0.5573304\n",
      "\t Label PHONE_ON_TABLE:::-> Balanced Accuracy 0.6410012\n",
      "\t Label WITH_CO-WORKERS:::-> Balanced Accuracy 0.4999814\n",
      "\t Label WITH_FRIENDS:::-> Balanced Accuracy 0.5362243\n",
      "********************\n",
      "For fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsrishankar/Desktop/Self_study/custom_dl_env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Label LYING_DOWN:::-> Balanced Accuracy 0.8116697\n",
      "\t Label SITTING:::-> Balanced Accuracy 0.6619347\n",
      "\t Label FIX_walking:::-> Balanced Accuracy 0.549993\n",
      "\t Label FIX_running:::-> Balanced Accuracy 0.5071534\n",
      "\t Label BICYCLING:::-> Balanced Accuracy 0.5198238\n",
      "\t Label SLEEPING:::-> Balanced Accuracy 0.7586737\n",
      "\t Label LAB_WORK:::-> Balanced Accuracy 0.5081321\n",
      "\t Label IN_CLASS:::-> Balanced Accuracy 0.4999866\n",
      "\t Label IN_A_MEETING:::-> Balanced Accuracy 0.5042194\n",
      "\t Label LOC_main_workplace:::-> Balanced Accuracy 0.5559077\n",
      "\t Label OR_indoors:::-> Balanced Accuracy 0.6907208\n",
      "\t Label OR_outside:::-> Balanced Accuracy 0.5045458\n",
      "\t Label IN_A_CAR:::-> Balanced Accuracy 0.5008255\n",
      "\t Label ON_A_BUS:::-> Balanced Accuracy 0.4999801\n",
      "\t Label DRIVE_-_I_M_THE_DRIVER:::-> Balanced Accuracy 0.5380139\n",
      "\t Label DRIVE_-_I_M_A_PASSENGER:::-> Balanced Accuracy 0.5064927\n",
      "\t Label LOC_home:::-> Balanced Accuracy 0.6470004\n",
      "\t Label FIX_restaurant:::-> Balanced Accuracy 0.49998\n",
      "\t Label PHONE_IN_POCKET:::-> Balanced Accuracy 0.5201558\n",
      "\t Label OR_exercise:::-> Balanced Accuracy 0.512622\n",
      "\t Label COOKING:::-> Balanced Accuracy 0.4999467\n",
      "\t Label SHOPPING:::-> Balanced Accuracy 0.49998\n",
      "\t Label STROLLING:::-> Balanced Accuracy 0.4999735\n",
      "\t Label DRINKING__ALCOHOL_:::-> Balanced Accuracy 0.4999867\n",
      "\t Label BATHING_-_SHOWER:::-> Balanced Accuracy 0.5\n",
      "\t Label CLEANING:::-> Balanced Accuracy 0.4999731\n",
      "\t Label DOING_LAUNDRY:::-> Balanced Accuracy 0.4999867\n",
      "\t Label WASHING_DISHES:::-> Balanced Accuracy 0.4999469\n",
      "\t Label WATCHING_TV:::-> Balanced Accuracy 0.5076293\n",
      "\t Label SURFING_THE_INTERNET:::-> Balanced Accuracy 0.4971424\n",
      "\t Label AT_A_PARTY:::-> Balanced Accuracy 0.4999469\n",
      "\t Label AT_A_BAR:::-> Balanced Accuracy 0.5\n",
      "\t Label LOC_beach:::-> Balanced Accuracy 0.5\n",
      "\t Label SINGING:::-> Balanced Accuracy 0.4999668\n",
      "\t Label TALKING:::-> Balanced Accuracy 0.5028076\n",
      "\t Label COMPUTER_WORK:::-> Balanced Accuracy 0.5292381\n",
      "\t Label EATING:::-> Balanced Accuracy 0.4999387\n",
      "\t Label TOILET:::-> Balanced Accuracy 0.49998\n",
      "\t Label GROOMING:::-> Balanced Accuracy 0.5020645\n",
      "\t Label DRESSING:::-> Balanced Accuracy 0.5\n",
      "\t Label AT_THE_GYM:::-> Balanced Accuracy 0.49998\n",
      "\t Label STAIRS_-_GOING_UP:::-> Balanced Accuracy 0.4999801\n",
      "\t Label STAIRS_-_GOING_DOWN:::-> Balanced Accuracy 0.4999801\n",
      "\t Label ELEVATOR:::-> Balanced Accuracy 0.4999602\n",
      "\t Label OR_standing:::-> Balanced Accuracy 0.4999848\n",
      "\t Label AT_SCHOOL:::-> Balanced Accuracy 0.5279902\n",
      "\t Label PHONE_IN_HAND:::-> Balanced Accuracy 0.5015681\n",
      "\t Label PHONE_IN_BAG:::-> Balanced Accuracy 0.5002913\n",
      "\t Label PHONE_ON_TABLE:::-> Balanced Accuracy 0.6258463\n",
      "\t Label WITH_CO-WORKERS:::-> Balanced Accuracy 0.4999866\n",
      "\t Label WITH_FRIENDS:::-> Balanced Accuracy 0.5012134\n",
      "Epoch 1::Minibatch 1::LR 0.1 --> Loss 0.003205339511235555\n",
      "Epoch 1::Minibatch 301::LR 0.1 --> Loss 0.0018047791719436644\n",
      "Epoch 1::Minibatch 601::LR 0.1 --> Loss 0.0005300949513912201\n",
      "Epoch 1::Minibatch 901::LR 0.1 --> Loss 2.5137267075479032e-05\n",
      "Epoch 2::Minibatch 1::LR 0.09775 --> Loss 0.002989921967188517\n",
      "Epoch 2::Minibatch 301::LR 0.09775 --> Loss 0.0018519554535547892\n",
      "Epoch 2::Minibatch 601::LR 0.09775 --> Loss 0.0005268757045269012\n",
      "Epoch 2::Minibatch 901::LR 0.09775 --> Loss 2.5018248707056045e-05\n",
      "Epoch 3::Minibatch 1::LR 0.0955 --> Loss 0.0028287140528361\n",
      "Epoch 3::Minibatch 301::LR 0.0955 --> Loss 0.001865504781405131\n",
      "Epoch 3::Minibatch 601::LR 0.0955 --> Loss 0.0005252680679162343\n",
      "Epoch 3::Minibatch 901::LR 0.0955 --> Loss 2.5032195262610914e-05\n",
      "Epoch 4::Minibatch 1::LR 0.09325 --> Loss 0.0027219690879185993\n",
      "Epoch 4::Minibatch 301::LR 0.09325 --> Loss 0.0018704734245936076\n",
      "Epoch 4::Minibatch 601::LR 0.09325 --> Loss 0.0005241681138674418\n",
      "Epoch 4::Minibatch 901::LR 0.09325 --> Loss 2.5106606384118397e-05\n",
      "Epoch 5::Minibatch 1::LR 0.09100000000000001 --> Loss 0.0026449420054753623\n",
      "Epoch 5::Minibatch 301::LR 0.09100000000000001 --> Loss 0.0018715977668762207\n",
      "Epoch 5::Minibatch 601::LR 0.09100000000000001 --> Loss 0.0005232442418734233\n",
      "Epoch 5::Minibatch 901::LR 0.09100000000000001 --> Loss 2.5210396076242128e-05\n",
      "Epoch 6::Minibatch 1::LR 0.08875000000000001 --> Loss 0.0025860901673634847\n",
      "Epoch 6::Minibatch 301::LR 0.08875000000000001 --> Loss 0.0018710583448410035\n",
      "Epoch 6::Minibatch 601::LR 0.08875000000000001 --> Loss 0.0005224075416723887\n",
      "Epoch 6::Minibatch 901::LR 0.08875000000000001 --> Loss 2.532868335644404e-05\n",
      "Epoch 7::Minibatch 1::LR 0.08650000000000001 --> Loss 0.002539098660151164\n",
      "Epoch 7::Minibatch 301::LR 0.08650000000000001 --> Loss 0.0018692902723948161\n",
      "Epoch 7::Minibatch 601::LR 0.08650000000000001 --> Loss 0.0005216326316197713\n",
      "Epoch 7::Minibatch 901::LR 0.08650000000000001 --> Loss 2.545339831461509e-05\n",
      "Epoch 8::Minibatch 1::LR 0.08425 --> Loss 0.002500415643056234\n",
      "Epoch 8::Minibatch 301::LR 0.08425 --> Loss 0.0018671423196792603\n",
      "Epoch 8::Minibatch 601::LR 0.08425 --> Loss 0.000520909974972407\n",
      "Epoch 8::Minibatch 901::LR 0.08425 --> Loss 2.5579975917935372e-05\n",
      "Epoch 9::Minibatch 1::LR 0.082 --> Loss 0.0024677203098932904\n",
      "Epoch 9::Minibatch 301::LR 0.082 --> Loss 0.0018645966053009034\n",
      "Epoch 9::Minibatch 601::LR 0.082 --> Loss 0.0005202405154705048\n",
      "Epoch 9::Minibatch 901::LR 0.082 --> Loss 2.5705695152282714e-05\n",
      "Epoch 10::Minibatch 1::LR 0.07975000000000002 --> Loss 0.0024395916859308877\n",
      "Epoch 10::Minibatch 301::LR 0.07975000000000002 --> Loss 0.0018620288372039795\n",
      "Epoch 10::Minibatch 601::LR 0.07975000000000002 --> Loss 0.0005196204781532288\n",
      "Epoch 10::Minibatch 901::LR 0.07975000000000002 --> Loss 2.5828944829603035e-05\n",
      "Epoch 11::Minibatch 1::LR 0.07750000000000001 --> Loss 0.0024149902661641438\n",
      "Epoch 11::Minibatch 301::LR 0.07750000000000001 --> Loss 0.0018593935171763102\n",
      "Epoch 11::Minibatch 601::LR 0.07750000000000001 --> Loss 0.0005190528432528178\n",
      "Epoch 11::Minibatch 901::LR 0.07750000000000001 --> Loss 2.594871601710717e-05\n",
      "Epoch 12::Minibatch 1::LR 0.07525000000000001 --> Loss 0.0023932133118311563\n",
      "Epoch 12::Minibatch 301::LR 0.07525000000000001 --> Loss 0.001856835683186849\n",
      "Epoch 12::Minibatch 601::LR 0.07525000000000001 --> Loss 0.0005185367663701375\n",
      "Epoch 12::Minibatch 901::LR 0.07525000000000001 --> Loss 2.606441887716452e-05\n",
      "Epoch 13::Minibatch 1::LR 0.07300000000000001 --> Loss 0.002373725771903992\n",
      "Epoch 13::Minibatch 301::LR 0.07300000000000001 --> Loss 0.0018543352683385212\n",
      "Epoch 13::Minibatch 601::LR 0.07300000000000001 --> Loss 0.0005180719991525015\n",
      "Epoch 13::Minibatch 901::LR 0.07300000000000001 --> Loss 2.6175683985153834e-05\n",
      "Epoch 14::Minibatch 1::LR 0.07075000000000001 --> Loss 0.0023561360438664756\n",
      "Epoch 14::Minibatch 301::LR 0.07075000000000001 --> Loss 0.0018519399563471477\n",
      "Epoch 14::Minibatch 601::LR 0.07075000000000001 --> Loss 0.0005176589886347453\n",
      "Epoch 14::Minibatch 901::LR 0.07075000000000001 --> Loss 2.6282329733173054e-05\n",
      "Epoch 15::Minibatch 1::LR 0.0685 --> Loss 0.0023401196797688804\n",
      "Epoch 15::Minibatch 301::LR 0.0685 --> Loss 0.0018496421972910564\n",
      "Epoch 15::Minibatch 601::LR 0.0685 --> Loss 0.0005172974367936452\n",
      "Epoch 15::Minibatch 901::LR 0.0685 --> Loss 2.6384281615416208e-05\n",
      "Epoch 16::Minibatch 1::LR 0.06625 --> Loss 0.0023254557450612385\n",
      "Epoch 16::Minibatch 301::LR 0.06625 --> Loss 0.0018474457661310831\n",
      "Epoch 16::Minibatch 601::LR 0.06625 --> Loss 0.0005169874926408132\n",
      "Epoch 16::Minibatch 901::LR 0.06625 --> Loss 2.6481539631883303e-05\n",
      "Epoch 17::Minibatch 1::LR 0.064 --> Loss 0.0023119304577509563\n",
      "Epoch 17::Minibatch 301::LR 0.064 --> Loss 0.0018453510602315266\n",
      "Epoch 17::Minibatch 601::LR 0.064 --> Loss 0.0005167284111181894\n",
      "Epoch 17::Minibatch 901::LR 0.064 --> Loss 2.657412551343441e-05\n",
      "Epoch 18::Minibatch 1::LR 0.061750000000000006 --> Loss 0.0022993940114974976\n",
      "Epoch 18::Minibatch 301::LR 0.061750000000000006 --> Loss 0.0018433564901351928\n",
      "Epoch 18::Minibatch 601::LR 0.061750000000000006 --> Loss 0.0005165207882722219\n",
      "Epoch 18::Minibatch 901::LR 0.061750000000000006 --> Loss 2.6662138601144155e-05\n",
      "Epoch 19::Minibatch 1::LR 0.05950000000000001 --> Loss 0.002287713686625163\n",
      "Epoch 19::Minibatch 301::LR 0.05950000000000001 --> Loss 0.0018414564927419026\n",
      "Epoch 19::Minibatch 601::LR 0.05950000000000001 --> Loss 0.0005163649221261342\n",
      "Epoch 19::Minibatch 901::LR 0.05950000000000001 --> Loss 2.674563166995843e-05\n",
      "Epoch 20::Minibatch 1::LR 0.05725000000000001 --> Loss 0.0022767895460128786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20::Minibatch 301::LR 0.05725000000000001 --> Loss 0.0018396409352620443\n",
      "Epoch 20::Minibatch 601::LR 0.05725000000000001 --> Loss 0.0005162618060906728\n",
      "Epoch 20::Minibatch 901::LR 0.05725000000000001 --> Loss 2.6824722687403362e-05\n",
      "Epoch 21::Minibatch 1::LR 0.05500000000000001 --> Loss 0.002266530990600586\n",
      "Epoch 21::Minibatch 301::LR 0.05500000000000001 --> Loss 0.0018379094203313192\n",
      "Epoch 21::Minibatch 601::LR 0.05500000000000001 --> Loss 0.0005162115891774496\n",
      "Epoch 21::Minibatch 901::LR 0.05500000000000001 --> Loss 2.6899523412187894e-05\n",
      "Epoch 22::Minibatch 1::LR 0.05275000000000001 --> Loss 0.0022568613290786742\n",
      "Epoch 22::Minibatch 301::LR 0.05275000000000001 --> Loss 0.0018362532059351603\n",
      "Epoch 22::Minibatch 601::LR 0.05275000000000001 --> Loss 0.0005162160098552704\n",
      "Epoch 22::Minibatch 901::LR 0.05275000000000001 --> Loss 2.6970161125063898e-05\n",
      "Epoch 23::Minibatch 1::LR 0.05050000000000001 --> Loss 0.0022477235396703085\n",
      "Epoch 23::Minibatch 301::LR 0.05050000000000001 --> Loss 0.0018346710999806721\n",
      "Epoch 23::Minibatch 601::LR 0.05050000000000001 --> Loss 0.00051627516746521\n",
      "Epoch 23::Minibatch 901::LR 0.05050000000000001 --> Loss 2.7036753793557484e-05\n",
      "Epoch 24::Minibatch 1::LR 0.04825000000000001 --> Loss 0.002239063580830892\n",
      "Epoch 24::Minibatch 301::LR 0.04825000000000001 --> Loss 0.0018331571420033773\n",
      "Epoch 24::Minibatch 601::LR 0.04825000000000001 --> Loss 0.000516391396522522\n",
      "Epoch 24::Minibatch 901::LR 0.04825000000000001 --> Loss 2.7099428698420525e-05\n",
      "Epoch 25::Minibatch 1::LR 0.04600000000000001 --> Loss 0.0022308367490768435\n",
      "Epoch 25::Minibatch 301::LR 0.04600000000000001 --> Loss 0.0018317071596781413\n",
      "Epoch 25::Minibatch 601::LR 0.04600000000000001 --> Loss 0.0005165650943915049\n",
      "Epoch 25::Minibatch 901::LR 0.04600000000000001 --> Loss 2.7158291389544804e-05\n",
      "Epoch 26::Minibatch 1::LR 0.04375000000000001 --> Loss 0.0022230058908462524\n",
      "Epoch 26::Minibatch 301::LR 0.04375000000000001 --> Loss 0.0018303165833155313\n",
      "Epoch 26::Minibatch 601::LR 0.04375000000000001 --> Loss 0.0005167967081069946\n",
      "Epoch 26::Minibatch 901::LR 0.04375000000000001 --> Loss 2.721343499918779e-05\n",
      "Epoch 27::Minibatch 1::LR 0.04150000000000001 --> Loss 0.0022155416011810304\n",
      "Epoch 27::Minibatch 301::LR 0.04150000000000001 --> Loss 0.0018289794524510702\n",
      "Epoch 27::Minibatch 601::LR 0.04150000000000001 --> Loss 0.0005170892179012298\n",
      "Epoch 27::Minibatch 901::LR 0.04150000000000001 --> Loss 2.7264955764015517e-05\n",
      "Epoch 28::Minibatch 1::LR 0.03925000000000001 --> Loss 0.00220841387907664\n",
      "Epoch 28::Minibatch 301::LR 0.03925000000000001 --> Loss 0.0018276957670847575\n",
      "Epoch 28::Minibatch 601::LR 0.03925000000000001 --> Loss 0.0005174428721268972\n",
      "Epoch 28::Minibatch 901::LR 0.03925000000000001 --> Loss 2.7312984069188437e-05\n",
      "Epoch 29::Minibatch 1::LR 0.037000000000000005 --> Loss 0.0022015961011250815\n",
      "Epoch 29::Minibatch 301::LR 0.037000000000000005 --> Loss 0.0018264639377593994\n",
      "Epoch 29::Minibatch 601::LR 0.037000000000000005 --> Loss 0.0005178582668304443\n",
      "Epoch 29::Minibatch 901::LR 0.037000000000000005 --> Loss 2.7357619255781174e-05\n",
      "Epoch 30::Minibatch 1::LR 0.03475000000000002 --> Loss 0.0021950717767079673\n",
      "Epoch 30::Minibatch 301::LR 0.03475000000000002 --> Loss 0.0018252791961034139\n",
      "Epoch 30::Minibatch 601::LR 0.03475000000000002 --> Loss 0.0005183351039886474\n",
      "Epoch 30::Minibatch 901::LR 0.03475000000000002 --> Loss 2.7398963769276937e-05\n",
      "Epoch 31::Minibatch 1::LR 0.032500000000000015 --> Loss 0.0021888269980748494\n",
      "Epoch 31::Minibatch 301::LR 0.032500000000000015 --> Loss 0.0018241473038991293\n",
      "Epoch 31::Minibatch 601::LR 0.032500000000000015 --> Loss 0.0005188759167989095\n",
      "Epoch 31::Minibatch 901::LR 0.032500000000000015 --> Loss 2.743710453311602e-05\n",
      "Epoch 32::Minibatch 1::LR 0.030250000000000013 --> Loss 0.0021828397115071616\n",
      "Epoch 32::Minibatch 301::LR 0.030250000000000013 --> Loss 0.0018230712413787843\n",
      "Epoch 32::Minibatch 601::LR 0.030250000000000013 --> Loss 0.0005194783707459768\n",
      "Epoch 32::Minibatch 901::LR 0.030250000000000013 --> Loss 2.7472112948695817e-05\n",
      "Epoch 33::Minibatch 1::LR 0.02800000000000001 --> Loss 0.002177106738090515\n",
      "Epoch 33::Minibatch 301::LR 0.02800000000000001 --> Loss 0.0018220450480779013\n",
      "Epoch 33::Minibatch 601::LR 0.02800000000000001 --> Loss 0.0005201426645119985\n",
      "Epoch 33::Minibatch 901::LR 0.02800000000000001 --> Loss 2.7504085252682368e-05\n",
      "Epoch 34::Minibatch 1::LR 0.02575000000000001 --> Loss 0.002171618342399597\n",
      "Epoch 34::Minibatch 301::LR 0.02575000000000001 --> Loss 0.0018210705121358235\n",
      "Epoch 34::Minibatch 601::LR 0.02575000000000001 --> Loss 0.0005208654701709748\n",
      "Epoch 34::Minibatch 901::LR 0.02575000000000001 --> Loss 2.753309595088164e-05\n",
      "Epoch 35::Minibatch 1::LR 0.023500000000000007 --> Loss 0.002166370948155721\n",
      "Epoch 35::Minibatch 301::LR 0.023500000000000007 --> Loss 0.001820161739985148\n",
      "Epoch 35::Minibatch 601::LR 0.023500000000000007 --> Loss 0.0005216437578201294\n",
      "Epoch 35::Minibatch 901::LR 0.023500000000000007 --> Loss 2.755919781823953e-05\n",
      "Epoch 36::Minibatch 1::LR 0.021250000000000005 --> Loss 0.0021613570054372154\n",
      "Epoch 36::Minibatch 301::LR 0.021250000000000005 --> Loss 0.0018193155527114868\n",
      "Epoch 36::Minibatch 601::LR 0.021250000000000005 --> Loss 0.0005224738021691641\n",
      "Epoch 36::Minibatch 901::LR 0.021250000000000005 --> Loss 2.7582477778196334e-05\n",
      "Epoch 37::Minibatch 1::LR 0.019000000000000017 --> Loss 0.0021565904219945273\n",
      "Epoch 37::Minibatch 301::LR 0.019000000000000017 --> Loss 0.001818548043568929\n",
      "Epoch 37::Minibatch 601::LR 0.019000000000000017 --> Loss 0.0005233478049437205\n",
      "Epoch 37::Minibatch 901::LR 0.019000000000000017 --> Loss 2.7602979292472202e-05\n",
      "Epoch 38::Minibatch 1::LR 0.016750000000000015 --> Loss 0.0021520666281382244\n",
      "Epoch 38::Minibatch 301::LR 0.016750000000000015 --> Loss 0.0018178657690684\n",
      "Epoch 38::Minibatch 601::LR 0.016750000000000015 --> Loss 0.0005242590606212616\n",
      "Epoch 38::Minibatch 901::LR 0.016750000000000015 --> Loss 2.7620764449238777e-05\n",
      "Epoch 39::Minibatch 1::LR 0.014500000000000013 --> Loss 0.0021478017171223957\n",
      "Epoch 39::Minibatch 301::LR 0.014500000000000013 --> Loss 0.0018172770738601684\n",
      "Epoch 39::Minibatch 601::LR 0.014500000000000013 --> Loss 0.0005251934130986531\n",
      "Epoch 39::Minibatch 901::LR 0.014500000000000013 --> Loss 2.7635879814624786e-05\n",
      "Epoch 40::Minibatch 1::LR 0.012250000000000011 --> Loss 0.0021438157558441163\n",
      "Epoch 40::Minibatch 301::LR 0.012250000000000011 --> Loss 0.0018168014287948608\n",
      "Epoch 40::Minibatch 601::LR 0.012250000000000011 --> Loss 0.0005261364082495371\n",
      "Epoch 40::Minibatch 901::LR 0.012250000000000011 --> Loss 2.7648362641533217e-05\n",
      "********************\n",
      "For fold 1\n",
      "\t Label LYING_DOWN:::-> Balanced Accuracy 0.8281245\n",
      "\t Label SITTING:::-> Balanced Accuracy 0.7131024\n",
      "\t Label FIX_walking:::-> Balanced Accuracy 0.573884\n",
      "\t Label FIX_running:::-> Balanced Accuracy 0.5055187\n",
      "\t Label BICYCLING:::-> Balanced Accuracy 0.652562\n",
      "\t Label SLEEPING:::-> Balanced Accuracy 0.8197388\n",
      "\t Label LAB_WORK:::-> Balanced Accuracy 0.645411\n",
      "\t Label IN_CLASS:::-> Balanced Accuracy 0.5261257\n",
      "\t Label IN_A_MEETING:::-> Balanced Accuracy 0.517288\n",
      "\t Label LOC_main_workplace:::-> Balanced Accuracy 0.7025352\n",
      "\t Label OR_indoors:::-> Balanced Accuracy 0.7357883\n",
      "\t Label OR_outside:::-> Balanced Accuracy 0.5163888\n",
      "\t Label IN_A_CAR:::-> Balanced Accuracy 0.5658766\n",
      "\t Label ON_A_BUS:::-> Balanced Accuracy 0.5009243\n",
      "\t Label DRIVE_-_I_M_THE_DRIVER:::-> Balanced Accuracy 0.5309062\n",
      "\t Label DRIVE_-_I_M_A_PASSENGER:::-> Balanced Accuracy 0.6033364\n",
      "\t Label LOC_home:::-> Balanced Accuracy 0.7745452\n",
      "\t Label FIX_restaurant:::-> Balanced Accuracy 0.49999\n",
      "\t Label PHONE_IN_POCKET:::-> Balanced Accuracy 0.5472466\n",
      "\t Label OR_exercise:::-> Balanced Accuracy 0.6061127\n",
      "\t Label COOKING:::-> Balanced Accuracy 0.4999933\n",
      "\t Label SHOPPING:::-> Balanced Accuracy 0.5009046\n",
      "\t Label STROLLING:::-> Balanced Accuracy 0.5\n",
      "\t Label DRINKING__ALCOHOL_:::-> Balanced Accuracy 0.4999983\n",
      "\t Label BATHING_-_SHOWER:::-> Balanced Accuracy 0.499855\n",
      "\t Label CLEANING:::-> Balanced Accuracy 0.5039803\n",
      "\t Label DOING_LAUNDRY:::-> Balanced Accuracy 0.5\n",
      "\t Label WASHING_DISHES:::-> Balanced Accuracy 0.5\n",
      "\t Label WATCHING_TV:::-> Balanced Accuracy 0.5178705\n",
      "\t Label SURFING_THE_INTERNET:::-> Balanced Accuracy 0.5711652\n",
      "\t Label AT_A_PARTY:::-> Balanced Accuracy 0.5036705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Label AT_A_BAR:::-> Balanced Accuracy 0.5\n",
      "\t Label LOC_beach:::-> Balanced Accuracy 0.5008616\n",
      "\t Label SINGING:::-> Balanced Accuracy 0.5007548\n",
      "\t Label TALKING:::-> Balanced Accuracy 0.5168685\n",
      "\t Label COMPUTER_WORK:::-> Balanced Accuracy 0.5739973\n",
      "\t Label EATING:::-> Balanced Accuracy 0.5000423\n",
      "\t Label TOILET:::-> Balanced Accuracy 0.5\n",
      "\t Label GROOMING:::-> Balanced Accuracy 0.4999916\n",
      "\t Label DRESSING:::-> Balanced Accuracy 0.499995\n",
      "\t Label AT_THE_GYM:::-> Balanced Accuracy 0.5386022\n",
      "\t Label STAIRS_-_GOING_UP:::-> Balanced Accuracy 0.500896\n",
      "\t Label STAIRS_-_GOING_DOWN:::-> Balanced Accuracy 0.4999983\n",
      "\t Label ELEVATOR:::-> Balanced Accuracy 0.5\n",
      "\t Label OR_standing:::-> Balanced Accuracy 0.5067315\n",
      "\t Label AT_SCHOOL:::-> Balanced Accuracy 0.5701112\n",
      "\t Label PHONE_IN_HAND:::-> Balanced Accuracy 0.5030241\n",
      "\t Label PHONE_IN_BAG:::-> Balanced Accuracy 0.536161\n",
      "\t Label PHONE_ON_TABLE:::-> Balanced Accuracy 0.656021\n",
      "\t Label WITH_CO-WORKERS:::-> Balanced Accuracy 0.4999629\n",
      "\t Label WITH_FRIENDS:::-> Balanced Accuracy 0.5557439\n",
      "********************\n",
      "For fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsrishankar/Desktop/Self_study/custom_dl_env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Label LYING_DOWN:::-> Balanced Accuracy 0.854786\n",
      "\t Label SITTING:::-> Balanced Accuracy 0.7017066\n",
      "\t Label FIX_walking:::-> Balanced Accuracy 0.5785246\n",
      "\t Label FIX_running:::-> Balanced Accuracy 0.4993631\n",
      "\t Label BICYCLING:::-> Balanced Accuracy 0.5083167\n",
      "\t Label SLEEPING:::-> Balanced Accuracy 0.8267246\n",
      "\t Label LAB_WORK:::-> Balanced Accuracy 0.4929907\n",
      "\t Label IN_CLASS:::-> Balanced Accuracy 0.4979063\n",
      "\t Label IN_A_MEETING:::-> Balanced Accuracy 0.4999067\n",
      "\t Label LOC_main_workplace:::-> Balanced Accuracy 0.5545783\n",
      "\t Label OR_indoors:::-> Balanced Accuracy 0.6000354\n",
      "\t Label OR_outside:::-> Balanced Accuracy 0.5030068\n",
      "\t Label IN_A_CAR:::-> Balanced Accuracy 0.5379483\n",
      "\t Label ON_A_BUS:::-> Balanced Accuracy 0.4999734\n",
      "\t Label DRIVE_-_I_M_THE_DRIVER:::-> Balanced Accuracy 0.5754064\n",
      "\t Label DRIVE_-_I_M_A_PASSENGER:::-> Balanced Accuracy 0.5174685\n",
      "\t Label LOC_home:::-> Balanced Accuracy 0.6063831\n",
      "\t Label FIX_restaurant:::-> Balanced Accuracy 0.4999933\n",
      "\t Label PHONE_IN_POCKET:::-> Balanced Accuracy 0.5381244\n",
      "\t Label OR_exercise:::-> Balanced Accuracy 0.5125112\n",
      "\t Label COOKING:::-> Balanced Accuracy 0.5\n",
      "\t Label SHOPPING:::-> Balanced Accuracy 0.4999801\n",
      "\t Label STROLLING:::-> Balanced Accuracy 0.5\n",
      "\t Label DRINKING__ALCOHOL_:::-> Balanced Accuracy 0.5\n",
      "\t Label BATHING_-_SHOWER:::-> Balanced Accuracy 0.4999934\n",
      "\t Label CLEANING:::-> Balanced Accuracy 0.4999933\n",
      "\t Label DOING_LAUNDRY:::-> Balanced Accuracy 0.5\n",
      "\t Label WASHING_DISHES:::-> Balanced Accuracy 0.5\n",
      "\t Label WATCHING_TV:::-> Balanced Accuracy 0.5673759\n",
      "\t Label SURFING_THE_INTERNET:::-> Balanced Accuracy 0.4778895\n",
      "\t Label AT_A_PARTY:::-> Balanced Accuracy 0.4999534\n",
      "\t Label AT_A_BAR:::-> Balanced Accuracy 0.5\n",
      "\t Label LOC_beach:::-> Balanced Accuracy 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsrishankar/Desktop/Self_study/custom_dl_env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Label SINGING:::-> Balanced Accuracy 0.9999867\n",
      "\t Label TALKING:::-> Balanced Accuracy 0.5012313\n",
      "\t Label COMPUTER_WORK:::-> Balanced Accuracy 0.5578525\n",
      "\t Label EATING:::-> Balanced Accuracy 0.5002159\n",
      "\t Label TOILET:::-> Balanced Accuracy 0.5\n",
      "\t Label GROOMING:::-> Balanced Accuracy 0.4999867\n",
      "\t Label DRESSING:::-> Balanced Accuracy 0.4999933\n",
      "\t Label AT_THE_GYM:::-> Balanced Accuracy 0.4994937\n",
      "\t Label STAIRS_-_GOING_UP:::-> Balanced Accuracy 0.4999668\n",
      "\t Label STAIRS_-_GOING_DOWN:::-> Balanced Accuracy 0.4999933\n",
      "\t Label ELEVATOR:::-> Balanced Accuracy 0.4999934\n",
      "\t Label OR_standing:::-> Balanced Accuracy 0.501574\n",
      "\t Label AT_SCHOOL:::-> Balanced Accuracy 0.5741963\n",
      "\t Label PHONE_IN_HAND:::-> Balanced Accuracy 0.5005701\n",
      "\t Label PHONE_IN_BAG:::-> Balanced Accuracy 0.5538305\n",
      "\t Label PHONE_ON_TABLE:::-> Balanced Accuracy 0.5859886\n",
      "\t Label WITH_CO-WORKERS:::-> Balanced Accuracy 0.5\n",
      "\t Label WITH_FRIENDS:::-> Balanced Accuracy 0.5035042\n",
      "Epoch 1::Minibatch 1::LR 0.1 --> Loss 0.0021401260296503703\n",
      "Epoch 1::Minibatch 301::LR 0.1 --> Loss 0.0004531766970952352\n",
      "Epoch 1::Minibatch 601::LR 0.1 --> Loss 0.0005021709203720093\n",
      "Epoch 1::Minibatch 901::LR 0.1 --> Loss 2.6748161762952804e-05\n",
      "Epoch 2::Minibatch 1::LR 0.09775 --> Loss 0.0023680126667022707\n",
      "Epoch 2::Minibatch 301::LR 0.09775 --> Loss 0.000439256876707077\n",
      "Epoch 2::Minibatch 601::LR 0.09775 --> Loss 0.0004969367881615957\n",
      "Epoch 2::Minibatch 901::LR 0.09775 --> Loss 2.6529645547270776e-05\n",
      "Epoch 3::Minibatch 1::LR 0.0955 --> Loss 0.002334174116452535\n",
      "Epoch 3::Minibatch 301::LR 0.0955 --> Loss 0.0004344816009203593\n",
      "Epoch 3::Minibatch 601::LR 0.0955 --> Loss 0.0004945534467697143\n",
      "Epoch 3::Minibatch 901::LR 0.0955 --> Loss 2.65020194152991e-05\n",
      "Epoch 4::Minibatch 1::LR 0.09325 --> Loss 0.0023019214471181235\n",
      "Epoch 4::Minibatch 301::LR 0.09325 --> Loss 0.00043249502778053284\n",
      "Epoch 4::Minibatch 601::LR 0.09325 --> Loss 0.0004932182530562083\n",
      "Epoch 4::Minibatch 901::LR 0.09325 --> Loss 2.65529565513134e-05\n",
      "Epoch 5::Minibatch 1::LR 0.09100000000000001 --> Loss 0.0022750006119410197\n",
      "Epoch 5::Minibatch 301::LR 0.09100000000000001 --> Loss 0.00043176258603731793\n",
      "Epoch 5::Minibatch 601::LR 0.09100000000000001 --> Loss 0.0004923000931739807\n",
      "Epoch 5::Minibatch 901::LR 0.09100000000000001 --> Loss 2.6640767852465312e-05\n",
      "Epoch 6::Minibatch 1::LR 0.08875000000000001 --> Loss 0.0022540350755055746\n",
      "Epoch 6::Minibatch 301::LR 0.08875000000000001 --> Loss 0.00043161317706108093\n",
      "Epoch 6::Minibatch 601::LR 0.08875000000000001 --> Loss 0.0004918075601259867\n",
      "Epoch 6::Minibatch 901::LR 0.08875000000000001 --> Loss 2.6747506732741993e-05\n",
      "Epoch 7::Minibatch 1::LR 0.08650000000000001 --> Loss 0.002236339648564657\n",
      "Epoch 7::Minibatch 301::LR 0.08650000000000001 --> Loss 0.0004316920538743337\n",
      "Epoch 7::Minibatch 601::LR 0.08650000000000001 --> Loss 0.0004915886123975118\n",
      "Epoch 7::Minibatch 901::LR 0.08650000000000001 --> Loss 2.686313974360625e-05\n",
      "Epoch 8::Minibatch 1::LR 0.08425 --> Loss 0.002220863898595174\n",
      "Epoch 8::Minibatch 301::LR 0.08425 --> Loss 0.0004318593939145406\n",
      "Epoch 8::Minibatch 601::LR 0.08425 --> Loss 0.000491542120774587\n",
      "Epoch 8::Minibatch 901::LR 0.08425 --> Loss 2.6982203125953673e-05\n",
      "Epoch 9::Minibatch 1::LR 0.082 --> Loss 0.002207012375195821\n",
      "Epoch 9::Minibatch 301::LR 0.082 --> Loss 0.0004320659736792246\n",
      "Epoch 9::Minibatch 601::LR 0.082 --> Loss 0.0004916149377822876\n",
      "Epoch 9::Minibatch 901::LR 0.082 --> Loss 2.7101399997870127e-05\n",
      "Epoch 10::Minibatch 1::LR 0.07975000000000002 --> Loss 0.00219448983669281\n",
      "Epoch 10::Minibatch 301::LR 0.07975000000000002 --> Loss 0.00043229868014653524\n",
      "Epoch 10::Minibatch 601::LR 0.07975000000000002 --> Loss 0.0004917664329210917\n",
      "Epoch 10::Minibatch 901::LR 0.07975000000000002 --> Loss 2.7218759059906004e-05\n",
      "Epoch 11::Minibatch 1::LR 0.07750000000000001 --> Loss 0.0021830672025680543\n",
      "Epoch 11::Minibatch 301::LR 0.07750000000000001 --> Loss 0.00043255517880121867\n",
      "Epoch 11::Minibatch 601::LR 0.07750000000000001 --> Loss 0.0004919824500878652\n",
      "Epoch 11::Minibatch 901::LR 0.07750000000000001 --> Loss 2.7333041653037072e-05\n",
      "Epoch 12::Minibatch 1::LR 0.07525000000000001 --> Loss 0.0021725902954737347\n",
      "Epoch 12::Minibatch 301::LR 0.07525000000000001 --> Loss 0.0004328339298566182\n",
      "Epoch 12::Minibatch 601::LR 0.07525000000000001 --> Loss 0.0004922417302926381\n",
      "Epoch 12::Minibatch 901::LR 0.07525000000000001 --> Loss 2.744349961479505e-05\n",
      "Epoch 13::Minibatch 1::LR 0.07300000000000001 --> Loss 0.0021629071235656737\n",
      "Epoch 13::Minibatch 301::LR 0.07300000000000001 --> Loss 0.0004331353306770325\n",
      "Epoch 13::Minibatch 601::LR 0.07300000000000001 --> Loss 0.0004925471047560374\n",
      "Epoch 13::Minibatch 901::LR 0.07300000000000001 --> Loss 2.754965176184972e-05\n",
      "Epoch 14::Minibatch 1::LR 0.07075000000000001 --> Loss 0.0021539254983266197\n",
      "Epoch 14::Minibatch 301::LR 0.07075000000000001 --> Loss 0.0004334576427936554\n",
      "Epoch 14::Minibatch 601::LR 0.07075000000000001 --> Loss 0.0004928807417551676\n",
      "Epoch 14::Minibatch 901::LR 0.07075000000000001 --> Loss 2.765128699441751e-05\n",
      "Epoch 15::Minibatch 1::LR 0.0685 --> Loss 0.002145540912946065\n",
      "Epoch 15::Minibatch 301::LR 0.0685 --> Loss 0.0004338038961092631\n",
      "Epoch 15::Minibatch 601::LR 0.0685 --> Loss 0.0004932499925295511\n",
      "Epoch 15::Minibatch 901::LR 0.0685 --> Loss 2.7748259405295055e-05\n",
      "Epoch 16::Minibatch 1::LR 0.06625 --> Loss 0.0021376854181289675\n",
      "Epoch 16::Minibatch 301::LR 0.06625 --> Loss 0.00043417289853096006\n",
      "Epoch 16::Minibatch 601::LR 0.06625 --> Loss 0.000493641843398412\n",
      "Epoch 16::Minibatch 901::LR 0.06625 --> Loss 2.7840559681256613e-05\n",
      "Epoch 17::Minibatch 1::LR 0.064 --> Loss 0.0021302910645802815\n",
      "Epoch 17::Minibatch 301::LR 0.064 --> Loss 0.000434566984574\n",
      "Epoch 17::Minibatch 601::LR 0.064 --> Loss 0.0004940639932950338\n",
      "Epoch 17::Minibatch 901::LR 0.064 --> Loss 2.7928178509076435e-05\n",
      "Epoch 18::Minibatch 1::LR 0.061750000000000006 --> Loss 0.0021233193079630533\n",
      "Epoch 18::Minibatch 301::LR 0.061750000000000006 --> Loss 0.00043498600522677105\n",
      "Epoch 18::Minibatch 601::LR 0.061750000000000006 --> Loss 0.0004945069551467896\n",
      "Epoch 18::Minibatch 901::LR 0.061750000000000006 --> Loss 2.8011233856280644e-05\n",
      "Epoch 19::Minibatch 1::LR 0.05950000000000001 --> Loss 0.0021167117357254028\n",
      "Epoch 19::Minibatch 301::LR 0.05950000000000001 --> Loss 0.00043543174862861636\n",
      "Epoch 19::Minibatch 601::LR 0.05950000000000001 --> Loss 0.0004949768384297689\n",
      "Epoch 19::Minibatch 901::LR 0.05950000000000001 --> Loss 2.808980333308379e-05\n",
      "Epoch 20::Minibatch 1::LR 0.05725000000000001 --> Loss 0.002110437750816345\n",
      "Epoch 20::Minibatch 301::LR 0.05725000000000001 --> Loss 0.00043590421477953593\n",
      "Epoch 20::Minibatch 601::LR 0.05725000000000001 --> Loss 0.0004954651991526286\n",
      "Epoch 20::Minibatch 901::LR 0.05725000000000001 --> Loss 2.8164029742280643e-05\n",
      "Epoch 21::Minibatch 1::LR 0.05500000000000001 --> Loss 0.0021044528484344483\n",
      "Epoch 21::Minibatch 301::LR 0.05500000000000001 --> Loss 0.0004364063342412313\n",
      "Epoch 21::Minibatch 601::LR 0.05500000000000001 --> Loss 0.0004959761599699656\n",
      "Epoch 21::Minibatch 901::LR 0.05500000000000001 --> Loss 2.8234000007311505e-05\n",
      "Epoch 22::Minibatch 1::LR 0.05275000000000001 --> Loss 0.0020987468957901\n",
      "Epoch 22::Minibatch 301::LR 0.05275000000000001 --> Loss 0.00043693860371907554\n",
      "Epoch 22::Minibatch 601::LR 0.05275000000000001 --> Loss 0.0004965062936147054\n",
      "Epoch 22::Minibatch 901::LR 0.05275000000000001 --> Loss 2.8299850722153982e-05\n",
      "Epoch 23::Minibatch 1::LR 0.05050000000000001 --> Loss 0.002093283732732137\n",
      "Epoch 23::Minibatch 301::LR 0.05050000000000001 --> Loss 0.00043750405311584475\n",
      "Epoch 23::Minibatch 601::LR 0.05050000000000001 --> Loss 0.0004970567921797434\n",
      "Epoch 23::Minibatch 901::LR 0.05050000000000001 --> Loss 2.8361715376377106e-05\n",
      "Epoch 24::Minibatch 1::LR 0.04825000000000001 --> Loss 0.0020880422989527385\n",
      "Epoch 24::Minibatch 301::LR 0.04825000000000001 --> Loss 0.0004381030797958374\n",
      "Epoch 24::Minibatch 601::LR 0.04825000000000001 --> Loss 0.0004976252714792887\n",
      "Epoch 24::Minibatch 901::LR 0.04825000000000001 --> Loss 2.8419711937506995e-05\n",
      "Epoch 25::Minibatch 1::LR 0.04600000000000001 --> Loss 0.0020830108722050987\n",
      "Epoch 25::Minibatch 301::LR 0.04600000000000001 --> Loss 0.00043873836596806844\n",
      "Epoch 25::Minibatch 601::LR 0.04600000000000001 --> Loss 0.0004982113341490428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25::Minibatch 901::LR 0.04600000000000001 --> Loss 2.8473958373069763e-05\n",
      "Epoch 26::Minibatch 1::LR 0.04375000000000001 --> Loss 0.002078169385592143\n",
      "Epoch 26::Minibatch 301::LR 0.04375000000000001 --> Loss 0.00043941214680671693\n",
      "Epoch 26::Minibatch 601::LR 0.04375000000000001 --> Loss 0.0004988148808479309\n",
      "Epoch 26::Minibatch 901::LR 0.04375000000000001 --> Loss 2.8524572650591534e-05\n",
      "Epoch 27::Minibatch 1::LR 0.04150000000000001 --> Loss 0.002073495586713155\n",
      "Epoch 27::Minibatch 301::LR 0.04150000000000001 --> Loss 0.0004401267071564992\n",
      "Epoch 27::Minibatch 601::LR 0.04150000000000001 --> Loss 0.000499436209599177\n",
      "Epoch 27::Minibatch 901::LR 0.04150000000000001 --> Loss 2.857169136404991e-05\n",
      "Epoch 28::Minibatch 1::LR 0.03925000000000001 --> Loss 0.002068993250528971\n",
      "Epoch 28::Minibatch 301::LR 0.03925000000000001 --> Loss 0.00044088467955589295\n",
      "Epoch 28::Minibatch 601::LR 0.03925000000000001 --> Loss 0.0005000745753447215\n",
      "Epoch 28::Minibatch 901::LR 0.03925000000000001 --> Loss 2.8615407645702363e-05\n",
      "Epoch 29::Minibatch 1::LR 0.037000000000000005 --> Loss 0.0020646472771962483\n",
      "Epoch 29::Minibatch 301::LR 0.037000000000000005 --> Loss 0.00044168760379155475\n",
      "Epoch 29::Minibatch 601::LR 0.037000000000000005 --> Loss 0.0005007287859916686\n",
      "Epoch 29::Minibatch 901::LR 0.037000000000000005 --> Loss 2.865583635866642e-05\n",
      "Epoch 30::Minibatch 1::LR 0.03475000000000002 --> Loss 0.0020604483286539715\n",
      "Epoch 30::Minibatch 301::LR 0.03475000000000002 --> Loss 0.00044253826141357424\n",
      "Epoch 30::Minibatch 601::LR 0.03475000000000002 --> Loss 0.000501399040222168\n",
      "Epoch 30::Minibatch 901::LR 0.03475000000000002 --> Loss 2.8693070635199547e-05\n",
      "Epoch 31::Minibatch 1::LR 0.032500000000000015 --> Loss 0.002056386868158976\n",
      "Epoch 31::Minibatch 301::LR 0.032500000000000015 --> Loss 0.00044343774517377216\n",
      "Epoch 31::Minibatch 601::LR 0.032500000000000015 --> Loss 0.0005020858347415925\n",
      "Epoch 31::Minibatch 901::LR 0.032500000000000015 --> Loss 2.8727203607559203e-05\n",
      "Epoch 32::Minibatch 1::LR 0.030250000000000013 --> Loss 0.0020524720350901288\n",
      "Epoch 32::Minibatch 301::LR 0.030250000000000013 --> Loss 0.0004443870981534322\n",
      "Epoch 32::Minibatch 601::LR 0.030250000000000013 --> Loss 0.000502788374821345\n",
      "Epoch 32::Minibatch 901::LR 0.030250000000000013 --> Loss 2.8758319094777106e-05\n",
      "Epoch 33::Minibatch 1::LR 0.02800000000000001 --> Loss 0.0020487002531687417\n",
      "Epoch 33::Minibatch 301::LR 0.02800000000000001 --> Loss 0.0004453858733177185\n",
      "Epoch 33::Minibatch 601::LR 0.02800000000000001 --> Loss 0.00050350621342659\n",
      "Epoch 33::Minibatch 901::LR 0.02800000000000001 --> Loss 2.8786516437927882e-05\n",
      "Epoch 34::Minibatch 1::LR 0.02575000000000001 --> Loss 0.002045068542162577\n",
      "Epoch 34::Minibatch 301::LR 0.02575000000000001 --> Loss 0.0004464331269264221\n",
      "Epoch 34::Minibatch 601::LR 0.02575000000000001 --> Loss 0.0005042384068171184\n",
      "Epoch 34::Minibatch 901::LR 0.02575000000000001 --> Loss 2.8811870142817496e-05\n",
      "Epoch 35::Minibatch 1::LR 0.023500000000000007 --> Loss 0.00204159418741862\n",
      "Epoch 35::Minibatch 301::LR 0.023500000000000007 --> Loss 0.00044752463698387144\n",
      "Epoch 35::Minibatch 601::LR 0.023500000000000007 --> Loss 0.0005049862464269003\n",
      "Epoch 35::Minibatch 901::LR 0.023500000000000007 --> Loss 2.8834451610843342e-05\n",
      "Epoch 36::Minibatch 1::LR 0.021250000000000005 --> Loss 0.002038270433743795\n",
      "Epoch 36::Minibatch 301::LR 0.021250000000000005 --> Loss 0.00044865712523460387\n",
      "Epoch 36::Minibatch 601::LR 0.021250000000000005 --> Loss 0.0005057470500469208\n",
      "Epoch 36::Minibatch 901::LR 0.021250000000000005 --> Loss 2.885437570512295e-05\n",
      "Epoch 37::Minibatch 1::LR 0.019000000000000017 --> Loss 0.002035109003384908\n",
      "Epoch 37::Minibatch 301::LR 0.019000000000000017 --> Loss 0.0004498217006524404\n",
      "Epoch 37::Minibatch 601::LR 0.019000000000000017 --> Loss 0.0005065161983172099\n",
      "Epoch 37::Minibatch 901::LR 0.019000000000000017 --> Loss 2.8871679678559303e-05\n",
      "Epoch 38::Minibatch 1::LR 0.016750000000000015 --> Loss 0.0020321422815322877\n",
      "Epoch 38::Minibatch 301::LR 0.016750000000000015 --> Loss 0.00045100907484690346\n",
      "Epoch 38::Minibatch 601::LR 0.016750000000000015 --> Loss 0.0005072909593582153\n",
      "Epoch 38::Minibatch 901::LR 0.016750000000000015 --> Loss 2.8886450454592705e-05\n",
      "Epoch 39::Minibatch 1::LR 0.014500000000000013 --> Loss 0.002029369274775187\n",
      "Epoch 39::Minibatch 301::LR 0.014500000000000013 --> Loss 0.0004522062838077545\n",
      "Epoch 39::Minibatch 601::LR 0.014500000000000013 --> Loss 0.0005080661674340566\n",
      "Epoch 39::Minibatch 901::LR 0.014500000000000013 --> Loss 2.8898762539029123e-05\n",
      "Epoch 40::Minibatch 1::LR 0.012250000000000011 --> Loss 0.0020268191893895466\n",
      "Epoch 40::Minibatch 301::LR 0.012250000000000011 --> Loss 0.0004533997674783071\n",
      "Epoch 40::Minibatch 601::LR 0.012250000000000011 --> Loss 0.0005088312427202861\n",
      "Epoch 40::Minibatch 901::LR 0.012250000000000011 --> Loss 2.8908693542083103e-05\n",
      "********************\n",
      "For fold 2\n",
      "\t Label LYING_DOWN:::-> Balanced Accuracy 0.8143667\n",
      "\t Label SITTING:::-> Balanced Accuracy 0.7063958\n",
      "\t Label FIX_walking:::-> Balanced Accuracy 0.5878954\n",
      "\t Label FIX_running:::-> Balanced Accuracy 0.5260896\n",
      "\t Label BICYCLING:::-> Balanced Accuracy 0.6622256\n",
      "\t Label SLEEPING:::-> Balanced Accuracy 0.7947601\n",
      "\t Label LAB_WORK:::-> Balanced Accuracy 0.6000196\n",
      "\t Label IN_CLASS:::-> Balanced Accuracy 0.5166112\n",
      "\t Label IN_A_MEETING:::-> Balanced Accuracy 0.5264562\n",
      "\t Label LOC_main_workplace:::-> Balanced Accuracy 0.6912908\n",
      "\t Label OR_indoors:::-> Balanced Accuracy 0.7180881\n",
      "\t Label OR_outside:::-> Balanced Accuracy 0.5110994\n",
      "\t Label IN_A_CAR:::-> Balanced Accuracy 0.5299134\n",
      "\t Label ON_A_BUS:::-> Balanced Accuracy 0.5018294\n",
      "\t Label DRIVE_-_I_M_THE_DRIVER:::-> Balanced Accuracy 0.6149214\n",
      "\t Label DRIVE_-_I_M_A_PASSENGER:::-> Balanced Accuracy 0.5811819\n",
      "\t Label LOC_home:::-> Balanced Accuracy 0.7644469\n",
      "\t Label FIX_restaurant:::-> Balanced Accuracy 0.4999933\n",
      "\t Label PHONE_IN_POCKET:::-> Balanced Accuracy 0.5549023\n",
      "\t Label OR_exercise:::-> Balanced Accuracy 0.6126631\n",
      "\t Label COOKING:::-> Balanced Accuracy 0.5001645\n",
      "\t Label SHOPPING:::-> Balanced Accuracy 0.4999983\n",
      "\t Label STROLLING:::-> Balanced Accuracy 0.5\n",
      "\t Label DRINKING__ALCOHOL_:::-> Balanced Accuracy 0.5\n",
      "\t Label BATHING_-_SHOWER:::-> Balanced Accuracy 0.49997\n",
      "\t Label CLEANING:::-> Balanced Accuracy 0.5059947\n",
      "\t Label DOING_LAUNDRY:::-> Balanced Accuracy 0.5\n",
      "\t Label WASHING_DISHES:::-> Balanced Accuracy 0.5\n",
      "\t Label WATCHING_TV:::-> Balanced Accuracy 0.5490517\n",
      "\t Label SURFING_THE_INTERNET:::-> Balanced Accuracy 0.5181857\n",
      "\t Label AT_A_PARTY:::-> Balanced Accuracy 0.5007669\n",
      "\t Label AT_A_BAR:::-> Balanced Accuracy 0.5\n",
      "\t Label LOC_beach:::-> Balanced Accuracy 0.5017094\n",
      "\t Label SINGING:::-> Balanced Accuracy 0.5007636\n",
      "\t Label TALKING:::-> Balanced Accuracy 0.525058\n",
      "\t Label COMPUTER_WORK:::-> Balanced Accuracy 0.5616036\n",
      "\t Label EATING:::-> Balanced Accuracy 0.4999827\n",
      "\t Label TOILET:::-> Balanced Accuracy 0.4999983\n",
      "\t Label GROOMING:::-> Balanced Accuracy 0.5002564\n",
      "\t Label DRESSING:::-> Balanced Accuracy 0.4999418\n",
      "\t Label AT_THE_GYM:::-> Balanced Accuracy 0.516637\n",
      "\t Label STAIRS_-_GOING_UP:::-> Balanced Accuracy 0.5005642\n",
      "\t Label STAIRS_-_GOING_DOWN:::-> Balanced Accuracy 0.5\n",
      "\t Label ELEVATOR:::-> Balanced Accuracy 0.5\n",
      "\t Label OR_standing:::-> Balanced Accuracy 0.5139294\n",
      "\t Label AT_SCHOOL:::-> Balanced Accuracy 0.5501121\n",
      "\t Label PHONE_IN_HAND:::-> Balanced Accuracy 0.5025859\n",
      "\t Label PHONE_IN_BAG:::-> Balanced Accuracy 0.5324996\n",
      "\t Label PHONE_ON_TABLE:::-> Balanced Accuracy 0.6354375\n",
      "\t Label WITH_CO-WORKERS:::-> Balanced Accuracy 0.4999747\n",
      "\t Label WITH_FRIENDS:::-> Balanced Accuracy 0.5757204\n",
      "********************\n",
      "For fold 2\n",
      "\t Label LYING_DOWN:::-> Balanced Accuracy 0.7952362\n",
      "\t Label SITTING:::-> Balanced Accuracy 0.6361085\n",
      "\t Label FIX_walking:::-> Balanced Accuracy 0.5567163\n",
      "\t Label FIX_running:::-> Balanced Accuracy 0.4999468\n",
      "\t Label BICYCLING:::-> Balanced Accuracy 0.5740496\n",
      "\t Label SLEEPING:::-> Balanced Accuracy 0.7279091\n",
      "\t Label LAB_WORK:::-> Balanced Accuracy 0.5239873\n",
      "\t Label IN_CLASS:::-> Balanced Accuracy 0.4999464\n",
      "\t Label IN_A_MEETING:::-> Balanced Accuracy 0.5012149\n",
      "\t Label LOC_main_workplace:::-> Balanced Accuracy 0.6414439\n",
      "\t Label OR_indoors:::-> Balanced Accuracy 0.6832748\n",
      "\t Label OR_outside:::-> Balanced Accuracy 0.5054143\n",
      "\t Label IN_A_CAR:::-> Balanced Accuracy 0.5180788\n",
      "\t Label ON_A_BUS:::-> Balanced Accuracy 0.4999533\n",
      "\t Label DRIVE_-_I_M_THE_DRIVER:::-> Balanced Accuracy 0.5450501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Label DRIVE_-_I_M_A_PASSENGER:::-> Balanced Accuracy 0.5131757\n",
      "\t Label LOC_home:::-> Balanced Accuracy 0.6851313\n",
      "\t Label FIX_restaurant:::-> Balanced Accuracy 0.4999801\n",
      "\t Label PHONE_IN_POCKET:::-> Balanced Accuracy 0.5159422\n",
      "\t Label OR_exercise:::-> Balanced Accuracy 0.519472\n",
      "\t Label COOKING:::-> Balanced Accuracy 0.4999798\n",
      "\t Label SHOPPING:::-> Balanced Accuracy 0.4999866\n",
      "\t Label STROLLING:::-> Balanced Accuracy 0.5\n",
      "\t Label DRINKING__ALCOHOL_:::-> Balanced Accuracy 0.5\n",
      "\t Label BATHING_-_SHOWER:::-> Balanced Accuracy 0.4999933\n",
      "\t Label CLEANING:::-> Balanced Accuracy 0.4999665\n",
      "\t Label DOING_LAUNDRY:::-> Balanced Accuracy 0.5\n",
      "\t Label WASHING_DISHES:::-> Balanced Accuracy 0.5\n",
      "\t Label WATCHING_TV:::-> Balanced Accuracy 0.5006489\n",
      "\t Label SURFING_THE_INTERNET:::-> Balanced Accuracy 0.5002286\n",
      "\t Label AT_A_PARTY:::-> Balanced Accuracy 0.5\n",
      "\t Label AT_A_BAR:::-> Balanced Accuracy 0.5\n",
      "\t Label LOC_beach:::-> Balanced Accuracy 1.0\n",
      "\t Label SINGING:::-> Balanced Accuracy 0.5\n",
      "\t Label TALKING:::-> Balanced Accuracy 0.4970117\n",
      "\t Label COMPUTER_WORK:::-> Balanced Accuracy 0.5146309\n",
      "\t Label EATING:::-> Balanced Accuracy 0.5000269\n",
      "\t Label TOILET:::-> Balanced Accuracy 0.4999933\n",
      "\t Label GROOMING:::-> Balanced Accuracy 0.4999394\n",
      "\t Label DRESSING:::-> Balanced Accuracy 0.4999531\n",
      "\t Label AT_THE_GYM:::-> Balanced Accuracy 0.4999867\n",
      "\t Label STAIRS_-_GOING_UP:::-> Balanced Accuracy 0.4999536\n",
      "\t Label STAIRS_-_GOING_DOWN:::-> Balanced Accuracy 0.5\n",
      "\t Label ELEVATOR:::-> Balanced Accuracy 0.5\n",
      "\t Label OR_standing:::-> Balanced Accuracy 0.5020879\n",
      "\t Label AT_SCHOOL:::-> Balanced Accuracy 0.5202326\n",
      "\t Label PHONE_IN_HAND:::-> Balanced Accuracy 0.5001785\n",
      "\t Label PHONE_IN_BAG:::-> Balanced Accuracy 0.5019863\n",
      "\t Label PHONE_ON_TABLE:::-> Balanced Accuracy 0.5790864\n",
      "\t Label WITH_CO-WORKERS:::-> Balanced Accuracy 0.4998923\n",
      "\t Label WITH_FRIENDS:::-> Balanced Accuracy 0.514257\n",
      "Epoch 1::Minibatch 1::LR 0.1 --> Loss 0.0020245095094045\n",
      "Epoch 1::Minibatch 301::LR 0.1 --> Loss 0.00042559449871381125\n",
      "Epoch 1::Minibatch 601::LR 0.1 --> Loss 0.0009978278477986654\n",
      "Epoch 1::Minibatch 901::LR 0.1 --> Loss 2.869605707625548e-05\n",
      "Epoch 2::Minibatch 1::LR 0.09775 --> Loss 0.0022665693362553915\n",
      "Epoch 2::Minibatch 301::LR 0.09775 --> Loss 0.0004283772905667623\n",
      "Epoch 2::Minibatch 601::LR 0.09775 --> Loss 0.0008646843830744425\n",
      "Epoch 2::Minibatch 901::LR 0.09775 --> Loss 2.8694110612074533e-05\n",
      "Epoch 3::Minibatch 1::LR 0.0955 --> Loss 0.0022566395998001097\n",
      "Epoch 3::Minibatch 301::LR 0.0955 --> Loss 0.0004296513895193736\n",
      "Epoch 3::Minibatch 601::LR 0.0955 --> Loss 0.0007961199680964152\n",
      "Epoch 3::Minibatch 901::LR 0.0955 --> Loss 2.87645123898983e-05\n",
      "Epoch 4::Minibatch 1::LR 0.09325 --> Loss 0.002254464824994405\n",
      "Epoch 4::Minibatch 301::LR 0.09325 --> Loss 0.00043097227811813356\n",
      "Epoch 4::Minibatch 601::LR 0.09325 --> Loss 0.0007558360199133555\n",
      "Epoch 4::Minibatch 901::LR 0.09325 --> Loss 2.8866349409023922e-05\n",
      "Epoch 5::Minibatch 1::LR 0.09100000000000001 --> Loss 0.002254169782002767\n",
      "Epoch 5::Minibatch 301::LR 0.09100000000000001 --> Loss 0.00043225576480229695\n",
      "Epoch 5::Minibatch 601::LR 0.09100000000000001 --> Loss 0.0007289021710554759\n",
      "Epoch 5::Minibatch 901::LR 0.09100000000000001 --> Loss 2.8981612995266915e-05\n",
      "Epoch 6::Minibatch 1::LR 0.08875000000000001 --> Loss 0.002253050406773885\n",
      "Epoch 6::Minibatch 301::LR 0.08875000000000001 --> Loss 0.00043346583843231203\n",
      "Epoch 6::Minibatch 601::LR 0.08875000000000001 --> Loss 0.0007094096144040425\n",
      "Epoch 6::Minibatch 901::LR 0.08875000000000001 --> Loss 2.910084401567777e-05\n",
      "Epoch 7::Minibatch 1::LR 0.08650000000000001 --> Loss 0.002251015305519104\n",
      "Epoch 7::Minibatch 301::LR 0.08650000000000001 --> Loss 0.0004345978796482086\n",
      "Epoch 7::Minibatch 601::LR 0.08650000000000001 --> Loss 0.000694573720296224\n",
      "Epoch 7::Minibatch 901::LR 0.08650000000000001 --> Loss 2.9219261681040127e-05\n",
      "Epoch 8::Minibatch 1::LR 0.08425 --> Loss 0.0022481777270634967\n",
      "Epoch 8::Minibatch 301::LR 0.08425 --> Loss 0.00043565819660822553\n",
      "Epoch 8::Minibatch 601::LR 0.08425 --> Loss 0.000682866374651591\n",
      "Epoch 8::Minibatch 901::LR 0.08425 --> Loss 2.9334472492337228e-05\n",
      "Epoch 9::Minibatch 1::LR 0.082 --> Loss 0.002244834899902344\n",
      "Epoch 9::Minibatch 301::LR 0.082 --> Loss 0.00043665389219919843\n",
      "Epoch 9::Minibatch 601::LR 0.082 --> Loss 0.0006733701626459757\n",
      "Epoch 9::Minibatch 901::LR 0.082 --> Loss 2.9445281252264977e-05\n",
      "Epoch 10::Minibatch 1::LR 0.07975000000000002 --> Loss 0.0022410831848780313\n",
      "Epoch 10::Minibatch 301::LR 0.07975000000000002 --> Loss 0.0004375883936882019\n",
      "Epoch 10::Minibatch 601::LR 0.07975000000000002 --> Loss 0.0006654796004295349\n",
      "Epoch 10::Minibatch 901::LR 0.07975000000000002 --> Loss 2.9551160211364427e-05\n",
      "Epoch 11::Minibatch 1::LR 0.07750000000000001 --> Loss 0.0022370825211207074\n",
      "Epoch 11::Minibatch 301::LR 0.07750000000000001 --> Loss 0.00043846964836120607\n",
      "Epoch 11::Minibatch 601::LR 0.07750000000000001 --> Loss 0.0006587842603524526\n",
      "Epoch 11::Minibatch 901::LR 0.07750000000000001 --> Loss 2.965191068748633e-05\n",
      "Epoch 12::Minibatch 1::LR 0.07525000000000001 --> Loss 0.0022328774134318034\n",
      "Epoch 12::Minibatch 301::LR 0.07525000000000001 --> Loss 0.0004393025239308675\n",
      "Epoch 12::Minibatch 601::LR 0.07525000000000001 --> Loss 0.0006529907882213593\n",
      "Epoch 12::Minibatch 901::LR 0.07525000000000001 --> Loss 2.9747523367404938e-05\n",
      "Epoch 13::Minibatch 1::LR 0.07300000000000001 --> Loss 0.002228545347849528\n",
      "Epoch 13::Minibatch 301::LR 0.07300000000000001 --> Loss 0.00044009223580360413\n",
      "Epoch 13::Minibatch 601::LR 0.07300000000000001 --> Loss 0.0006478845079739889\n",
      "Epoch 13::Minibatch 901::LR 0.07300000000000001 --> Loss 2.9838082070151966e-05\n",
      "Epoch 14::Minibatch 1::LR 0.07075000000000001 --> Loss 0.0022240994373957314\n",
      "Epoch 14::Minibatch 301::LR 0.07075000000000001 --> Loss 0.00044084290663401287\n",
      "Epoch 14::Minibatch 601::LR 0.07075000000000001 --> Loss 0.0006433115402857463\n",
      "Epoch 14::Minibatch 901::LR 0.07075000000000001 --> Loss 2.9923726494113604e-05\n",
      "Epoch 15::Minibatch 1::LR 0.0685 --> Loss 0.0022195788224538167\n",
      "Epoch 15::Minibatch 301::LR 0.0685 --> Loss 0.0004415576159954071\n",
      "Epoch 15::Minibatch 601::LR 0.0685 --> Loss 0.0006391509373982748\n",
      "Epoch 15::Minibatch 901::LR 0.0685 --> Loss 3.0004627381761867e-05\n",
      "Epoch 16::Minibatch 1::LR 0.06625 --> Loss 0.0022149868806203205\n",
      "Epoch 16::Minibatch 301::LR 0.06625 --> Loss 0.00044223934412002564\n",
      "Epoch 16::Minibatch 601::LR 0.06625 --> Loss 0.0006353093187014262\n",
      "Epoch 16::Minibatch 901::LR 0.06625 --> Loss 3.0080964788794516e-05\n",
      "Epoch 17::Minibatch 1::LR 0.064 --> Loss 0.00221034566561381\n",
      "Epoch 17::Minibatch 301::LR 0.064 --> Loss 0.00044288963079452514\n",
      "Epoch 17::Minibatch 601::LR 0.064 --> Loss 0.0006317164997259776\n",
      "Epoch 17::Minibatch 901::LR 0.064 --> Loss 3.015291877090931e-05\n",
      "Epoch 18::Minibatch 1::LR 0.061750000000000006 --> Loss 0.0022056527932484945\n",
      "Epoch 18::Minibatch 301::LR 0.061750000000000006 --> Loss 0.0004435105621814728\n",
      "Epoch 18::Minibatch 601::LR 0.061750000000000006 --> Loss 0.0006283148129781087\n",
      "Epoch 18::Minibatch 901::LR 0.061750000000000006 --> Loss 3.0220669383804003e-05\n",
      "Epoch 19::Minibatch 1::LR 0.05950000000000001 --> Loss 0.0022009164094924927\n",
      "Epoch 19::Minibatch 301::LR 0.05950000000000001 --> Loss 0.00044410288333892825\n",
      "Epoch 19::Minibatch 601::LR 0.05950000000000001 --> Loss 0.000625057170788447\n",
      "Epoch 19::Minibatch 901::LR 0.05950000000000001 --> Loss 3.0284381161133448e-05\n",
      "Epoch 20::Minibatch 1::LR 0.05725000000000001 --> Loss 0.0021961418787638347\n",
      "Epoch 20::Minibatch 301::LR 0.05725000000000001 --> Loss 0.00044466753800710045\n",
      "Epoch 20::Minibatch 601::LR 0.05725000000000001 --> Loss 0.0006219076613585155\n",
      "Epoch 20::Minibatch 901::LR 0.05725000000000001 --> Loss 3.0344221740961075e-05\n",
      "Epoch 21::Minibatch 1::LR 0.05500000000000001 --> Loss 0.002191323439280192\n",
      "Epoch 21::Minibatch 301::LR 0.05500000000000001 --> Loss 0.00044520482420921324\n",
      "Epoch 21::Minibatch 601::LR 0.05500000000000001 --> Loss 0.0006188330054283142\n",
      "Epoch 21::Minibatch 901::LR 0.05500000000000001 --> Loss 3.0400343239307403e-05\n",
      "Epoch 22::Minibatch 1::LR 0.05275000000000001 --> Loss 0.00218646506468455\n",
      "Epoch 22::Minibatch 301::LR 0.05275000000000001 --> Loss 0.00044571558634440105\n"
     ]
    }
   ],
   "source": [
    "class LinearMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearMLP,self).__init__()\n",
    "        self.fc1=nn.Linear(input_size,output_size)\n",
    "    def forward(self,x):\n",
    "        x=self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Train for Linear MLP\n",
    "model=LinearMLP()\n",
    "print(model)\n",
    "mlp0H_train_dict,mlp0H_valid_dict=kfold_validation(model,X=X_combined,Y=Y_combined,\n",
    "                 instance_weights=instance_weights,n_folds=n_fold,n_epochs=n_epoch,\n",
    "                 lr_init=lr_init,momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Accuracy outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing outputs\n",
    "mlp0H_train_summary=Customdictionary()\n",
    "mlp0H_valid_summary=Customdictionary()\n",
    "\n",
    "for k,v in mlp0H_train_dict.items():\n",
    "    for k2,v2 in v.items():\n",
    "        mlp0H_train_summary[k2]=v2\n",
    "for k,v in mlp0H_valid_dict.items():\n",
    "    for k2,v2 in v.items():\n",
    "        mlp0H_valid_summary[k2]=v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean dicts\n",
    "for k,v in mlp0H_valid_summary.items():\n",
    "    v_mean=np.mean(v)\n",
    "    print('{} --> Bal. accuracy {}'.format(k,np.round(v_mean,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "source": [
    "# Saving trained models\n",
    "root='saved_models/multilabel_classifier/'\n",
    "model_path=root+'mlp_0hidden'\n",
    "checkpoint_path=root+'mlp_0hidden_checkpoint'\n",
    "\n",
    "torch.save(model,model_path) # Saving the whole model\n",
    "\n",
    "# Saving checkpoint model\n",
    "torch.save({'epoch': n_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss':loss.item(),\n",
    "            'sumloss':sum_total/bs},checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (1 Hidden Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_1H(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_1H,self).__init__()\n",
    "        self.hidden0=nn.Sequential(\n",
    "            nn.Linear(input_size,hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "        self.out=nn.Sequential(\n",
    "            nn.Linear(hidden_size,output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.hidden0(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "# Train for MLP-1 Hidden\n",
    "model=MLP_1H()\n",
    "print(model)\n",
    "mlp1H_train_dict,mlp1H_valid_dict=kfold_validation(model,X=X_combined,Y=Y_combined,\n",
    "                 instance_weights=instance_weights,n_folds=n_fold,n_epochs=n_epoch,\n",
    "                 lr_init=lr_init,momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Accuracy outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing outputs\n",
    "mlp1H_train_summary=Customdictionary()\n",
    "mlp1H_valid_summary=Customdictionary()\n",
    "\n",
    "for k,v in mlp1H_train_dict.items():\n",
    "    for k2,v2 in v.items():\n",
    "        mlp01H_train_summary[k2]=v2\n",
    "for k,v in mlp1H_valid_dict.items():\n",
    "    for k2,v2 in v.items():\n",
    "        mlp1H_valid_summary[k2]=v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean dicts\n",
    "for k,v in mlp1H_valid_summary.items():\n",
    "    v_mean=np.mean(v)\n",
    "    print('{} --> Bal. accuracy {}'.format(k,np.round(v_mean,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# Saving trained models\n",
    "root='saved_models/multilabel_classifier/'\n",
    "model_path=root+'mlp_1hidden'\n",
    "checkpoint_path=root+'mlp_1hidden_checkpoint'\n",
    "\n",
    "torch.save(model,model_path) # Saving the whole model\n",
    "\n",
    "# Saving checkpoint model\n",
    "torch.save({'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss':loss.item(),\n",
    "            'sumloss':sum_total/bs},checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (2 Hidden Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_2H(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_2H,self).__init__()\n",
    "        self.hidden0=nn.Sequential(\n",
    "            nn.Linear(input_size,hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "        self.hidden1=nn.Sequential(\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "        self.out=nn.Sequential(\n",
    "            nn.Linear(hidden_size,output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "# Train for MLP-2 Hidden\n",
    "model=MLP_2H()\n",
    "print(model)\n",
    "mlp2H_train_dict,mlp2H_valid_dict=kfold_validation(model,X=X_combined,Y=Y_combined,\n",
    "                 instance_weights=instance_weights,n_folds=n_fold,n_epochs=n_epoch,\n",
    "                 lr_init=lr_init,momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Accuracy outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing outputs\n",
    "mlp2H_train_summary=Customdictionary()\n",
    "mlp2H_valid_summary=Customdictionary()\n",
    "\n",
    "for k,v in mlp2H_train_dict.items():\n",
    "    for k2,v2 in v.items():\n",
    "        mlp2H_train_summary[k2]=v2\n",
    "for k,v in mlp2H_valid_dict.items():\n",
    "    for k2,v2 in v.items():\n",
    "        mlp2H_valid_summary[k2]=v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean dicts\n",
    "for k,v in mlp2H_valid_summary.items():\n",
    "    v_mean=np.mean(v)\n",
    "    print('{} --> Bal. accuracy {}'.format(k,np.round(v_mean,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Accuracy outputs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# Saving trained models\n",
    "root='saved_models/multilabel_classifier/'\n",
    "model_path=root+'mlp_2hidden'\n",
    "checkpoint_path=root+'mlp_2hidden_checkpoint'\n",
    "\n",
    "torch.save(model,model_path) # Saving the whole model\n",
    "\n",
    "# Saving checkpoint model\n",
    "torch.save({'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss':loss.item(),\n",
    "            'sumloss':sum_total/bs},checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (2 Hidden Layers, with Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_2HDrop(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_2HDrop,self).__init__()\n",
    "        self.hidden0=nn.Sequential(\n",
    "            nn.Linear(input_size,hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Dropout(0.20)\n",
    "        )\n",
    "        self.hidden1=nn.Sequential(\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Dropout(0.20)\n",
    "        )\n",
    "        self.out=nn.Sequential(\n",
    "            nn.Linear(hidden_size,output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "model=MLP_2HDrop()\n",
    "C(model) # Train model with CUDA\n",
    "\n",
    "optimizer=optim.SGD(model.parameters(),lr=lr_init,momentum=momentum)\n",
    "#criterion=C(nn.BCEWithLogitsLoss()) # Or MultiLabelSoftMarginLoss (same thing in this case)\n",
    "#criterion=C(nn.BCELoss()) # BCEWithLogitsLoss adds a sigmoid layer to the BCELoss layer.\n",
    "# However, we want to binarize the outputs of the sigmoid first before getting the loss.\n",
    "# Though the BCELoss isn't very stable by itself.\n",
    "#criterion=C(nn.MultiLabelMarginLoss) # Needs the sigmoid output first\n",
    "criterion=C(nn.MultiLabelSoftMarginLoss())\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    sum_total=0.\n",
    "    done=1\n",
    "    for i,data in enumerate(train_loader,0):\n",
    "        inputs,labels=data\n",
    "        inputs=V(C(inputs)).float()\n",
    "        labels=V(C(labels),requires_grad=False).type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        optimizer.zero_grad() # Zero gradients\n",
    "        if done:\n",
    "            linear_lr_scheduler(optimizer,epoch) # Reduce LR once every epoch\n",
    "            done=0\n",
    "        \n",
    "        output=model(inputs) # Log probabilities\n",
    "        #sigmoid_output=torch.sigmoid(output) # Squash log probabilities to between 0 -1 (linear scale)\n",
    "        #sigmoid_output=(sigmoid_output>=0.50).type(torch.cuda.FloatTensor)# Binarize outputs using a threshold\n",
    "        #sigmoid_output=V(sigmoid_output,requires_grad=True)\n",
    "        \n",
    "        loss=criterion(output.type(torch.cuda.FloatTensor),labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        sum_total+=loss.item()\n",
    "        #print(\"Batch Loss: \",loss.item())\n",
    "        for param_group in optimizer.param_groups:\n",
    "            epoch_lr=param_group['lr']\n",
    "        if i%300==0: # Every minibatch\n",
    "            print(\"Epoch {}::Minibatch {}::LR {} --> Loss {}\".format(epoch+1,i+1,epoch_lr,sum_total/bs))\n",
    "            sum_total=0.\n",
    "    done=1\n",
    "print('\\n Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Saving trained models\n",
    "root='saved_models/multilabel_classifier/'\n",
    "model_path=root+'mlp_2hiddendrop'\n",
    "checkpoint_path=root+'mlp_2hiddendrop_checkpoint'\n",
    "\n",
    "torch.save(model,model_path) # Saving the whole model\n",
    "\n",
    "# Saving checkpoint model\n",
    "torch.save({'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss':loss.item(),\n",
    "            'sumloss':sum_total/bs},checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Test dataset model performance\n",
    "\n",
    "concat_predictions=torch.zeros(0,output_size)\n",
    "concat_truelabels=torch.zeros(0,output_size)\n",
    "\n",
    "for data in test_loader:\n",
    "    inputs,labels=data\n",
    "    inputs=V(C(inputs)).float()\n",
    "    labels=V(C(labels)).type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    outputs=model.forward(inputs).cpu() # Perform test time on CPU instead of GPU\n",
    "    \n",
    "    # Concat test set into one tensor\n",
    "    concat_predictions=torch.cat((concat_predictions,outputs),0)\n",
    "    concat_truelabels=torch.cat((concat_truelabels,labels.cpu()),0)\n",
    "\n",
    "concat_predictions=torch.sigmoid(concat_predictions) # Squash log probabilities to between 0 -1 (linear scale)\n",
    "concat_predictions=concat_predictions>=0.50 # Binarize outputs using a threshold\n",
    "\n",
    "# Convert tensor to numpy float array\n",
    "concat_predictions=concat_predictions.numpy().astype(np.float)\n",
    "concat_truelabels=concat_truelabels.numpy().astype(np.float)\n",
    "\n",
    "# Precision, Recall, F-1, support\n",
    "mlp_0H_clfreport=classification_report(y_true=concat_truelabels,y_pred=concat_predictions,\n",
    "                                       target_names=labelname_user,output_dict=True)\n",
    "print('Test Set')\n",
    "for i in range(output_size):\n",
    "    true_perlabel=concat_truelabels[:,i]\n",
    "    prediction_perlabel=concat_predictions[:,i]\n",
    "    bal_acc=balanced_accuracy_score(y_true=true_perlabel,y_pred=prediction_perlabel)\n",
    "    \n",
    "    print('Label {} :::-> Balanced Accuracy {}'.format(labelname_user[i],round(bal_acc,5)))\n",
    "    \n",
    "for key,value in enumerate(mlp_0H_clfreport.items()):\n",
    "    print(key,\"\\n\")\n",
    "    print(\"\\t\",value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Train dataset model performance\n",
    "\n",
    "concat_predictions=torch.zeros(0,output_size)\n",
    "concat_truelabels=torch.zeros(0,output_size)\n",
    "\n",
    "for data in train_loader:\n",
    "    inputs,labels=data\n",
    "    inputs=V(C(inputs)).float()\n",
    "    labels=V(C(labels)).type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    outputs=model.forward(inputs).cpu() # Perform test time on CPU instead of GPU\n",
    "    \n",
    "    # Concat test set into one tensor\n",
    "    concat_predictions=torch.cat((concat_predictions,outputs),0)\n",
    "    concat_truelabels=torch.cat((concat_truelabels,labels.cpu()),0)\n",
    "\n",
    "concat_predictions=torch.sigmoid(concat_predictions) # Squash log probabilities to between 0 -1 (linear scale)\n",
    "concat_predictions=concat_predictions>=0.50 # Binarize outputs using a threshold\n",
    "\n",
    "# Convert tensor to numpy float array\n",
    "concat_predictions=concat_predictions.numpy().astype(np.float)\n",
    "concat_truelabels=concat_truelabels.numpy().astype(np.float)\n",
    "\n",
    "mlp_0H_clfreport_train=classification_report(y_true=concat_truelabels,y_pred=concat_predictions,\n",
    "                                       target_names=labelname_user,output_dict=True)\n",
    "print('Train Set')\n",
    "for i in range(output_size):\n",
    "    true_perlabel=concat_truelabels[:,i]\n",
    "    prediction_perlabel=concat_predictions[:,i]\n",
    "    bal_acc=balanced_accuracy_score(y_true=true_perlabel,y_pred=prediction_perlabel)\n",
    "    \n",
    "    print('Label {} :::-> Balanced Accuracy {}'.format(labelname_user[i],round(bal_acc,5)))\n",
    "    \n",
    "for key,value in enumerate(mlp_0H_clfreport_train.items()):\n",
    "    print(key,\"\\n\")\n",
    "    print(\"\\t\",value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12.2333px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "353.983px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "930.85px",
    "left": "2.28333px",
    "right": "1417.63px",
    "top": "578px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
