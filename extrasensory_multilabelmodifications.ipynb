{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Dataset-parsers-and-cleaning-functions\" data-toc-modified-id=\"Dataset-parsers-and-cleaning-functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset parsers and cleaning functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-data\" data-toc-modified-id=\"Test-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Test data</a></span></li></ul></li><li><span><a href=\"#Training-Functions\" data-toc-modified-id=\"Training-Functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training Functions</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Test-data-on-particular-sensors\" data-toc-modified-id=\"Test-data-on-particular-sensors-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Test data on particular sensors</a></span></li></ul></li><li><span><a href=\"#Creating-a-new-data-structure-for-all-valid-data-and-pickling-it\" data-toc-modified-id=\"Creating-a-new-data-structure-for-all-valid-data-and-pickling-it-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Creating a new data structure for all valid data and pickling it</a></span></li><li><span><a href=\"#Creating-and-pickling-instance-weight-matrix\" data-toc-modified-id=\"Creating-and-pickling-instance-weight-matrix-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Creating and pickling instance weight matrix</a></span></li><li><span><a href=\"#Miscellaneous-train/test-functions\" data-toc-modified-id=\"Miscellaneous-train/test-functions-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Miscellaneous train/test functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cuda-enable\" data-toc-modified-id=\"Cuda-enable-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Cuda-enable</a></span></li><li><span><a href=\"#Tackling-missing-labels-using-a-mask\" data-toc-modified-id=\"Tackling-missing-labels-using-a-mask-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Tackling missing labels using a mask</a></span></li><li><span><a href=\"#Linear-Learning-Rate-scheduler\" data-toc-modified-id=\"Linear-Learning-Rate-scheduler-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Linear Learning-Rate scheduler</a></span></li><li><span><a href=\"#Euclidean-Norm-for-weight-matrices\" data-toc-modified-id=\"Euclidean-Norm-for-weight-matrices-3.3.4\"><span class=\"toc-item-num\">3.3.4&nbsp;&nbsp;</span>Euclidean Norm for weight matrices</a></span></li><li><span><a href=\"#Accuracy-(Precision,-Recall,-F1,-Support,-Balanced-Accuracy)-metrics\" data-toc-modified-id=\"Accuracy-(Precision,-Recall,-F1,-Support,-Balanced-Accuracy)-metrics-3.3.5\"><span class=\"toc-item-num\">3.3.5&nbsp;&nbsp;</span>Accuracy (Precision, Recall, F1, Support, Balanced Accuracy) metrics</a></span></li><li><span><a href=\"#K-Fold-cross-validation-Train-Function\" data-toc-modified-id=\"K-Fold-cross-validation-Train-Function-3.3.6\"><span class=\"toc-item-num\">3.3.6&nbsp;&nbsp;</span>K-Fold cross validation Train Function</a></span></li><li><span><a href=\"#Custom-K-Fold-cross-validation-Train-Function-w/-Sensor-Dropout\" data-toc-modified-id=\"Custom-K-Fold-cross-validation-Train-Function-w/-Sensor-Dropout-3.3.7\"><span class=\"toc-item-num\">3.3.7&nbsp;&nbsp;</span>Custom K-Fold cross validation Train Function w/ Sensor Dropout</a></span></li><li><span><a href=\"#Unclassified-column-if-no-labels-are-marked\" data-toc-modified-id=\"Unclassified-column-if-no-labels-are-marked-3.3.8\"><span class=\"toc-item-num\">3.3.8&nbsp;&nbsp;</span>Unclassified column if no labels are marked</a></span></li></ul></li></ul></li><li><span><a href=\"#Multi-Class-Classifier:-Learning-and-Evaluation\" data-toc-modified-id=\"Multi-Class-Classifier:-Learning-and-Evaluation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Multi Class Classifier: Learning and Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Global-Hyperparameter-variables\" data-toc-modified-id=\"Global-Hyperparameter-variables-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Global Hyperparameter variables</a></span></li><li><span><a href=\"#Multi-Layer-Perceptron-(0-Hidden-Layers)\" data-toc-modified-id=\"Multi-Layer-Perceptron-(0-Hidden-Layers)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Multi-Layer Perceptron (0 Hidden Layers)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Balanced-Accuracy-outputs\" data-toc-modified-id=\"Balanced-Accuracy-outputs-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Balanced Accuracy outputs</a></span></li><li><span><a href=\"#Saving-data\" data-toc-modified-id=\"Saving-data-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Saving data</a></span></li></ul></li><li><span><a href=\"#Multi-Layer-Perceptron-(1-Hidden-Layer)\" data-toc-modified-id=\"Multi-Layer-Perceptron-(1-Hidden-Layer)-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Multi-Layer Perceptron (1 Hidden Layer)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Balanced-Accuracy-outputs\" data-toc-modified-id=\"Balanced-Accuracy-outputs-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Balanced Accuracy outputs</a></span></li><li><span><a href=\"#Saving-data\" data-toc-modified-id=\"Saving-data-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Saving data</a></span></li></ul></li><li><span><a href=\"#Multi-Layer-Perceptron-(2-Hidden-Layers)\" data-toc-modified-id=\"Multi-Layer-Perceptron-(2-Hidden-Layers)-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Multi-Layer Perceptron (2 Hidden Layers)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Balanced-Accuracy-outputs\" data-toc-modified-id=\"Balanced-Accuracy-outputs-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Balanced Accuracy outputs</a></span></li><li><span><a href=\"#Saving-data\" data-toc-modified-id=\"Saving-data-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Saving data</a></span></li></ul></li><li><span><a href=\"#Multi-Layer-Perceptron-(2-Hidden-Layers,-with-Dropout)\" data-toc-modified-id=\"Multi-Layer-Perceptron-(2-Hidden-Layers,-with-Dropout)-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Multi-Layer Perceptron (2 Hidden Layers, with Dropout)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Balanced-Accuracy-outputs\" data-toc-modified-id=\"Balanced-Accuracy-outputs-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>Balanced Accuracy outputs</a></span></li><li><span><a href=\"#Saving-data\" data-toc-modified-id=\"Saving-data-4.5.2\"><span class=\"toc-item-num\">4.5.2&nbsp;&nbsp;</span>Saving data</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import glob\n",
    "import pickle\n",
    "import copy\n",
    "import math\n",
    "from io import StringIO\n",
    "import importlib.machinery\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,balanced_accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support,classification_report\n",
    "#from sklearn.metrics import multilabel_confusion_matrix # Only available in dev .21\n",
    "\n",
    "# Need Pytorch for multilabel classifications\n",
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "#import skorch [Scikit-learn wrapper around Pytorch so allowing for K-fold cross-validation]\n",
    "random_state=10\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Data location and sample user\n",
    "prefix='dataset/Extrasensory_uuid_fl_uTAR/'\n",
    "cross_validation_user_loc='dataset/cv_5_folds/'\n",
    "user_sample='3600D531-0C55-44A7-AE95-A7A38519464E.features_labels'\n",
    "done=1 # Pickled files are created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset parsers and cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Dataset parsers for header/ body for CSVs\n",
    "def parse_header_of_csv(csv_str):\n",
    "    # Isolate the headline columns:\n",
    "    headline = csv_str[:csv_str.index('\\n')];\n",
    "    columns = headline.split(',');\n",
    "\n",
    "    # The first column should be timestamp:\n",
    "    assert columns[0] == 'timestamp';\n",
    "    # The last column should be label_source:\n",
    "    assert columns[-1] == 'label_source';\n",
    "    \n",
    "    # Search for the column of the first label:\n",
    "    for (ci,col) in enumerate(columns):\n",
    "        if col.startswith('label:'):\n",
    "            first_label_ind = ci;\n",
    "            break;\n",
    "        pass;\n",
    "\n",
    "    # Feature columns come after timestamp and before the labels:\n",
    "    feature_names = columns[1:first_label_ind];\n",
    "    # Then come the labels, till the one-before-last column:\n",
    "    label_names = columns[first_label_ind:-1];\n",
    "    for (li,label) in enumerate(label_names):\n",
    "        # In the CSV the label names appear with prefix 'label:', but we don't need it after reading the data:\n",
    "        assert label.startswith('label:');\n",
    "        label_names[li] = label.replace('label:','');\n",
    "        pass;\n",
    "    \n",
    "    return (feature_names,label_names);\n",
    "\n",
    "def parse_body_of_csv(csv_str,n_features):\n",
    "    # Read the entire CSV body into a single numeric matrix:\n",
    "    full_table = np.loadtxt(StringIO(csv_str),delimiter=',',skiprows=1);\n",
    "    \n",
    "    # Timestamp is the primary key for the records (examples):\n",
    "    timestamps = full_table[:,0].astype(int);\n",
    "    \n",
    "    # Read the sensor features:\n",
    "    X = full_table[:,1:(n_features+1)];\n",
    "    \n",
    "    # Read the binary label values, and the 'missing label' indicators:\n",
    "    trinary_labels_mat = full_table[:,(n_features+1):-1]; # This should have values of either 0., 1. or NaN\n",
    "    M = np.isnan(trinary_labels_mat); # M is the missing label matrix\n",
    "    \n",
    "    #print(\"M matrix shape:\",M.shape)\n",
    "    #print(\"Matrix: \",np.argwhere(M))\n",
    "    \n",
    "    Y = np.where(M,0,trinary_labels_mat) > 0.; # Y is the label matrix\n",
    "    \n",
    "    return (X,Y,M,timestamps);\n",
    "\n",
    "def read_user_data(directory):\n",
    "    print('Reading {}'.format(directory.split(\"/\")[-1]))\n",
    "\n",
    "    # Read the entire csv file of the user:\n",
    "    with gzip.open(directory,'rb') as fid:\n",
    "        csv_str = fid.read();\n",
    "        csv_str = csv_str.decode(\"utf-8\")\n",
    "        pass;\n",
    "\n",
    "    (feature_names,label_names) = parse_header_of_csv(csv_str);\n",
    "    n_features = len(feature_names);\n",
    "    (X,Y,M,timestamps) = parse_body_of_csv(csv_str,n_features);\n",
    "\n",
    "    return (X,Y,M,timestamps,feature_names,label_names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Clean labels\n",
    "def clean_labels(input_label):\n",
    "    if label.endswith('_'):\n",
    "        label=label[:-1]+')'\n",
    "    label=label.replace('__',' (').replace('_',' ')\n",
    "    label=label[0]+label[1:].lower()\n",
    "    label=label.replace('i m','I\\'m')\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Get a summary of the sensor feature\n",
    "'''\n",
    "# Summarize features as we are only using phone_acc,phone_gyro,phone_mag,phone_loc,phone_audio,\n",
    "# phone_app,phone_battery,phone_use,phone_callstat,phone_wifi,phone_lf,phone_time\n",
    "# We are ignoring the use of the smartwatch features. There are definitely features that will be used\n",
    "# much more (e.g. than the phone_callstat) but we'll leave that up to the ML algorithm.\n",
    "'''\n",
    "def summarize_features(feature_list):\n",
    "    summary_feature_list=np.empty_like(feature_list)\n",
    "    for (ind,feature) in enumerate(feature_list):\n",
    "        if feature.startswith('raw_acc'):\n",
    "            summary_feature_list[ind]='phone_acc' \n",
    "        if feature.startswith('proc_gyro'):\n",
    "            summary_feature_list[ind]='phone_gyro'\n",
    "        if feature.startswith('raw_magnet'):\n",
    "            summary_feature_list[ind]='phone_mag'\n",
    "        if feature.startswith('watch_acc'):\n",
    "            summary_feature_list[ind]='watch_acc'\n",
    "        if feature.startswith('watch_heading'):\n",
    "            summary_feature_list[ind]='watch_dir'\n",
    "        if feature.startswith('location'):\n",
    "            summary_feature_list[ind]='phone_loc'\n",
    "        if feature.startswith('audio'):\n",
    "            summary_feature_list[ind]='phone_audio'\n",
    "        if feature.startswith('discrete:app_state'):\n",
    "            summary_feature_list[ind]='phone_app'\n",
    "        if feature.startswith('discrete:battery'):\n",
    "            summary_feature_list[ind]='phone_battery'\n",
    "        if feature.startswith('discrete:on'):\n",
    "            summary_feature_list[ind]='phone_use'\n",
    "        if feature.startswith('discrete:ringer'):\n",
    "            summary_feature_list[ind]='phone_callstat'\n",
    "        if feature.startswith('discrete:wifi'):\n",
    "            summary_feature_list[ind]='phone_wifi'\n",
    "        if feature.startswith('lf'):\n",
    "            summary_feature_list[ind]='phone_lf'\n",
    "        if feature.startswith('discrete:time'):\n",
    "            summary_feature_list[ind]='phone_time'\n",
    "\n",
    "    return summary_feature_list\n",
    "\n",
    "\n",
    "# Get a summary of the sensor feature along with the original label that was used\n",
    "def summarize_features_worig(feature_list):\n",
    "    summary_feature_list=np.empty((len(feature_list),2),dtype=object)\n",
    "    \n",
    "    for (ind,feature) in enumerate(feature_list):\n",
    "        if feature.startswith('raw_acc'):\n",
    "            summary_feature_list[ind,0]='phone_acc'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('proc_gyro'):\n",
    "            summary_feature_list[ind,0]='phone_gyro'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('raw_magnet'):\n",
    "            summary_feature_list[ind,0]='phone_mag'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('watch_acc'):\n",
    "            summary_feature_list[ind,0]='watch_acc'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('watch_heading'):\n",
    "            summary_feature_list[ind,0]='watch_dir'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('location'):\n",
    "            summary_feature_list[ind,0]='phone_loc'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('audio'):\n",
    "            summary_feature_list[ind,0]='phone_audio'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:app_state'):\n",
    "            summary_feature_list[ind,0]='phone_app'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:battery'):\n",
    "            summary_feature_list[ind,0]='phone_battery'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:on'):\n",
    "            summary_feature_list[ind,0]='phone_use'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:ringer'):\n",
    "            summary_feature_list[ind,0]='phone_callstat'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:wifi'):\n",
    "            summary_feature_list[ind,0]='phone_wifi'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('lf'):\n",
    "            summary_feature_list[ind,0]='phone_lf'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "            \n",
    "        if feature.startswith('discrete:time'):\n",
    "            summary_feature_list[ind,0]='phone_time'\n",
    "            summary_feature_list[ind,1]=feature\n",
    "\n",
    "    return summary_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Custom dictionary class with help for duplicate keys\n",
    "class Customdictionary(dict):\n",
    "    def __setitem__(self,key,value):\n",
    "        try:\n",
    "            self[key]\n",
    "        except KeyError:\n",
    "            super(Customdictionary,self).__setitem__(key,[])\n",
    "        self[key].append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 3600D531-0C55-44A7-AE95-A7A38519464E.features_labels.csv.gz\n",
      "Data shape input for user (Len minutes/num examples, num sensors):  (5203, 225)\n",
      "Label shape for user (Len minutes, num labels):  (5203, 51) \n",
      "\n",
      "Sensor feature names:\n",
      "\n",
      "Activities and counts:\n",
      "[('LOC_home', 3040), ('OR_indoors', 2487), ('PHONE_ON_TABLE', 2179), ('SITTING', 1916), ('WITH_FRIENDS', 1730), ('LYING_DOWN', 1336), ('SLEEPING', 1021), ('WATCHING_TV', 912), ('EATING', 762), ('PHONE_IN_POCKET', 706), ('TALKING', 638), ('DRIVE_-_I_M_A_PASSENGER', 409), ('OR_standing', 384), ('IN_A_CAR', 342), ('OR_exercise', 162), ('AT_THE_GYM', 162), ('SINGING', 136), ('FIX_walking', 132), ('OR_outside', 127), ('SHOPPING', 111), ('AT_SCHOOL', 105), ('BATHING_-_SHOWER', 85), ('DRESSING', 67), ('DRINKING__ALCOHOL_', 66), ('PHONE_IN_HAND', 64), ('FIX_restaurant', 59), ('IN_CLASS', 54), ('PHONE_IN_BAG', 33), ('IN_A_MEETING', 27), ('TOILET', 12), ('COOKING', 5), ('ELEVATOR', 1), ('FIX_running', 0), ('BICYCLING', 0), ('LAB_WORK', 0), ('LOC_main_workplace', 0), ('ON_A_BUS', 0), ('DRIVE_-_I_M_THE_DRIVER', 0), ('STROLLING', 0), ('CLEANING', 0), ('DOING_LAUNDRY', 0), ('WASHING_DISHES', 0), ('SURFING_THE_INTERNET', 0), ('AT_A_PARTY', 0), ('AT_A_BAR', 0), ('LOC_beach', 0), ('COMPUTER_WORK', 0), ('GROOMING', 0), ('STAIRS_-_GOING_UP', 0), ('STAIRS_-_GOING_DOWN', 0), ('WITH_CO-WORKERS', 0)]\n"
     ]
    }
   ],
   "source": [
    "# Reading sample data\n",
    "sample_loc='{}/{}.csv.gz'.format(prefix,user_sample)\n",
    "x_user,y_user,missedlabel_user,tstamp_user,featurename_user,labelname_user=read_user_data(sample_loc)\n",
    "\n",
    "# Dataset summaries for this user\n",
    "print('Data shape input for user (Len minutes/num examples, num sensors): ',x_user.shape) # Timestep examples, number of sensors\n",
    "print('Label shape for user (Len minutes, num labels): ',y_user.shape,'\\n') # Timestep examples, labels\n",
    "\n",
    "countlabels_user=np.sum(y_user,axis=0) # Column summary\n",
    "labelname_countlabel_user=zip(labelname_user,countlabels_user) # Zip together names, counts\n",
    "labelname_countlabel_user=sorted(labelname_countlabel_user,key=lambda row:row[-1],reverse=True)\n",
    "\n",
    "print('Sensor feature names:\\n')\n",
    "feature_names=summarize_features(featurename_user)\n",
    "    \n",
    "# for i,sensor_feature in enumerate(featurename_user):\n",
    "#     print('{} :: {} ::--> {}\\n'.format(i,feature_names[i],sensor_feature))\n",
    "\n",
    "print('Activities and counts:')\n",
    "print(labelname_countlabel_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "    ISSUE: There are some labels (e.g. Phone location:bag etc.) that some users have not filled out for any timestep and shows up as np.nan. The label sum above was a check to see if the same label wasn't filled out for other users (hence would have a count of zero) and would let the label being completely removed. The lowest count was (Elevator:200) which doesn't help.\n",
    "    I cannot do blindly remove rows because a particular label wasn't filled out for any timestep for a user. For single label case, this is fine...but for a multi-label case, this will mean that other valid labels are ignored. The only option that I have so far is to naively convert all nans in the labels to zeros. This could mean a loss of accuracy (the user might have been doing the task in the label but have omitted annotating it, and so we are incorrectly training a feature vector....but there is no choice so far.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Choosing sensor labels\n",
    "'''\n",
    "Summary sensor choices are: phone_acc,phone_gyro,phone_mag,watch_acc,watch_dir,phone_loc,phone_audio,\n",
    "phone_app,phone_battery,phone_use,phone_callstat,phone_wifi,phone_lf,phone_time\n",
    "In this project, we aren't using watch_acc,watch_dir (no smartwatch)\n",
    "'''\n",
    "\n",
    "def choose_sensors(X_train,used_sensors,summarized_feature_names):\n",
    "    used_sensor_feature_names=np.zeros(len(summarized_feature_names),dtype=bool)\n",
    "    # Creates a zero boolean vector of all possible feature names\n",
    "    for s in used_sensors:\n",
    "        used_sensor_feature_names=np.logical_or(used_sensor_feature_names,(s==summarized_feature_names))\n",
    "    X_train=X_train[:,used_sensor_feature_names]\n",
    "    return X_train\n",
    "\n",
    "def choose_sensors_dropout(X_train,used_sensors,summarized_feature_names):\n",
    "    used_sensor_feature_names=np.zeros(len(summarized_feature_names),dtype=bool)\n",
    "    data_length=len(X_train)\n",
    "    \n",
    "    # Creates a zero boolean vector of all possible feature names\n",
    "    for s in used_sensors:\n",
    "        used_sensor_feature_names=np.logical_or(used_sensor_feature_names,(s==summarized_feature_names))\n",
    "    mask=np.tile(used_sensor_feature_names,(data_length,1))\n",
    "    \n",
    "    X_train=np.multiply(X_train,mask) # Element-wise matrix multiply\n",
    "    return X_train\n",
    "\n",
    "def choose_sensors_longnames(X_train,used_sensors,long_featurenames):\n",
    "    \n",
    "    used_sensor_feature_names=np.zeros(len(long_featurenames),dtype=bool)\n",
    "    used_feature_actualnames=np.zeros(len(long_featurenames),dtype=bool)\n",
    "    # Creates a zero boolean vector of all possible feature names\n",
    "    summary_features=long_featurenames[:,0]\n",
    "    all_complete_features=long_featurenames[:,-1]\n",
    "    \n",
    "    for s in used_sensors:\n",
    "        similar=(s==summary_features)\n",
    "        \n",
    "        #used_complete_features=(all_complete_features[similar.astype(int)])\n",
    "       \n",
    "        used_sensor_feature_names=np.logical_or(used_sensor_feature_names,similar)\n",
    "        used_feature_actualnames=np.logical_or(used_feature_actualnames,similar)\n",
    "    \n",
    "    X_train=X_train[:,used_sensor_feature_names]\n",
    "    long_names=all_complete_features[used_feature_actualnames]\n",
    "    return X_train,long_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Returns a standardized (0 mean, 1 variance) dataset\n",
    "def standardize(X_train):\n",
    "    mean=np.nanmean(X_train,axis=0).reshape((1,-1))# Ignores NaNs while finding the mean across rows\n",
    "    standard_dev=np.nanstd(X_train,axis=0) # Ignores NaNs while finding the standard deviation across rows\n",
    "    standard_dev_nonzero=np.where(standard_dev>0,standard_dev,1.).reshape((1,-1)) # Div zero\n",
    "    \n",
    "    X=(X_train-mean)/standard_dev_nonzero\n",
    "    return X,mean,standard_dev_nonzero   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Sensor Types, Label Possibilities variables\n",
    "sensor_types=['phone_acc','phone_gyro','phone_mag','phone_loc','phone_audio',\n",
    "'phone_app','phone_battery','phone_use','phone_callstat','phone_wifi','phone_lf',\n",
    "'phone_time']\n",
    "label_possibilities=['LOC_home','OR_indoors','PHONE_ON_TABLE','SITTING','WITH_FRIENDS',\n",
    " 'LYING_DOWN','SLEEPING','WATCHING_TV','EATING','PHONE_IN_POCKET',\n",
    " 'TALKING','DRIVE_-_I_M_A_PASSENGER','OR_standing','IN_A_CAR',\n",
    " 'OR_exercise','AT_THE_GYM','SINGING','FIX_walking','OR_outside',\n",
    " 'SHOPPING','AT_SCHOOL','BATHING_-_SHOWER','DRESSING','DRINKING__ALCOHOL_',\n",
    " 'PHONE_IN_HAND','FIX_restaurant','IN_CLASS','PHONE_IN_BAG','IN_A_MEETING',\n",
    " 'TOILET','COOKING','ELEVATOR','FIX_running','BICYCLING','LAB_WORK',\n",
    " 'LOC_main_workplace','ON_A_BUS','DRIVE_-_I_M_THE_DRIVER','STROLLING',\n",
    " 'CLEANING','DOING_LAUNDRY','WASHING_DISHES','SURFING_THE_INTERNET',\n",
    " 'AT_A_PARTY','AT_A_BAR','LOC_beach','COMPUTER_WORK','GROOMING','STAIRS_-_GOING_UP',\n",
    " 'STAIRS_-_GOING_DOWN','WITH_CO-WORKERS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data on particular sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 3600D531-0C55-44A7-AE95-A7A38519464E.features_labels.csv.gz\n",
      "Data shape input for user (Len minutes/num examples, num sensors):  (5203, 225)\n",
      "Label shape for user (Len minutes, num labels):  (5203, 51) \n",
      "\n",
      "Sensor feature names:\n",
      "\n",
      "Activities and counts:\n",
      "[('LOC_home', 3040), ('OR_indoors', 2487), ('PHONE_ON_TABLE', 2179), ('SITTING', 1916), ('WITH_FRIENDS', 1730), ('LYING_DOWN', 1336), ('SLEEPING', 1021), ('WATCHING_TV', 912), ('EATING', 762), ('PHONE_IN_POCKET', 706), ('TALKING', 638), ('DRIVE_-_I_M_A_PASSENGER', 409), ('OR_standing', 384), ('IN_A_CAR', 342), ('OR_exercise', 162), ('AT_THE_GYM', 162), ('SINGING', 136), ('FIX_walking', 132), ('OR_outside', 127), ('SHOPPING', 111), ('AT_SCHOOL', 105), ('BATHING_-_SHOWER', 85), ('DRESSING', 67), ('DRINKING__ALCOHOL_', 66), ('PHONE_IN_HAND', 64), ('FIX_restaurant', 59), ('IN_CLASS', 54), ('PHONE_IN_BAG', 33), ('IN_A_MEETING', 27), ('TOILET', 12), ('COOKING', 5), ('ELEVATOR', 1), ('FIX_running', 0), ('BICYCLING', 0), ('LAB_WORK', 0), ('LOC_main_workplace', 0), ('ON_A_BUS', 0), ('DRIVE_-_I_M_THE_DRIVER', 0), ('STROLLING', 0), ('CLEANING', 0), ('DOING_LAUNDRY', 0), ('WASHING_DISHES', 0), ('SURFING_THE_INTERNET', 0), ('AT_A_PARTY', 0), ('AT_A_BAR', 0), ('LOC_beach', 0), ('COMPUTER_WORK', 0), ('GROOMING', 0), ('STAIRS_-_GOING_UP', 0), ('STAIRS_-_GOING_DOWN', 0), ('WITH_CO-WORKERS', 0)]\n"
     ]
    }
   ],
   "source": [
    "# Reading sample data\n",
    "sample_loc='{}/{}.csv.gz'.format(prefix,user_sample)\n",
    "x_user,y_user,missedlabel_user,tstamp_user,featurename_user,labelname_user=read_user_data(sample_loc)\n",
    "\n",
    "# Dataset summaries for this user\n",
    "print('Data shape input for user (Len minutes/num examples, num sensors): ',x_user.shape) # Timestep examples, number of sensors\n",
    "print('Label shape for user (Len minutes, num labels): ',y_user.shape,'\\n') # Timestep examples, labels\n",
    "\n",
    "countlabels_user=np.sum(y_user,axis=0) # Column summary\n",
    "labelname_countlabel_user=zip(labelname_user,countlabels_user) # Zip together names, counts\n",
    "labelname_countlabel_user=sorted(labelname_countlabel_user,key=lambda row:row[-1],reverse=True)\n",
    "\n",
    "print('Sensor feature names:\\n')\n",
    "feature_names_woriginallabels=summarize_features_worig(featurename_user)\n",
    "    \n",
    "print('Activities and counts:')\n",
    "print(labelname_countlabel_user)\n",
    "\n",
    "x_train_chosen,feature_long_names=choose_sensors_longnames(x_user,sensor_types,feature_names_woriginallabels)\n",
    "# feature_long_names is original long feature name from the chosen sensor list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new data structure for all valid data and pickling it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with np.nan labels (missing labels). Zero impute missing feature entries. Standardization done at train time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping step\n"
     ]
    }
   ],
   "source": [
    "# Skipping cell if the data files were already created previously \n",
    "if done!=1:\n",
    "    # Reading data in the directory (Stacked)\n",
    "    \n",
    "    #M_train_t=np.empty((0,51))\n",
    "    #M_test_t=np.empty((0,51))\n",
    "    for fold_n in [0,1,2,3,4]:\n",
    "        X_train_t=np.empty((0,170))\n",
    "        Y_train_t=np.empty((0,51))\n",
    "        X_test_t=np.empty((0,170))\n",
    "        Y_test_t=np.empty((0,51))\n",
    "        train = glob.glob(cross_validation_user_loc+'fold_%d_train_*_uuids.txt'%fold_n)\n",
    "        test = glob.glob(cross_validation_user_loc+'fold_%d_test_*_uuids.txt'%fold_n)\n",
    "        for tr in train:\n",
    "            with open(tr,'r') as file:\n",
    "                for line in file:\n",
    "                    line = line.replace(\"\\n\",\"\")\n",
    "                    \n",
    "                    (x_user_train,y_user_train,missed_label_user,tstamp_user,featurename_user,labelname_user) = read_user_data(prefix+line+'.features_labels.csv.gz')\n",
    "                    x_sh=x_user_train.shape\n",
    "                    y_sh=y_user_train.shape\n",
    "                    y_user_train[np.isnan(y_user_train)]=-1\n",
    "                    x_user_train=np.nan_to_num(x_user_train)\n",
    "                    \n",
    "                    x_user_train=choose_sensors(x_user_train,used_sensors=sensor_types,summarized_feature_names=feature_names)\n",
    "                    X_train_t=np.vstack((X_train_t,x_user_train))\n",
    "                    Y_train_t=np.vstack((Y_train_t,y_user_train))\n",
    "        X_train_t,mean,dev=standardize(X_train_t)\n",
    "        assert len(X_train_t)==len(Y_train_t)\n",
    "        print('\\nTraining: Fold::{} X::{} ,Y::{}'.format(fold_n,X_train_t.shape,Y_train_t.shape))\n",
    "\n",
    "        for te in test:\n",
    "            with open(te,'r') as file:\n",
    "                for line in file:\n",
    "                    line = line.replace(\"\\n\",\"\")\n",
    "                   \n",
    "                    (x_user_test,y_user_test,missed_label_user,tstamp_user,featurename_user,labelname_user) = read_user_data(prefix+line+'.features_labels.csv.gz')\n",
    "                    x_sh=x_user_test.shape\n",
    "                    y_sh=y_user_test.shape\n",
    "                    y_user_test[np.isnan(y_user_test)]=-1\n",
    "                    x_user_test=np.nan_to_num(x_user_test)\n",
    "                    \n",
    "                    x_user_test=choose_sensors(x_user_test,used_sensors=sensor_types,summarized_feature_names=feature_names)\n",
    "                    X_test_t=np.vstack((X_test_t,x_user_test))\n",
    "                    Y_test_t=np.vstack((Y_test_t,y_user_test))\n",
    "        X_test_t=(X_test_t-mean)/dev\n",
    "\n",
    "        assert len(X_test_t)==len(Y_test_t)\n",
    "        print('\\nTesting: Fold::{} X::{} ,Y::{}'.format(fold_n,X_test_t.shape,Y_test_t.shape))\n",
    "        \n",
    "        print(\"Pickling data files\")\n",
    "        # Split datasets\n",
    "        with open('dataset/pickled/x_train_cv_orig{}.pkl'.format(fold_n),'wb') as f:\n",
    "            pickle.dump(X_train_t,f)\n",
    "        with open('dataset/pickled/y_train_cv_orig{}.pkl'.format(fold_n),'wb') as f:\n",
    "            pickle.dump(Y_train_t,f)\n",
    "        with open('dataset/pickled/x_test_cv_orig{}.pkl'.format(fold_n),'wb') as f:\n",
    "            pickle.dump(X_test_t,f)\n",
    "        with open('dataset/pickled/y_test_cv_orig{}.pkl'.format(fold_n),'wb') as f:\n",
    "            pickle.dump(Y_test_t,f)\n",
    "        print(\"Done for fold {}\".format(fold_n))\n",
    "    print (\"DONE\") \n",
    "else:\n",
    "    print(\"Skipping step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and pickling instance weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Creating an instance weight matrix for the training labels\n",
    "def instance_weight_matrix(y_train):\n",
    "    instance_weights=np.zeros_like(y_train)\n",
    "    for l in range(len(labelname_user)):\n",
    "        temp_column=y_train[:,l]\n",
    "        count_neg=0\n",
    "        count_0=0\n",
    "        count_1=0\n",
    "        for i in range(len(temp_column)): # n^2 bincount doesn't work with arrays consisting of negative numbers\n",
    "            if (temp_column[i]==-1):\n",
    "                count_neg+=1\n",
    "            elif (temp_column[i]==0):\n",
    "                count_0+=1\n",
    "            elif (temp_column[i]==1):\n",
    "                count_1+=1\n",
    "            else:\n",
    "                raise ValueError(\"Bad Loop\")\n",
    "        weight_0=float((count_0+count_1)/count_0)\n",
    "        weight_1=float((count_0+count_1)/count_1)\n",
    "\n",
    "        weight_0=weight_0/(weight_0+weight_1)\n",
    "        weight_1=weight_1/(weight_0+weight_1)\n",
    "\n",
    "        for i in range(len(temp_column)):\n",
    "            if (temp_column[i]==-1):\n",
    "                instance_weights[i,l]=0.\n",
    "            elif (temp_column[i]==0):\n",
    "                instance_weights[i,l]=weight_0\n",
    "            elif (temp_column[i]==1):\n",
    "                instance_weights[i,l]=weight_1\n",
    "    return instance_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping step\n"
     ]
    }
   ],
   "source": [
    "# Skipping cell if the data files were already created previously \n",
    "done=1\n",
    "if done!=1:\n",
    "    weights = dict()\n",
    "    for fold_n in [0,1,2,3,4]:\n",
    "        with open('dataset/pickled/y_train_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "            y_train=pickle.load(f)\n",
    "            y_train=unclassified_labels(y_train)\n",
    "\n",
    "        weights[fold_n] = instance_weight_matrix(y_train)\n",
    "\n",
    "    for fold_n in [0,1,2,3,4]:\n",
    "        with open('dataset/pickled/instance_weights_cv_orig{}.pkl'.format(fold_n),'wb') as f:\n",
    "            pickle.dump(weights[fold_n],f)\n",
    "else:\n",
    "    print(\"Skipping step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous train/test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda-enable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Simple function to run using GPU when available\n",
    "def C(structure):\n",
    "    if torch.cuda.is_available():\n",
    "        device=torch.device(\"cuda\")\n",
    "        return structure.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tackling missing labels using a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create a mask to hide -1 nans before training and then input to a train criterion\n",
    "def mask(criterion,y_true,y_pred,mask_value):\n",
    "    mask=torch.ne(y_true,mask_value).type(torch.cuda.FloatTensor)\n",
    "    # Cast the ByteTensor from elementwise comparison to a FloatTensor\n",
    "    return criterion(torch.mul(y_pred,mask),torch.mul(y_true,mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Learning-Rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Linear decreasing LR scheduler\n",
    "def linear_lr_scheduler(optimizer,epoch):\n",
    "    \"\"\"\n",
    "    LR_init=0.1, LR_final=0.01, n_epochs=40\n",
    "    Sets the learning rate to the initial LR decayed by 1.04 every epoch\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr=param_group['lr']\n",
    "    m=-3/1300\n",
    "    c=0.1\n",
    "    lr=(epoch*m)+c # Linear LR decay based on a set number of epochs\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr']=lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Norm for weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Adds euclidean regularization to weight matrices\n",
    "def frobenius_norm(model,loss):\n",
    "    regularizer_loss=0\n",
    "    \n",
    "    for m in model.modules():\n",
    "        if isinstance(m,nn.Linear): # Linear layer\n",
    "            frobenius_norm=torch.norm(m.weight,p='fro')\n",
    "            regularizer_loss+=frobenius_norm # Regularization over the weight matrices for linear layers\n",
    "    return loss+0.001*regularizer_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy (Precision, Recall, F1, Support, Balanced Accuracy) metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Function for the required accuracy metrics per fold\n",
    "def accuracy(fold,target_labels,y_true,y_pred):\n",
    "    y_true=y_true.detach().numpy()\n",
    "    y_pred=y_pred.detach().numpy()\n",
    "    balanced_accuracy_dict={}\n",
    "    print('*'*20)\n",
    "    print('For fold {}'.format(fold))\n",
    "    # Precision, Recall, F1, Support\n",
    "#     clf_report=classification_report(y_true=y_true,y_pred=y_pred,\n",
    "#                                       target_names=target_labels,output_dict=True)\n",
    "    # Balanced accuracy\n",
    "    for i in range(len(target_labels)):\n",
    "        true_perlabel=y_true[:,i]\n",
    "        pred_perlabel=y_pred[:,i]\n",
    "        bal_acc=balanced_accuracy_score(y_true=true_perlabel,y_pred=pred_perlabel)\n",
    "        print('\\t Label {}:::-> Balanced Accuracy {}'.format(target_labels[i],round(bal_acc,7)))\n",
    "        balanced_accuracy_dict[target_labels[i]]=round(bal_acc,5)\n",
    "    return balanced_accuracy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold cross validation Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Train function w/BCE loss, linear LR scheduler, instance weights\n",
    "def train(model,X,Y,X_test,Y_test,weights,n_epoch,batch_size,lr_init,momentum,fold):\n",
    "    \n",
    "    optimizer=optim.SGD(model.parameters(),lr=lr_init,momentum=momentum)\n",
    "\n",
    "    X=V(torch.cuda.FloatTensor(X),requires_grad=True)\n",
    "    Y=V(torch.cuda.FloatTensor(Y),requires_grad=False)\n",
    "    X_test=V(torch.cuda.FloatTensor(X_test),requires_grad=False)\n",
    "    Y_test=V(torch.cuda.FloatTensor(Y_test),requires_grad=False)\n",
    "    weights=V(torch.cuda.FloatTensor(weights),requires_grad=False)\n",
    "   \n",
    "    # Cuda-Compatible Model\n",
    "    model = C(model)\n",
    "    # Create dataloaders\n",
    "    # Dataloader creation\n",
    "    # Wrap weights for instance weight tensor along with data & label tensors s.t.\n",
    "    # it can be called properly as a dataloader in batches.\n",
    "    train_dataset=utils.TensorDataset(X,Y,weights)\n",
    "    train_loader=utils.DataLoader(dataset=train_dataset,batch_size=bs\n",
    "                                  ,shuffle=False,drop_last=False)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        linear_lr_scheduler(optimizer,epoch)\n",
    "        for i,data in enumerate(train_loader,0):\n",
    "\n",
    "            inputs,labels,weights=data\n",
    "            inputs=V(torch.cuda.FloatTensor(inputs),requires_grad=True)\n",
    "            labels=V(torch.cuda.FloatTensor(labels),requires_grad=False)\n",
    "            weights=V(torch.cuda.FloatTensor(weights),requires_grad=False)\n",
    "\n",
    "            criterion=C(nn.BCEWithLogitsLoss(weight=weights))\n",
    "            optimizer.zero_grad()   \n",
    "            sum_total=0\n",
    "\n",
    "            outputs=model(inputs)\n",
    "\n",
    "            # Zero gradients, backward pass, weight update\n",
    "            loss=criterion(outputs,labels) \n",
    "            #loss=mask(criterion=criterion,y_true=labels,y_pred=outputs,mask_value=-1)\n",
    "            loss=frobenius_norm(model,loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_total+=loss.item()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                epoch_lr=param_group['lr']\n",
    "\n",
    "            print(\"Epoch {}::Minibatch {}::LR {} --> Loss {}\".format(epoch+1,i+1,epoch_lr,sum_total/bs))\n",
    "            sum_total=0.\n",
    "        \n",
    "    \n",
    "    print(\"Training finished, Prediction\")\n",
    "    \n",
    "    model.eval() # Evaluation model\n",
    "    \n",
    "    y_pred=torch.sigmoid(model(X))>=0.5\n",
    "    fold_train_dict=accuracy(fold,labelname_user,Y.cpu(),y_pred.cpu())\n",
    "    \n",
    "    Y_test_pred=torch.sigmoid(model(X_test))>=0.5\n",
    "    fold_test_dict=accuracy(fold,labelname_user,Y_test.cpu(),Y_test_pred.cpu())\n",
    "    \n",
    "    model.train() # Back to train model\n",
    "    \n",
    "    return fold_train_dict,fold_test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom K-Fold cross validation Train Function w/ Sensor Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "    Which sensors are dropped is based on sensor dropout probability (20 %) and changes every minibatch.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Randomly choose sensors (features) and replace those by 0 (dropout-ish)\n",
    "def random_choice(sensor_list,feature_names,x_dataset):\n",
    "    '''\n",
    "    Sensor list if of the form:\n",
    "    sensor_list=['phone_acc','phone_gyro','phone_mag','phone_loc','phone_audio',\n",
    "'phone_app','phone_battery','phone_use','phone_callstat','phone_wifi','phone_lf',\n",
    "'phone_time']\n",
    "    '''\n",
    "    sensor_length=len(sensor_list)\n",
    "    chosen_sensors=np.random.choice(sensor_list,math.floor(sensor_length*0.8),replace=False)\n",
    "    ignored_sensors=list(set(sensor_list)-set(chosen_sensors)) \n",
    "    print(\"\\t\\t\\tIgnoring {}\".format(ignored_sensors))\n",
    "    new_summary_features=summarize_features(feature_long_names)\n",
    "    x_dataset=choose_sensors_dropout(x_dataset,chosen_sensors,new_summary_features)\n",
    "    \n",
    "    return x_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Train function w/BCE loss, linear LR scheduler, instance weights\n",
    "def train_sensordropout(model,X,Y,X_test,Y_test,weights,sensor_list,feature_names,n_epoch,batch_size,lr_init,momentum,fold):\n",
    "    \n",
    "    optimizer=optim.SGD(model.parameters(),lr=lr_init,momentum=momentum)\n",
    "\n",
    "    X=V(torch.cuda.FloatTensor(X),requires_grad=True)\n",
    "    Y=V(torch.cuda.FloatTensor(Y),requires_grad=False)\n",
    "    X_test=V(torch.cuda.FloatTensor(X_test),requires_grad=False)\n",
    "    Y_test=V(torch.cuda.FloatTensor(Y_test),requires_grad=False)\n",
    "    weights=V(torch.cuda.FloatTensor(weights),requires_grad=False)\n",
    "   \n",
    "    # Cuda-Compatible Model\n",
    "    model = C(model)\n",
    "    # Create dataloaders\n",
    "    # Dataloader creation\n",
    "    # Wrap weights for instance weight tensor along with data & label tensors s.t.\n",
    "    # it can be called properly as a dataloader in batches.\n",
    "    train_dataset=utils.TensorDataset(X,Y,weights)\n",
    "    train_loader=utils.DataLoader(dataset=train_dataset,batch_size=bs\n",
    "                                  ,shuffle=False,drop_last=False)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        linear_lr_scheduler(optimizer,epoch)\n",
    "        for i,data in enumerate(train_loader,0):\n",
    "\n",
    "            inputs,labels,weights=data\n",
    "            inputs_detached=inputs.cpu().detach().numpy()\n",
    "            \n",
    "            inputs=random_choice(sensor_list,feature_names,inputs_detached) # Sensor Dropout\n",
    "            \n",
    "            inputs=V(torch.cuda.FloatTensor(inputs),requires_grad=True)\n",
    "            labels=V(torch.cuda.FloatTensor(labels),requires_grad=False)\n",
    "            weights=V(torch.cuda.FloatTensor(weights),requires_grad=False)\n",
    "\n",
    "            criterion=C(nn.BCEWithLogitsLoss(weight=weights))\n",
    "            optimizer.zero_grad()   \n",
    "            sum_total=0\n",
    "\n",
    "            outputs=model(inputs)\n",
    "\n",
    "            # Zero gradients, backward pass, weight update\n",
    "            loss=criterion(outputs,labels) \n",
    "#             loss=mask(criterion=criterion,y_true=labels,y_pred=outputs,mask_value=-1)\n",
    "            loss=frobenius_norm(model,loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_total+=loss.item()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                epoch_lr=param_group['lr']\n",
    "\n",
    "            print(\"Epoch {}::Minibatch {}::LR {} --> Loss {}\".format(epoch+1,i+1,epoch_lr,sum_total/bs))\n",
    "            sum_total=0.\n",
    "        \n",
    "    \n",
    "    print(\"Training finished, Prediction\")\n",
    "    \n",
    "    model.eval() # Evaluation model\n",
    "    \n",
    "    y_pred=torch.sigmoid(model(X))>=0.5\n",
    "    fold_train_dict=accuracy(fold,labelname_user,Y.cpu(),y_pred.cpu())\n",
    "    \n",
    "    Y_test_pred=torch.sigmoid(model(X_test))>=0.5\n",
    "    fold_test_dict=accuracy(fold,labelname_user,Y_test.cpu(),Y_test_pred.cpu())\n",
    "    \n",
    "    model.train() # Back to train model\n",
    "    \n",
    "    return fold_train_dict,fold_test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unclassified column if no labels are marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def unclassified_labels(label_set):\n",
    "    df=pd.DataFrame(label_set)\n",
    "    df['UNK']=np.where(df.sum(axis=1)==0.,float(1.0),float(0.0))\n",
    "    print(\"Unclassified labels: \",df[df['UNK']==1.0].shape[0])\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Class Classifier: Learning and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Hyperparameter variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size 170, Output Size 51\n"
     ]
    }
   ],
   "source": [
    "# Defining sizes for neural networks and other global hyperparameters\n",
    "# input_size=x_train[0].shape[-1]\n",
    "input_size=170\n",
    "hidden_size=16\n",
    "# output_size=y_train[0].shape[-1]\n",
    "output_size=51\n",
    "n_epoch=40\n",
    "bs=300\n",
    "lr_init=0.1\n",
    "momentum=0.5\n",
    "print('Input Size {}, Output Size {}'.format(input_size,output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (0 Hidden Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearMLP(\n",
      "  (fc1): Linear(in_features=170, out_features=51, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Linear MLP no hidden layer\n",
    "class LinearMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearMLP,self).__init__()\n",
    "        self.fc1=nn.Linear(input_size,output_size)\n",
    "    def forward(self,x):\n",
    "        x=self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "model=LinearMLP()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Iteratively loading CV data to prevent memory issues. Will have to recreate weight matrix \n",
    "# now that we have a new column \n",
    "\n",
    "mlp0H_train_dict=dict()\n",
    "mlp0H_test_dict=dict()\n",
    "\n",
    "for fold_n in [0,1,2,3,4]:\n",
    "    print(\"*\"*50)\n",
    "    print(\"Fold {} loading datasets\".format(fold_n))\n",
    "    with open('dataset/pickled/x_train_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        x_train_temp=pickle.load(f)\n",
    "        x_train,mean,dev = standardize(x_train_temp)\n",
    "    \n",
    "    with open('dataset/pickled/y_train_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        y_train=pickle.load(f)\n",
    "        \n",
    "    \n",
    "    with open('dataset/pickled/x_test_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        x_test_temp=pickle.load(f)\n",
    "        x_test=(x_test_temp-mean)/dev\n",
    "    \n",
    "    with open('dataset/pickled/y_test_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        y_test=pickle.load(f)\n",
    "        \n",
    "        \n",
    "    with open('dataset/pickled/instance_weights_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        weights=pickle.load(f)\n",
    "\n",
    "    # 1. \n",
    "    # Unclassified Labels Method: Adds a new label named Unclassified if all of the previous labels are 0\n",
    "    # Issue might wrongly cluster together different unlabeled activities together\n",
    "    # y_train=unclassified_labels(y_train)\n",
    "    # y_test=unclassified_labels(y_test)\n",
    "    \n",
    "    # 2.\n",
    "    # Row removal Method: Remove data points where all of the labels are 0.\n",
    "    train_mask=np.where(np.sum(y_train,axis=1)==0) # Where zeros\n",
    "    x_train=np.delete(x_train,train_mask,axis=0)\n",
    "    weights=weights[:,:-1] # Remove last column because they were created with Method 1 in mind\n",
    "    weights=np.delete(weights,train_mask,axis=0)\n",
    "    y_train=np.delete(y_train,train_mask,axis=0)\n",
    "    test_mask=np.where(np.sum(y_test,axis=1)==0) # Where zeros\n",
    "    x_test=np.delete(x_test,test_mask,axis=0)\n",
    "    y_test=np.delete(y_test,test_mask,axis=0)\n",
    "    \n",
    "        \n",
    "    print(\"\\t Loaded datasets\")\n",
    "    \n",
    "    model=LinearMLP() # Creating a new instance of the model every fold\n",
    "    \n",
    "    mlp0H_train_dict[fold_n],mlp0H_test_dict[fold_n]=train(model,\n",
    "                                                           X=x_train,\n",
    "                                                           Y=y_train,\n",
    "                                                           X_test=x_test,\n",
    "                                                           Y_test=y_test,\n",
    "                                                           weights=weights,\n",
    "                                                           n_epoch=n_epoch,\n",
    "                                                           batch_size=bs,\n",
    "                                                           lr_init=lr_init,\n",
    "                                                           momentum=momentum,\n",
    "                                                           fold=fold_n)\n",
    "    print(\"Finished for fold {}\".format(fold_n))\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Accuracy outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LYING_DOWN</th>\n",
       "      <td>0.86739</td>\n",
       "      <td>0.81829</td>\n",
       "      <td>0.85181</td>\n",
       "      <td>0.84996</td>\n",
       "      <td>0.81575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SITTING</th>\n",
       "      <td>0.72942</td>\n",
       "      <td>0.73009</td>\n",
       "      <td>0.75627</td>\n",
       "      <td>0.76492</td>\n",
       "      <td>0.65466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIX_walking</th>\n",
       "      <td>0.80476</td>\n",
       "      <td>0.74695</td>\n",
       "      <td>0.79259</td>\n",
       "      <td>0.78806</td>\n",
       "      <td>0.76839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIX_running</th>\n",
       "      <td>0.78636</td>\n",
       "      <td>0.80603</td>\n",
       "      <td>0.59859</td>\n",
       "      <td>0.44222</td>\n",
       "      <td>0.77196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BICYCLING</th>\n",
       "      <td>0.80183</td>\n",
       "      <td>0.84042</td>\n",
       "      <td>0.82024</td>\n",
       "      <td>0.82637</td>\n",
       "      <td>0.84159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SLEEPING</th>\n",
       "      <td>0.87711</td>\n",
       "      <td>0.82771</td>\n",
       "      <td>0.84790</td>\n",
       "      <td>0.85840</td>\n",
       "      <td>0.79844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAB_WORK</th>\n",
       "      <td>0.83918</td>\n",
       "      <td>0.78227</td>\n",
       "      <td>0.84299</td>\n",
       "      <td>0.81250</td>\n",
       "      <td>0.36278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN_CLASS</th>\n",
       "      <td>0.76747</td>\n",
       "      <td>0.77917</td>\n",
       "      <td>0.80571</td>\n",
       "      <td>0.77243</td>\n",
       "      <td>0.70345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN_A_MEETING</th>\n",
       "      <td>0.76559</td>\n",
       "      <td>0.69057</td>\n",
       "      <td>0.62463</td>\n",
       "      <td>0.78433</td>\n",
       "      <td>0.58153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC_main_workplace</th>\n",
       "      <td>0.80680</td>\n",
       "      <td>0.80291</td>\n",
       "      <td>0.75707</td>\n",
       "      <td>0.79295</td>\n",
       "      <td>0.72514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_indoors</th>\n",
       "      <td>0.72664</td>\n",
       "      <td>0.72051</td>\n",
       "      <td>0.65979</td>\n",
       "      <td>0.66631</td>\n",
       "      <td>0.56496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_outside</th>\n",
       "      <td>0.79505</td>\n",
       "      <td>0.81010</td>\n",
       "      <td>0.76702</td>\n",
       "      <td>0.76713</td>\n",
       "      <td>0.75893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN_A_CAR</th>\n",
       "      <td>0.87646</td>\n",
       "      <td>0.81254</td>\n",
       "      <td>0.80586</td>\n",
       "      <td>0.64712</td>\n",
       "      <td>0.77139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ON_A_BUS</th>\n",
       "      <td>0.84101</td>\n",
       "      <td>0.75149</td>\n",
       "      <td>0.78919</td>\n",
       "      <td>0.79631</td>\n",
       "      <td>0.82541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIVE_-_I_M_THE_DRIVER</th>\n",
       "      <td>0.86678</td>\n",
       "      <td>0.83628</td>\n",
       "      <td>0.83184</td>\n",
       "      <td>0.73015</td>\n",
       "      <td>0.81033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIVE_-_I_M_A_PASSENGER</th>\n",
       "      <td>0.72789</td>\n",
       "      <td>0.81262</td>\n",
       "      <td>0.77747</td>\n",
       "      <td>0.77985</td>\n",
       "      <td>0.77672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC_home</th>\n",
       "      <td>0.79814</td>\n",
       "      <td>0.72608</td>\n",
       "      <td>0.72594</td>\n",
       "      <td>0.76308</td>\n",
       "      <td>0.58551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIX_restaurant</th>\n",
       "      <td>0.71356</td>\n",
       "      <td>0.75787</td>\n",
       "      <td>0.78239</td>\n",
       "      <td>0.77922</td>\n",
       "      <td>0.62501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_IN_POCKET</th>\n",
       "      <td>0.69108</td>\n",
       "      <td>0.77608</td>\n",
       "      <td>0.80968</td>\n",
       "      <td>0.76862</td>\n",
       "      <td>0.63417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_exercise</th>\n",
       "      <td>0.74366</td>\n",
       "      <td>0.77504</td>\n",
       "      <td>0.77898</td>\n",
       "      <td>0.77835</td>\n",
       "      <td>0.69538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COOKING</th>\n",
       "      <td>0.71660</td>\n",
       "      <td>0.63194</td>\n",
       "      <td>0.73339</td>\n",
       "      <td>0.64025</td>\n",
       "      <td>0.67388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHOPPING</th>\n",
       "      <td>0.78975</td>\n",
       "      <td>0.69357</td>\n",
       "      <td>0.80472</td>\n",
       "      <td>0.63609</td>\n",
       "      <td>0.70906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STROLLING</th>\n",
       "      <td>0.77347</td>\n",
       "      <td>0.73422</td>\n",
       "      <td>0.85000</td>\n",
       "      <td>0.69442</td>\n",
       "      <td>0.75628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRINKING__ALCOHOL_</th>\n",
       "      <td>0.70024</td>\n",
       "      <td>0.72125</td>\n",
       "      <td>0.76537</td>\n",
       "      <td>0.74222</td>\n",
       "      <td>0.65947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BATHING_-_SHOWER</th>\n",
       "      <td>0.63838</td>\n",
       "      <td>0.54872</td>\n",
       "      <td>0.57144</td>\n",
       "      <td>0.53135</td>\n",
       "      <td>0.48173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLEANING</th>\n",
       "      <td>0.69695</td>\n",
       "      <td>0.59976</td>\n",
       "      <td>0.72733</td>\n",
       "      <td>0.62767</td>\n",
       "      <td>0.54487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOING_LAUNDRY</th>\n",
       "      <td>0.60242</td>\n",
       "      <td>0.66616</td>\n",
       "      <td>0.72184</td>\n",
       "      <td>0.67897</td>\n",
       "      <td>0.49110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WASHING_DISHES</th>\n",
       "      <td>0.37498</td>\n",
       "      <td>0.65982</td>\n",
       "      <td>0.42328</td>\n",
       "      <td>0.50266</td>\n",
       "      <td>0.57521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WATCHING_TV</th>\n",
       "      <td>0.76312</td>\n",
       "      <td>0.68925</td>\n",
       "      <td>0.74300</td>\n",
       "      <td>0.67695</td>\n",
       "      <td>0.71334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SURFING_THE_INTERNET</th>\n",
       "      <td>0.65540</td>\n",
       "      <td>0.56226</td>\n",
       "      <td>0.66904</td>\n",
       "      <td>0.62030</td>\n",
       "      <td>0.63848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_A_PARTY</th>\n",
       "      <td>0.72214</td>\n",
       "      <td>0.57468</td>\n",
       "      <td>0.70207</td>\n",
       "      <td>0.74999</td>\n",
       "      <td>0.75840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_A_BAR</th>\n",
       "      <td>0.60265</td>\n",
       "      <td>0.55911</td>\n",
       "      <td>0.75972</td>\n",
       "      <td>0.46309</td>\n",
       "      <td>0.76614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC_beach</th>\n",
       "      <td>0.62123</td>\n",
       "      <td>0.69434</td>\n",
       "      <td>0.72670</td>\n",
       "      <td>0.48088</td>\n",
       "      <td>0.50222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SINGING</th>\n",
       "      <td>0.69609</td>\n",
       "      <td>0.77615</td>\n",
       "      <td>0.19392</td>\n",
       "      <td>0.75571</td>\n",
       "      <td>0.64495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TALKING</th>\n",
       "      <td>0.66420</td>\n",
       "      <td>0.70549</td>\n",
       "      <td>0.70853</td>\n",
       "      <td>0.66490</td>\n",
       "      <td>0.56499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMPUTER_WORK</th>\n",
       "      <td>0.68017</td>\n",
       "      <td>0.77210</td>\n",
       "      <td>0.68008</td>\n",
       "      <td>0.75227</td>\n",
       "      <td>0.61016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EATING</th>\n",
       "      <td>0.66934</td>\n",
       "      <td>0.65352</td>\n",
       "      <td>0.65251</td>\n",
       "      <td>0.68304</td>\n",
       "      <td>0.62513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOILET</th>\n",
       "      <td>0.66954</td>\n",
       "      <td>0.64357</td>\n",
       "      <td>0.65581</td>\n",
       "      <td>0.57906</td>\n",
       "      <td>0.67473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GROOMING</th>\n",
       "      <td>0.72407</td>\n",
       "      <td>0.64363</td>\n",
       "      <td>0.58382</td>\n",
       "      <td>0.53187</td>\n",
       "      <td>0.51066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRESSING</th>\n",
       "      <td>0.72912</td>\n",
       "      <td>0.64527</td>\n",
       "      <td>0.64482</td>\n",
       "      <td>0.69747</td>\n",
       "      <td>0.52456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_THE_GYM</th>\n",
       "      <td>0.70107</td>\n",
       "      <td>0.72906</td>\n",
       "      <td>0.73675</td>\n",
       "      <td>0.57915</td>\n",
       "      <td>0.47031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STAIRS_-_GOING_UP</th>\n",
       "      <td>0.65855</td>\n",
       "      <td>0.50136</td>\n",
       "      <td>0.81218</td>\n",
       "      <td>0.84935</td>\n",
       "      <td>0.71472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STAIRS_-_GOING_DOWN</th>\n",
       "      <td>0.65478</td>\n",
       "      <td>0.57635</td>\n",
       "      <td>0.66280</td>\n",
       "      <td>0.88303</td>\n",
       "      <td>0.69220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELEVATOR</th>\n",
       "      <td>0.60494</td>\n",
       "      <td>0.74725</td>\n",
       "      <td>0.56235</td>\n",
       "      <td>0.65821</td>\n",
       "      <td>0.66542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_standing</th>\n",
       "      <td>0.62537</td>\n",
       "      <td>0.63675</td>\n",
       "      <td>0.66992</td>\n",
       "      <td>0.65740</td>\n",
       "      <td>0.64966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_SCHOOL</th>\n",
       "      <td>0.71772</td>\n",
       "      <td>0.70818</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.72677</td>\n",
       "      <td>0.64462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_IN_HAND</th>\n",
       "      <td>0.62111</td>\n",
       "      <td>0.71704</td>\n",
       "      <td>0.70374</td>\n",
       "      <td>0.66260</td>\n",
       "      <td>0.62596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_IN_BAG</th>\n",
       "      <td>0.71453</td>\n",
       "      <td>0.70423</td>\n",
       "      <td>0.80477</td>\n",
       "      <td>0.60992</td>\n",
       "      <td>0.61474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_ON_TABLE</th>\n",
       "      <td>0.56541</td>\n",
       "      <td>0.60327</td>\n",
       "      <td>0.61443</td>\n",
       "      <td>0.58273</td>\n",
       "      <td>0.62012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WITH_CO-WORKERS</th>\n",
       "      <td>0.76565</td>\n",
       "      <td>0.72666</td>\n",
       "      <td>0.73893</td>\n",
       "      <td>0.76030</td>\n",
       "      <td>0.40307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WITH_FRIENDS</th>\n",
       "      <td>0.64707</td>\n",
       "      <td>0.64541</td>\n",
       "      <td>0.63697</td>\n",
       "      <td>0.62108</td>\n",
       "      <td>0.56133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0        1        2        3        4\n",
       "LYING_DOWN               0.86739  0.81829  0.85181  0.84996  0.81575\n",
       "SITTING                  0.72942  0.73009  0.75627  0.76492  0.65466\n",
       "FIX_walking              0.80476  0.74695  0.79259  0.78806  0.76839\n",
       "FIX_running              0.78636  0.80603  0.59859  0.44222  0.77196\n",
       "BICYCLING                0.80183  0.84042  0.82024  0.82637  0.84159\n",
       "SLEEPING                 0.87711  0.82771  0.84790  0.85840  0.79844\n",
       "LAB_WORK                 0.83918  0.78227  0.84299  0.81250  0.36278\n",
       "IN_CLASS                 0.76747  0.77917  0.80571  0.77243  0.70345\n",
       "IN_A_MEETING             0.76559  0.69057  0.62463  0.78433  0.58153\n",
       "LOC_main_workplace       0.80680  0.80291  0.75707  0.79295  0.72514\n",
       "OR_indoors               0.72664  0.72051  0.65979  0.66631  0.56496\n",
       "OR_outside               0.79505  0.81010  0.76702  0.76713  0.75893\n",
       "IN_A_CAR                 0.87646  0.81254  0.80586  0.64712  0.77139\n",
       "ON_A_BUS                 0.84101  0.75149  0.78919  0.79631  0.82541\n",
       "DRIVE_-_I_M_THE_DRIVER   0.86678  0.83628  0.83184  0.73015  0.81033\n",
       "DRIVE_-_I_M_A_PASSENGER  0.72789  0.81262  0.77747  0.77985  0.77672\n",
       "LOC_home                 0.79814  0.72608  0.72594  0.76308  0.58551\n",
       "FIX_restaurant           0.71356  0.75787  0.78239  0.77922  0.62501\n",
       "PHONE_IN_POCKET          0.69108  0.77608  0.80968  0.76862  0.63417\n",
       "OR_exercise              0.74366  0.77504  0.77898  0.77835  0.69538\n",
       "COOKING                  0.71660  0.63194  0.73339  0.64025  0.67388\n",
       "SHOPPING                 0.78975  0.69357  0.80472  0.63609  0.70906\n",
       "STROLLING                0.77347  0.73422  0.85000  0.69442  0.75628\n",
       "DRINKING__ALCOHOL_       0.70024  0.72125  0.76537  0.74222  0.65947\n",
       "BATHING_-_SHOWER         0.63838  0.54872  0.57144  0.53135  0.48173\n",
       "CLEANING                 0.69695  0.59976  0.72733  0.62767  0.54487\n",
       "DOING_LAUNDRY            0.60242  0.66616  0.72184  0.67897  0.49110\n",
       "WASHING_DISHES           0.37498  0.65982  0.42328  0.50266  0.57521\n",
       "WATCHING_TV              0.76312  0.68925  0.74300  0.67695  0.71334\n",
       "SURFING_THE_INTERNET     0.65540  0.56226  0.66904  0.62030  0.63848\n",
       "AT_A_PARTY               0.72214  0.57468  0.70207  0.74999  0.75840\n",
       "AT_A_BAR                 0.60265  0.55911  0.75972  0.46309  0.76614\n",
       "LOC_beach                0.62123  0.69434  0.72670  0.48088  0.50222\n",
       "SINGING                  0.69609  0.77615  0.19392  0.75571  0.64495\n",
       "TALKING                  0.66420  0.70549  0.70853  0.66490  0.56499\n",
       "COMPUTER_WORK            0.68017  0.77210  0.68008  0.75227  0.61016\n",
       "EATING                   0.66934  0.65352  0.65251  0.68304  0.62513\n",
       "TOILET                   0.66954  0.64357  0.65581  0.57906  0.67473\n",
       "GROOMING                 0.72407  0.64363  0.58382  0.53187  0.51066\n",
       "DRESSING                 0.72912  0.64527  0.64482  0.69747  0.52456\n",
       "AT_THE_GYM               0.70107  0.72906  0.73675  0.57915  0.47031\n",
       "STAIRS_-_GOING_UP        0.65855  0.50136  0.81218  0.84935  0.71472\n",
       "STAIRS_-_GOING_DOWN      0.65478  0.57635  0.66280  0.88303  0.69220\n",
       "ELEVATOR                 0.60494  0.74725  0.56235  0.65821  0.66542\n",
       "OR_standing              0.62537  0.63675  0.66992  0.65740  0.64966\n",
       "AT_SCHOOL                0.71772  0.70818  0.74195  0.72677  0.64462\n",
       "PHONE_IN_HAND            0.62111  0.71704  0.70374  0.66260  0.62596\n",
       "PHONE_IN_BAG             0.71453  0.70423  0.80477  0.60992  0.61474\n",
       "PHONE_ON_TABLE           0.56541  0.60327  0.61443  0.58273  0.62012\n",
       "WITH_CO-WORKERS          0.76565  0.72666  0.73893  0.76030  0.40307\n",
       "WITH_FRIENDS             0.64707  0.64541  0.63697  0.62108  0.56133"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean dicts\n",
    "for k,v in mlp0H_test_dict.items():\n",
    "    if k==0:\n",
    "         mlp0H_test_dict_df=pd.DataFrame.from_dict(v,orient='index',columns=[str(k)])\n",
    "    else:\n",
    "        temp_df=pd.DataFrame.from_dict(v,orient='index',columns=[str(k)])\n",
    "        mlp0H_test_dict_df=pd.concat([mlp0H_test_dict_df,temp_df],axis=1)\n",
    "        \n",
    "mlp0H_test_dict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsrishankar/Desktop/Self_study/custom_dl_env/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type LinearMLP. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# Saving trained models\n",
    "save_dir='saved_models/multilabel_classifier/'\n",
    "model_path=save_dir+'mlp_0hidden_nomask'\n",
    "\n",
    "torch.save(model,model_path) # Saving the whole model\n",
    "\n",
    "# Save accuracy data\n",
    "data_sv_dir='summaries/'\n",
    "mlp0H_test_dict_df.to_csv(os.path.join(data_sv_dir,'mlp0H_rowremoval.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (1 Hidden Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_1H(\n",
      "  (hidden0): Sequential(\n",
      "    (0): Linear(in_features=170, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=51, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Linear MLP 1 hidden layer\n",
    "class MLP_1H(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_1H,self).__init__()\n",
    "        self.hidden0=nn.Sequential(\n",
    "            nn.Linear(input_size,hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "        self.out=nn.Sequential(\n",
    "            nn.Linear(hidden_size,output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.hidden0(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "# Train for MLP-1 Hidden\n",
    "model=MLP_1H()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Iteratively loading CV data to prevent memory issues\n",
    "\n",
    "mlp1H_train_dict=dict()\n",
    "mlp1H_test_dict=dict()\n",
    "\n",
    "for fold_n in [0,1,2,3,4]:\n",
    "    print(\"*\"*50)\n",
    "    print(\"Fold {} loading datasets\".format(fold_n))\n",
    "    with open('dataset/pickled/x_train_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        x_train_temp=pickle.load(f)\n",
    "        x_train,mean,dev = standardize(x_train_temp)\n",
    "    \n",
    "    with open('dataset/pickled/y_train_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        y_train=pickle.load(f)\n",
    "        \n",
    "    with open('dataset/pickled/x_test_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        x_test_temp=pickle.load(f)\n",
    "        x_test=(x_test_temp-mean)/dev\n",
    "    \n",
    "    with open('dataset/pickled/y_test_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        y_test=pickle.load(f)\n",
    "        \n",
    "    with open('dataset/pickled/instance_weights_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        weights=pickle.load(f)\n",
    "    \n",
    "    # 1. \n",
    "    # Unclassified Labels Method: Adds a new label named Unclassified if all of the previous labels are 0\n",
    "    # Issue might wrongly cluster together different unlabeled activities together\n",
    "    # y_train=unclassified_labels(y_train)\n",
    "    # y_test=unclassified_labels(y_test)\n",
    "    \n",
    "    # 2.\n",
    "    # Row removal Method: Remove data points where all of the labels are 0.\n",
    "    train_mask=np.where(np.sum(y_train,axis=1)==0) # Where zeros\n",
    "    x_train=np.delete(x_train,train_mask,axis=0)\n",
    "    weights=weights[:,:-1] # Remove last column because they were created with Method 1 in mind\n",
    "    weights=np.delete(weights,train_mask,axis=0)\n",
    "    y_train=np.delete(y_train,train_mask,axis=0)\n",
    "    test_mask=np.where(np.sum(y_test,axis=1)==0) # Where zeros\n",
    "    x_test=np.delete(x_test,test_mask,axis=0)\n",
    "    y_test=np.delete(y_test,test_mask,axis=0)\n",
    "    \n",
    "    print(\"\\t Loaded datasets\")\n",
    "    \n",
    "    model=MLP_1H() # Creating a new instance of the model every fold\n",
    "    \n",
    "    mlp1H_train_dict[fold_n],mlp1H_test_dict[fold_n]=train(model,\n",
    "                                                           X=x_train,\n",
    "                                                           Y=y_train,\n",
    "                                                           X_test=x_test,\n",
    "                                                           Y_test=y_test,\n",
    "                                                           weights=weights,\n",
    "                                                           n_epoch=n_epoch,\n",
    "                                                           batch_size=bs,\n",
    "                                                           lr_init=lr_init,\n",
    "                                                           momentum=momentum,\n",
    "                                                           fold=fold_n)\n",
    "    print(\"Finished for fold {}\".format(fold_n))\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Accuracy outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LYING_DOWN</th>\n",
       "      <td>0.87824</td>\n",
       "      <td>0.81199</td>\n",
       "      <td>0.86771</td>\n",
       "      <td>0.85233</td>\n",
       "      <td>0.79866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SITTING</th>\n",
       "      <td>0.72480</td>\n",
       "      <td>0.73371</td>\n",
       "      <td>0.75885</td>\n",
       "      <td>0.77010</td>\n",
       "      <td>0.65161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIX_walking</th>\n",
       "      <td>0.79520</td>\n",
       "      <td>0.75023</td>\n",
       "      <td>0.80778</td>\n",
       "      <td>0.79234</td>\n",
       "      <td>0.79013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIX_running</th>\n",
       "      <td>0.76608</td>\n",
       "      <td>0.74186</td>\n",
       "      <td>0.64665</td>\n",
       "      <td>0.61371</td>\n",
       "      <td>0.66768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BICYCLING</th>\n",
       "      <td>0.79882</td>\n",
       "      <td>0.84453</td>\n",
       "      <td>0.83312</td>\n",
       "      <td>0.83091</td>\n",
       "      <td>0.87564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SLEEPING</th>\n",
       "      <td>0.89277</td>\n",
       "      <td>0.82920</td>\n",
       "      <td>0.87185</td>\n",
       "      <td>0.86559</td>\n",
       "      <td>0.81184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAB_WORK</th>\n",
       "      <td>0.78709</td>\n",
       "      <td>0.82016</td>\n",
       "      <td>0.82035</td>\n",
       "      <td>0.81756</td>\n",
       "      <td>0.61323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN_CLASS</th>\n",
       "      <td>0.70694</td>\n",
       "      <td>0.76238</td>\n",
       "      <td>0.76480</td>\n",
       "      <td>0.78903</td>\n",
       "      <td>0.75998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN_A_MEETING</th>\n",
       "      <td>0.73397</td>\n",
       "      <td>0.72668</td>\n",
       "      <td>0.65639</td>\n",
       "      <td>0.75382</td>\n",
       "      <td>0.59531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC_main_workplace</th>\n",
       "      <td>0.84441</td>\n",
       "      <td>0.81779</td>\n",
       "      <td>0.78598</td>\n",
       "      <td>0.84318</td>\n",
       "      <td>0.74695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_indoors</th>\n",
       "      <td>0.72495</td>\n",
       "      <td>0.73581</td>\n",
       "      <td>0.68602</td>\n",
       "      <td>0.68185</td>\n",
       "      <td>0.60285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_outside</th>\n",
       "      <td>0.80814</td>\n",
       "      <td>0.82655</td>\n",
       "      <td>0.76351</td>\n",
       "      <td>0.77171</td>\n",
       "      <td>0.78744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN_A_CAR</th>\n",
       "      <td>0.86134</td>\n",
       "      <td>0.82303</td>\n",
       "      <td>0.80389</td>\n",
       "      <td>0.67887</td>\n",
       "      <td>0.79359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ON_A_BUS</th>\n",
       "      <td>0.80673</td>\n",
       "      <td>0.84665</td>\n",
       "      <td>0.82235</td>\n",
       "      <td>0.85432</td>\n",
       "      <td>0.70666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIVE_-_I_M_THE_DRIVER</th>\n",
       "      <td>0.88707</td>\n",
       "      <td>0.84906</td>\n",
       "      <td>0.83831</td>\n",
       "      <td>0.75791</td>\n",
       "      <td>0.79356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIVE_-_I_M_A_PASSENGER</th>\n",
       "      <td>0.74327</td>\n",
       "      <td>0.83584</td>\n",
       "      <td>0.77850</td>\n",
       "      <td>0.85448</td>\n",
       "      <td>0.79720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC_home</th>\n",
       "      <td>0.81130</td>\n",
       "      <td>0.77749</td>\n",
       "      <td>0.72834</td>\n",
       "      <td>0.78074</td>\n",
       "      <td>0.55952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIX_restaurant</th>\n",
       "      <td>0.72381</td>\n",
       "      <td>0.73594</td>\n",
       "      <td>0.78355</td>\n",
       "      <td>0.82534</td>\n",
       "      <td>0.78295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_IN_POCKET</th>\n",
       "      <td>0.69369</td>\n",
       "      <td>0.72812</td>\n",
       "      <td>0.79791</td>\n",
       "      <td>0.77103</td>\n",
       "      <td>0.79406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_exercise</th>\n",
       "      <td>0.64583</td>\n",
       "      <td>0.81393</td>\n",
       "      <td>0.79983</td>\n",
       "      <td>0.73046</td>\n",
       "      <td>0.70522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COOKING</th>\n",
       "      <td>0.71566</td>\n",
       "      <td>0.62700</td>\n",
       "      <td>0.65812</td>\n",
       "      <td>0.63678</td>\n",
       "      <td>0.73487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHOPPING</th>\n",
       "      <td>0.74609</td>\n",
       "      <td>0.74747</td>\n",
       "      <td>0.81390</td>\n",
       "      <td>0.72751</td>\n",
       "      <td>0.79251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STROLLING</th>\n",
       "      <td>0.84037</td>\n",
       "      <td>0.79037</td>\n",
       "      <td>0.78176</td>\n",
       "      <td>0.73919</td>\n",
       "      <td>0.68212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRINKING__ALCOHOL_</th>\n",
       "      <td>0.71915</td>\n",
       "      <td>0.69956</td>\n",
       "      <td>0.78776</td>\n",
       "      <td>0.74157</td>\n",
       "      <td>0.64008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BATHING_-_SHOWER</th>\n",
       "      <td>0.61261</td>\n",
       "      <td>0.47223</td>\n",
       "      <td>0.62607</td>\n",
       "      <td>0.49758</td>\n",
       "      <td>0.61940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLEANING</th>\n",
       "      <td>0.72493</td>\n",
       "      <td>0.65042</td>\n",
       "      <td>0.66051</td>\n",
       "      <td>0.63408</td>\n",
       "      <td>0.62070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOING_LAUNDRY</th>\n",
       "      <td>0.45162</td>\n",
       "      <td>0.72576</td>\n",
       "      <td>0.73159</td>\n",
       "      <td>0.68194</td>\n",
       "      <td>0.47853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WASHING_DISHES</th>\n",
       "      <td>0.56694</td>\n",
       "      <td>0.47316</td>\n",
       "      <td>0.49738</td>\n",
       "      <td>0.53211</td>\n",
       "      <td>0.70457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WATCHING_TV</th>\n",
       "      <td>0.77636</td>\n",
       "      <td>0.68846</td>\n",
       "      <td>0.75365</td>\n",
       "      <td>0.72906</td>\n",
       "      <td>0.73146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SURFING_THE_INTERNET</th>\n",
       "      <td>0.58248</td>\n",
       "      <td>0.59374</td>\n",
       "      <td>0.73876</td>\n",
       "      <td>0.63062</td>\n",
       "      <td>0.61865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_A_PARTY</th>\n",
       "      <td>0.79501</td>\n",
       "      <td>0.73825</td>\n",
       "      <td>0.80375</td>\n",
       "      <td>0.77846</td>\n",
       "      <td>0.79951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_A_BAR</th>\n",
       "      <td>0.56452</td>\n",
       "      <td>0.57466</td>\n",
       "      <td>0.36686</td>\n",
       "      <td>0.73608</td>\n",
       "      <td>0.72319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC_beach</th>\n",
       "      <td>0.76879</td>\n",
       "      <td>0.50028</td>\n",
       "      <td>0.56885</td>\n",
       "      <td>0.48163</td>\n",
       "      <td>0.73090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SINGING</th>\n",
       "      <td>0.71099</td>\n",
       "      <td>0.65919</td>\n",
       "      <td>0.69653</td>\n",
       "      <td>0.89152</td>\n",
       "      <td>0.45272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TALKING</th>\n",
       "      <td>0.64480</td>\n",
       "      <td>0.70022</td>\n",
       "      <td>0.70855</td>\n",
       "      <td>0.67666</td>\n",
       "      <td>0.60752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMPUTER_WORK</th>\n",
       "      <td>0.67311</td>\n",
       "      <td>0.79934</td>\n",
       "      <td>0.69998</td>\n",
       "      <td>0.76298</td>\n",
       "      <td>0.64276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EATING</th>\n",
       "      <td>0.64144</td>\n",
       "      <td>0.67944</td>\n",
       "      <td>0.65686</td>\n",
       "      <td>0.67958</td>\n",
       "      <td>0.60880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOILET</th>\n",
       "      <td>0.65943</td>\n",
       "      <td>0.48675</td>\n",
       "      <td>0.58271</td>\n",
       "      <td>0.53863</td>\n",
       "      <td>0.58106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GROOMING</th>\n",
       "      <td>0.69060</td>\n",
       "      <td>0.49167</td>\n",
       "      <td>0.57468</td>\n",
       "      <td>0.64030</td>\n",
       "      <td>0.49585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRESSING</th>\n",
       "      <td>0.67013</td>\n",
       "      <td>0.63468</td>\n",
       "      <td>0.44786</td>\n",
       "      <td>0.58978</td>\n",
       "      <td>0.45644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_THE_GYM</th>\n",
       "      <td>0.69539</td>\n",
       "      <td>0.75747</td>\n",
       "      <td>0.74260</td>\n",
       "      <td>0.65258</td>\n",
       "      <td>0.83348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STAIRS_-_GOING_UP</th>\n",
       "      <td>0.49589</td>\n",
       "      <td>0.51393</td>\n",
       "      <td>0.75014</td>\n",
       "      <td>0.71348</td>\n",
       "      <td>0.82835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STAIRS_-_GOING_DOWN</th>\n",
       "      <td>0.63105</td>\n",
       "      <td>0.54262</td>\n",
       "      <td>0.69039</td>\n",
       "      <td>0.77596</td>\n",
       "      <td>0.81303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELEVATOR</th>\n",
       "      <td>0.07311</td>\n",
       "      <td>0.62814</td>\n",
       "      <td>0.64761</td>\n",
       "      <td>0.65395</td>\n",
       "      <td>0.65022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_standing</th>\n",
       "      <td>0.60603</td>\n",
       "      <td>0.63755</td>\n",
       "      <td>0.63529</td>\n",
       "      <td>0.63918</td>\n",
       "      <td>0.61085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_SCHOOL</th>\n",
       "      <td>0.73855</td>\n",
       "      <td>0.70483</td>\n",
       "      <td>0.74193</td>\n",
       "      <td>0.73976</td>\n",
       "      <td>0.66996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_IN_HAND</th>\n",
       "      <td>0.58921</td>\n",
       "      <td>0.72589</td>\n",
       "      <td>0.68833</td>\n",
       "      <td>0.69991</td>\n",
       "      <td>0.70617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_IN_BAG</th>\n",
       "      <td>0.73387</td>\n",
       "      <td>0.73798</td>\n",
       "      <td>0.80156</td>\n",
       "      <td>0.57770</td>\n",
       "      <td>0.68870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_ON_TABLE</th>\n",
       "      <td>0.56104</td>\n",
       "      <td>0.61277</td>\n",
       "      <td>0.63080</td>\n",
       "      <td>0.59988</td>\n",
       "      <td>0.61730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WITH_CO-WORKERS</th>\n",
       "      <td>0.76174</td>\n",
       "      <td>0.70385</td>\n",
       "      <td>0.74267</td>\n",
       "      <td>0.73226</td>\n",
       "      <td>0.76637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WITH_FRIENDS</th>\n",
       "      <td>0.52377</td>\n",
       "      <td>0.62614</td>\n",
       "      <td>0.64499</td>\n",
       "      <td>0.69500</td>\n",
       "      <td>0.58566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0        1        2        3        4\n",
       "LYING_DOWN               0.87824  0.81199  0.86771  0.85233  0.79866\n",
       "SITTING                  0.72480  0.73371  0.75885  0.77010  0.65161\n",
       "FIX_walking              0.79520  0.75023  0.80778  0.79234  0.79013\n",
       "FIX_running              0.76608  0.74186  0.64665  0.61371  0.66768\n",
       "BICYCLING                0.79882  0.84453  0.83312  0.83091  0.87564\n",
       "SLEEPING                 0.89277  0.82920  0.87185  0.86559  0.81184\n",
       "LAB_WORK                 0.78709  0.82016  0.82035  0.81756  0.61323\n",
       "IN_CLASS                 0.70694  0.76238  0.76480  0.78903  0.75998\n",
       "IN_A_MEETING             0.73397  0.72668  0.65639  0.75382  0.59531\n",
       "LOC_main_workplace       0.84441  0.81779  0.78598  0.84318  0.74695\n",
       "OR_indoors               0.72495  0.73581  0.68602  0.68185  0.60285\n",
       "OR_outside               0.80814  0.82655  0.76351  0.77171  0.78744\n",
       "IN_A_CAR                 0.86134  0.82303  0.80389  0.67887  0.79359\n",
       "ON_A_BUS                 0.80673  0.84665  0.82235  0.85432  0.70666\n",
       "DRIVE_-_I_M_THE_DRIVER   0.88707  0.84906  0.83831  0.75791  0.79356\n",
       "DRIVE_-_I_M_A_PASSENGER  0.74327  0.83584  0.77850  0.85448  0.79720\n",
       "LOC_home                 0.81130  0.77749  0.72834  0.78074  0.55952\n",
       "FIX_restaurant           0.72381  0.73594  0.78355  0.82534  0.78295\n",
       "PHONE_IN_POCKET          0.69369  0.72812  0.79791  0.77103  0.79406\n",
       "OR_exercise              0.64583  0.81393  0.79983  0.73046  0.70522\n",
       "COOKING                  0.71566  0.62700  0.65812  0.63678  0.73487\n",
       "SHOPPING                 0.74609  0.74747  0.81390  0.72751  0.79251\n",
       "STROLLING                0.84037  0.79037  0.78176  0.73919  0.68212\n",
       "DRINKING__ALCOHOL_       0.71915  0.69956  0.78776  0.74157  0.64008\n",
       "BATHING_-_SHOWER         0.61261  0.47223  0.62607  0.49758  0.61940\n",
       "CLEANING                 0.72493  0.65042  0.66051  0.63408  0.62070\n",
       "DOING_LAUNDRY            0.45162  0.72576  0.73159  0.68194  0.47853\n",
       "WASHING_DISHES           0.56694  0.47316  0.49738  0.53211  0.70457\n",
       "WATCHING_TV              0.77636  0.68846  0.75365  0.72906  0.73146\n",
       "SURFING_THE_INTERNET     0.58248  0.59374  0.73876  0.63062  0.61865\n",
       "AT_A_PARTY               0.79501  0.73825  0.80375  0.77846  0.79951\n",
       "AT_A_BAR                 0.56452  0.57466  0.36686  0.73608  0.72319\n",
       "LOC_beach                0.76879  0.50028  0.56885  0.48163  0.73090\n",
       "SINGING                  0.71099  0.65919  0.69653  0.89152  0.45272\n",
       "TALKING                  0.64480  0.70022  0.70855  0.67666  0.60752\n",
       "COMPUTER_WORK            0.67311  0.79934  0.69998  0.76298  0.64276\n",
       "EATING                   0.64144  0.67944  0.65686  0.67958  0.60880\n",
       "TOILET                   0.65943  0.48675  0.58271  0.53863  0.58106\n",
       "GROOMING                 0.69060  0.49167  0.57468  0.64030  0.49585\n",
       "DRESSING                 0.67013  0.63468  0.44786  0.58978  0.45644\n",
       "AT_THE_GYM               0.69539  0.75747  0.74260  0.65258  0.83348\n",
       "STAIRS_-_GOING_UP        0.49589  0.51393  0.75014  0.71348  0.82835\n",
       "STAIRS_-_GOING_DOWN      0.63105  0.54262  0.69039  0.77596  0.81303\n",
       "ELEVATOR                 0.07311  0.62814  0.64761  0.65395  0.65022\n",
       "OR_standing              0.60603  0.63755  0.63529  0.63918  0.61085\n",
       "AT_SCHOOL                0.73855  0.70483  0.74193  0.73976  0.66996\n",
       "PHONE_IN_HAND            0.58921  0.72589  0.68833  0.69991  0.70617\n",
       "PHONE_IN_BAG             0.73387  0.73798  0.80156  0.57770  0.68870\n",
       "PHONE_ON_TABLE           0.56104  0.61277  0.63080  0.59988  0.61730\n",
       "WITH_CO-WORKERS          0.76174  0.70385  0.74267  0.73226  0.76637\n",
       "WITH_FRIENDS             0.52377  0.62614  0.64499  0.69500  0.58566"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean dicts\n",
    "for k,v in mlp1H_test_dict.items():\n",
    "    if k==0:\n",
    "         mlp1H_test_dict_df=pd.DataFrame.from_dict(v,orient='index',columns=[str(k)])\n",
    "    else:\n",
    "        temp_df=pd.DataFrame.from_dict(v,orient='index',columns=[str(k)])\n",
    "        mlp1H_test_dict_df=pd.concat([mlp1H_test_dict_df,temp_df],axis=1)\n",
    "        \n",
    "mlp1H_test_dict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsrishankar/Desktop/Self_study/custom_dl_env/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type MLP_1H. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# Saving trained models\n",
    "save_dir='saved_models/multilabel_classifier/'\n",
    "model_path=save_dir+'mlp_1hidden_nomask'\n",
    "\n",
    "torch.save(model,model_path) # Saving the whole model\n",
    "\n",
    "# Save accuracy data\n",
    "data_sv_dir='summaries/'\n",
    "mlp1H_test_dict_df.to_csv(os.path.join(data_sv_dir,'mlp1H_rowremoval.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (2 Hidden Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_2H(\n",
      "  (hidden0): Sequential(\n",
      "    (0): Linear(in_features=170, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (hidden1): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=51, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP_2H(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_2H,self).__init__()\n",
    "        self.hidden0=nn.Sequential(\n",
    "            nn.Linear(input_size,hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "        self.hidden1=nn.Sequential(\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "        self.out=nn.Sequential(\n",
    "            nn.Linear(hidden_size,output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "# Train for MLP-2 Hidden\n",
    "model=MLP_2H()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Iteratively loading CV data to prevent memory issues\n",
    "\n",
    "mlp2H_train_dict=dict()\n",
    "mlp2H_test_dict=dict()\n",
    "\n",
    "for fold_n in [0,1,2,3,4]:\n",
    "    print(\"*\"*50)\n",
    "    print(\"Fold {} loading datasets\".format(fold_n))\n",
    "    with open('dataset/pickled/x_train_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        x_train_temp=pickle.load(f)\n",
    "        x_train,mean,dev = standardize(x_train_temp)\n",
    "    \n",
    "    with open('dataset/pickled/y_train_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        y_train=pickle.load(f)\n",
    "        \n",
    "    with open('dataset/pickled/x_test_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        x_test_temp=pickle.load(f)\n",
    "        x_test=(x_test_temp-mean)/dev\n",
    "    \n",
    "    with open('dataset/pickled/y_test_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        y_test=pickle.load(f)\n",
    "        \n",
    "    with open('dataset/pickled/instance_weights_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        weights=pickle.load(f)\n",
    "        \n",
    "    # 1. \n",
    "    # Unclassified Labels Method: Adds a new label named Unclassified if all of the previous labels are 0\n",
    "    # Issue might wrongly cluster together different unlabeled activities together\n",
    "    # y_train=unclassified_labels(y_train)\n",
    "    # y_test=unclassified_labels(y_test)\n",
    "    \n",
    "    # 2.\n",
    "    # Row removal Method: Remove data points where all of the labels are 0.\n",
    "    train_mask=np.where(np.sum(y_train,axis=1)==0) # Where zeros\n",
    "    x_train=np.delete(x_train,train_mask,axis=0)\n",
    "    weights=weights[:,:-1] # Remove last column because they were created with Method 1 in mind\n",
    "    weights=np.delete(weights,train_mask,axis=0)\n",
    "    y_train=np.delete(y_train,train_mask,axis=0)\n",
    "    test_mask=np.where(np.sum(y_test,axis=1)==0) # Where zeros\n",
    "    x_test=np.delete(x_test,test_mask,axis=0)\n",
    "    y_test=np.delete(y_test,test_mask,axis=0)    \n",
    "        \n",
    "    print(\"\\t Loaded datasets\")\n",
    "    \n",
    "    model=MLP_2H() # Creating a new instance of the model every fold\n",
    "    \n",
    "    mlp2H_train_dict[fold_n],mlp2H_test_dict[fold_n]=train(model,\n",
    "                                                           X=x_train,\n",
    "                                                           Y=y_train,\n",
    "                                                           X_test=x_test,\n",
    "                                                           Y_test=y_test,\n",
    "                                                           weights=weights,\n",
    "                                                           n_epoch=n_epoch,\n",
    "                                                           batch_size=bs,\n",
    "                                                           lr_init=lr_init,\n",
    "                                                           momentum=momentum,\n",
    "                                                           fold=fold_n)\n",
    "    print(\"Finished for fold {}\".format(fold_n))\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Accuracy outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LYING_DOWN</th>\n",
       "      <td>0.88027</td>\n",
       "      <td>0.80822</td>\n",
       "      <td>0.86869</td>\n",
       "      <td>0.85181</td>\n",
       "      <td>0.78015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SITTING</th>\n",
       "      <td>0.71484</td>\n",
       "      <td>0.72796</td>\n",
       "      <td>0.75393</td>\n",
       "      <td>0.75940</td>\n",
       "      <td>0.65519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIX_walking</th>\n",
       "      <td>0.79510</td>\n",
       "      <td>0.74905</td>\n",
       "      <td>0.79650</td>\n",
       "      <td>0.78035</td>\n",
       "      <td>0.79833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIX_running</th>\n",
       "      <td>0.81043</td>\n",
       "      <td>0.68953</td>\n",
       "      <td>0.65098</td>\n",
       "      <td>0.50825</td>\n",
       "      <td>0.60157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BICYCLING</th>\n",
       "      <td>0.79356</td>\n",
       "      <td>0.84230</td>\n",
       "      <td>0.83909</td>\n",
       "      <td>0.84474</td>\n",
       "      <td>0.87056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SLEEPING</th>\n",
       "      <td>0.89524</td>\n",
       "      <td>0.82783</td>\n",
       "      <td>0.86959</td>\n",
       "      <td>0.87283</td>\n",
       "      <td>0.77201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAB_WORK</th>\n",
       "      <td>0.72871</td>\n",
       "      <td>0.70359</td>\n",
       "      <td>0.72823</td>\n",
       "      <td>0.73477</td>\n",
       "      <td>0.44964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN_CLASS</th>\n",
       "      <td>0.67139</td>\n",
       "      <td>0.71738</td>\n",
       "      <td>0.75394</td>\n",
       "      <td>0.76483</td>\n",
       "      <td>0.74447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN_A_MEETING</th>\n",
       "      <td>0.71509</td>\n",
       "      <td>0.70014</td>\n",
       "      <td>0.75625</td>\n",
       "      <td>0.77028</td>\n",
       "      <td>0.62364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC_main_workplace</th>\n",
       "      <td>0.78389</td>\n",
       "      <td>0.78341</td>\n",
       "      <td>0.77411</td>\n",
       "      <td>0.82911</td>\n",
       "      <td>0.66705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_indoors</th>\n",
       "      <td>0.73813</td>\n",
       "      <td>0.73381</td>\n",
       "      <td>0.69667</td>\n",
       "      <td>0.70519</td>\n",
       "      <td>0.59047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_outside</th>\n",
       "      <td>0.79722</td>\n",
       "      <td>0.81519</td>\n",
       "      <td>0.77021</td>\n",
       "      <td>0.77465</td>\n",
       "      <td>0.81072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN_A_CAR</th>\n",
       "      <td>0.81149</td>\n",
       "      <td>0.76579</td>\n",
       "      <td>0.79810</td>\n",
       "      <td>0.74069</td>\n",
       "      <td>0.75569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ON_A_BUS</th>\n",
       "      <td>0.74708</td>\n",
       "      <td>0.77574</td>\n",
       "      <td>0.81823</td>\n",
       "      <td>0.83972</td>\n",
       "      <td>0.68492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIVE_-_I_M_THE_DRIVER</th>\n",
       "      <td>0.84692</td>\n",
       "      <td>0.78481</td>\n",
       "      <td>0.83034</td>\n",
       "      <td>0.78813</td>\n",
       "      <td>0.73659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIVE_-_I_M_A_PASSENGER</th>\n",
       "      <td>0.69131</td>\n",
       "      <td>0.75052</td>\n",
       "      <td>0.80536</td>\n",
       "      <td>0.84413</td>\n",
       "      <td>0.78306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC_home</th>\n",
       "      <td>0.79889</td>\n",
       "      <td>0.76807</td>\n",
       "      <td>0.73518</td>\n",
       "      <td>0.78119</td>\n",
       "      <td>0.58291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIX_restaurant</th>\n",
       "      <td>0.69915</td>\n",
       "      <td>0.71422</td>\n",
       "      <td>0.73516</td>\n",
       "      <td>0.78295</td>\n",
       "      <td>0.77103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_IN_POCKET</th>\n",
       "      <td>0.71731</td>\n",
       "      <td>0.71012</td>\n",
       "      <td>0.76570</td>\n",
       "      <td>0.76628</td>\n",
       "      <td>0.73392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_exercise</th>\n",
       "      <td>0.64158</td>\n",
       "      <td>0.79013</td>\n",
       "      <td>0.80098</td>\n",
       "      <td>0.73408</td>\n",
       "      <td>0.78702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COOKING</th>\n",
       "      <td>0.71181</td>\n",
       "      <td>0.54493</td>\n",
       "      <td>0.61490</td>\n",
       "      <td>0.60398</td>\n",
       "      <td>0.69799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHOPPING</th>\n",
       "      <td>0.79846</td>\n",
       "      <td>0.74563</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.72557</td>\n",
       "      <td>0.82116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STROLLING</th>\n",
       "      <td>0.73195</td>\n",
       "      <td>0.78173</td>\n",
       "      <td>0.81089</td>\n",
       "      <td>0.72436</td>\n",
       "      <td>0.63643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRINKING__ALCOHOL_</th>\n",
       "      <td>0.68068</td>\n",
       "      <td>0.59806</td>\n",
       "      <td>0.76599</td>\n",
       "      <td>0.72693</td>\n",
       "      <td>0.51584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BATHING_-_SHOWER</th>\n",
       "      <td>0.63514</td>\n",
       "      <td>0.54932</td>\n",
       "      <td>0.64873</td>\n",
       "      <td>0.59723</td>\n",
       "      <td>0.57064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLEANING</th>\n",
       "      <td>0.66858</td>\n",
       "      <td>0.42385</td>\n",
       "      <td>0.62372</td>\n",
       "      <td>0.60232</td>\n",
       "      <td>0.63572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOING_LAUNDRY</th>\n",
       "      <td>0.43204</td>\n",
       "      <td>0.37524</td>\n",
       "      <td>0.33518</td>\n",
       "      <td>0.46626</td>\n",
       "      <td>0.53921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WASHING_DISHES</th>\n",
       "      <td>0.38044</td>\n",
       "      <td>0.53389</td>\n",
       "      <td>0.53801</td>\n",
       "      <td>0.67020</td>\n",
       "      <td>0.46135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WATCHING_TV</th>\n",
       "      <td>0.68116</td>\n",
       "      <td>0.66567</td>\n",
       "      <td>0.74375</td>\n",
       "      <td>0.66949</td>\n",
       "      <td>0.73617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SURFING_THE_INTERNET</th>\n",
       "      <td>0.55149</td>\n",
       "      <td>0.57305</td>\n",
       "      <td>0.71298</td>\n",
       "      <td>0.63260</td>\n",
       "      <td>0.61385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_A_PARTY</th>\n",
       "      <td>0.70094</td>\n",
       "      <td>0.71607</td>\n",
       "      <td>0.75844</td>\n",
       "      <td>0.82599</td>\n",
       "      <td>0.80843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_A_BAR</th>\n",
       "      <td>0.41201</td>\n",
       "      <td>0.36568</td>\n",
       "      <td>0.53625</td>\n",
       "      <td>0.62180</td>\n",
       "      <td>0.42684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC_beach</th>\n",
       "      <td>0.72773</td>\n",
       "      <td>0.50537</td>\n",
       "      <td>0.67180</td>\n",
       "      <td>0.74357</td>\n",
       "      <td>0.62485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SINGING</th>\n",
       "      <td>0.58978</td>\n",
       "      <td>0.45940</td>\n",
       "      <td>0.33092</td>\n",
       "      <td>0.32237</td>\n",
       "      <td>0.50743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TALKING</th>\n",
       "      <td>0.64659</td>\n",
       "      <td>0.67644</td>\n",
       "      <td>0.69544</td>\n",
       "      <td>0.67089</td>\n",
       "      <td>0.62607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMPUTER_WORK</th>\n",
       "      <td>0.65784</td>\n",
       "      <td>0.78416</td>\n",
       "      <td>0.69814</td>\n",
       "      <td>0.75123</td>\n",
       "      <td>0.61719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EATING</th>\n",
       "      <td>0.62529</td>\n",
       "      <td>0.65080</td>\n",
       "      <td>0.65825</td>\n",
       "      <td>0.66418</td>\n",
       "      <td>0.60540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOILET</th>\n",
       "      <td>0.55628</td>\n",
       "      <td>0.51841</td>\n",
       "      <td>0.55228</td>\n",
       "      <td>0.51493</td>\n",
       "      <td>0.65181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GROOMING</th>\n",
       "      <td>0.63598</td>\n",
       "      <td>0.56181</td>\n",
       "      <td>0.65237</td>\n",
       "      <td>0.59948</td>\n",
       "      <td>0.49336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRESSING</th>\n",
       "      <td>0.52825</td>\n",
       "      <td>0.51408</td>\n",
       "      <td>0.53726</td>\n",
       "      <td>0.51574</td>\n",
       "      <td>0.42652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_THE_GYM</th>\n",
       "      <td>0.70259</td>\n",
       "      <td>0.69862</td>\n",
       "      <td>0.78212</td>\n",
       "      <td>0.70269</td>\n",
       "      <td>0.70448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STAIRS_-_GOING_UP</th>\n",
       "      <td>0.70022</td>\n",
       "      <td>0.51111</td>\n",
       "      <td>0.60146</td>\n",
       "      <td>0.73278</td>\n",
       "      <td>0.56109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STAIRS_-_GOING_DOWN</th>\n",
       "      <td>0.70017</td>\n",
       "      <td>0.49540</td>\n",
       "      <td>0.45593</td>\n",
       "      <td>0.82431</td>\n",
       "      <td>0.82175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELEVATOR</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.60745</td>\n",
       "      <td>0.44221</td>\n",
       "      <td>0.49982</td>\n",
       "      <td>0.65993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_standing</th>\n",
       "      <td>0.58874</td>\n",
       "      <td>0.62148</td>\n",
       "      <td>0.63610</td>\n",
       "      <td>0.63597</td>\n",
       "      <td>0.59032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_SCHOOL</th>\n",
       "      <td>0.71066</td>\n",
       "      <td>0.67974</td>\n",
       "      <td>0.71277</td>\n",
       "      <td>0.73420</td>\n",
       "      <td>0.65083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_IN_HAND</th>\n",
       "      <td>0.54492</td>\n",
       "      <td>0.68733</td>\n",
       "      <td>0.66261</td>\n",
       "      <td>0.70017</td>\n",
       "      <td>0.63411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_IN_BAG</th>\n",
       "      <td>0.72077</td>\n",
       "      <td>0.73635</td>\n",
       "      <td>0.80602</td>\n",
       "      <td>0.57663</td>\n",
       "      <td>0.67548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_ON_TABLE</th>\n",
       "      <td>0.61571</td>\n",
       "      <td>0.63248</td>\n",
       "      <td>0.63339</td>\n",
       "      <td>0.61816</td>\n",
       "      <td>0.62308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WITH_CO-WORKERS</th>\n",
       "      <td>0.71824</td>\n",
       "      <td>0.71702</td>\n",
       "      <td>0.76886</td>\n",
       "      <td>0.77206</td>\n",
       "      <td>0.78831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WITH_FRIENDS</th>\n",
       "      <td>0.61521</td>\n",
       "      <td>0.52871</td>\n",
       "      <td>0.63542</td>\n",
       "      <td>0.70248</td>\n",
       "      <td>0.64048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0        1        2        3        4\n",
       "LYING_DOWN               0.88027  0.80822  0.86869  0.85181  0.78015\n",
       "SITTING                  0.71484  0.72796  0.75393  0.75940  0.65519\n",
       "FIX_walking              0.79510  0.74905  0.79650  0.78035  0.79833\n",
       "FIX_running              0.81043  0.68953  0.65098  0.50825  0.60157\n",
       "BICYCLING                0.79356  0.84230  0.83909  0.84474  0.87056\n",
       "SLEEPING                 0.89524  0.82783  0.86959  0.87283  0.77201\n",
       "LAB_WORK                 0.72871  0.70359  0.72823  0.73477  0.44964\n",
       "IN_CLASS                 0.67139  0.71738  0.75394  0.76483  0.74447\n",
       "IN_A_MEETING             0.71509  0.70014  0.75625  0.77028  0.62364\n",
       "LOC_main_workplace       0.78389  0.78341  0.77411  0.82911  0.66705\n",
       "OR_indoors               0.73813  0.73381  0.69667  0.70519  0.59047\n",
       "OR_outside               0.79722  0.81519  0.77021  0.77465  0.81072\n",
       "IN_A_CAR                 0.81149  0.76579  0.79810  0.74069  0.75569\n",
       "ON_A_BUS                 0.74708  0.77574  0.81823  0.83972  0.68492\n",
       "DRIVE_-_I_M_THE_DRIVER   0.84692  0.78481  0.83034  0.78813  0.73659\n",
       "DRIVE_-_I_M_A_PASSENGER  0.69131  0.75052  0.80536  0.84413  0.78306\n",
       "LOC_home                 0.79889  0.76807  0.73518  0.78119  0.58291\n",
       "FIX_restaurant           0.69915  0.71422  0.73516  0.78295  0.77103\n",
       "PHONE_IN_POCKET          0.71731  0.71012  0.76570  0.76628  0.73392\n",
       "OR_exercise              0.64158  0.79013  0.80098  0.73408  0.78702\n",
       "COOKING                  0.71181  0.54493  0.61490  0.60398  0.69799\n",
       "SHOPPING                 0.79846  0.74563  0.81607  0.72557  0.82116\n",
       "STROLLING                0.73195  0.78173  0.81089  0.72436  0.63643\n",
       "DRINKING__ALCOHOL_       0.68068  0.59806  0.76599  0.72693  0.51584\n",
       "BATHING_-_SHOWER         0.63514  0.54932  0.64873  0.59723  0.57064\n",
       "CLEANING                 0.66858  0.42385  0.62372  0.60232  0.63572\n",
       "DOING_LAUNDRY            0.43204  0.37524  0.33518  0.46626  0.53921\n",
       "WASHING_DISHES           0.38044  0.53389  0.53801  0.67020  0.46135\n",
       "WATCHING_TV              0.68116  0.66567  0.74375  0.66949  0.73617\n",
       "SURFING_THE_INTERNET     0.55149  0.57305  0.71298  0.63260  0.61385\n",
       "AT_A_PARTY               0.70094  0.71607  0.75844  0.82599  0.80843\n",
       "AT_A_BAR                 0.41201  0.36568  0.53625  0.62180  0.42684\n",
       "LOC_beach                0.72773  0.50537  0.67180  0.74357  0.62485\n",
       "SINGING                  0.58978  0.45940  0.33092  0.32237  0.50743\n",
       "TALKING                  0.64659  0.67644  0.69544  0.67089  0.62607\n",
       "COMPUTER_WORK            0.65784  0.78416  0.69814  0.75123  0.61719\n",
       "EATING                   0.62529  0.65080  0.65825  0.66418  0.60540\n",
       "TOILET                   0.55628  0.51841  0.55228  0.51493  0.65181\n",
       "GROOMING                 0.63598  0.56181  0.65237  0.59948  0.49336\n",
       "DRESSING                 0.52825  0.51408  0.53726  0.51574  0.42652\n",
       "AT_THE_GYM               0.70259  0.69862  0.78212  0.70269  0.70448\n",
       "STAIRS_-_GOING_UP        0.70022  0.51111  0.60146  0.73278  0.56109\n",
       "STAIRS_-_GOING_DOWN      0.70017  0.49540  0.45593  0.82431  0.82175\n",
       "ELEVATOR                 0.00000  0.60745  0.44221  0.49982  0.65993\n",
       "OR_standing              0.58874  0.62148  0.63610  0.63597  0.59032\n",
       "AT_SCHOOL                0.71066  0.67974  0.71277  0.73420  0.65083\n",
       "PHONE_IN_HAND            0.54492  0.68733  0.66261  0.70017  0.63411\n",
       "PHONE_IN_BAG             0.72077  0.73635  0.80602  0.57663  0.67548\n",
       "PHONE_ON_TABLE           0.61571  0.63248  0.63339  0.61816  0.62308\n",
       "WITH_CO-WORKERS          0.71824  0.71702  0.76886  0.77206  0.78831\n",
       "WITH_FRIENDS             0.61521  0.52871  0.63542  0.70248  0.64048"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean dicts\n",
    "for k,v in mlp2H_test_dict.items():\n",
    "    if k==0:\n",
    "         mlp2H_test_dict_df=pd.DataFrame.from_dict(v,orient='index',columns=[str(k)])\n",
    "    else:\n",
    "        temp_df=pd.DataFrame.from_dict(v,orient='index',columns=[str(k)])\n",
    "        mlp2H_test_dict_df=pd.concat([mlp2H_test_dict_df,temp_df],axis=1)\n",
    "        \n",
    "mlp2H_test_dict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsrishankar/Desktop/Self_study/custom_dl_env/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type MLP_2H. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# Saving trained models\n",
    "save_dir='saved_models/multilabel_classifier/'\n",
    "model_path=save_dir+'mlp_2hidden_nomask'\n",
    "\n",
    "torch.save(model,model_path) # Saving the whole model\n",
    "\n",
    "# Save accuracy data\n",
    "data_sv_dir='summaries/'\n",
    "mlp2H_test_dict_df.to_csv(os.path.join(data_sv_dir,'mlp2H_rowremoval.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (2 Hidden Layers, with Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_2HDrop(\n",
      "  (hidden0): Sequential(\n",
      "    (0): Linear(in_features=170, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.1)\n",
      "    (2): Dropout(p=0.2)\n",
      "  )\n",
      "  (hidden1): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.1)\n",
      "    (2): Dropout(p=0.2)\n",
      "  )\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=51, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP_2HDrop(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_2HDrop,self).__init__()\n",
    "        self.hidden0=nn.Sequential(\n",
    "            nn.Linear(input_size,hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Dropout(0.20)\n",
    "        )\n",
    "        self.hidden1=nn.Sequential(\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Dropout(0.20)\n",
    "        )\n",
    "        self.out=nn.Sequential(\n",
    "            nn.Linear(hidden_size,output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "model=MLP_2HDrop()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Iteratively loading CV data to prevent memory issues\n",
    "\n",
    "mlp2Hdrop_train_dict=dict()\n",
    "mlp2Hdrop_test_dict=dict()\n",
    "\n",
    "for fold_n in [0,1,2,3,4]:\n",
    "    print(\"*\"*50)\n",
    "    print(\"Fold {} loading datasets\".format(fold_n))\n",
    "    with open('dataset/pickled/x_train_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        x_train_temp=pickle.load(f)\n",
    "        x_train,mean,dev = standardize(x_train_temp)\n",
    "    \n",
    "    with open('dataset/pickled/y_train_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        y_train=pickle.load(f)\n",
    "        \n",
    "    with open('dataset/pickled/x_test_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        x_test_temp=pickle.load(f)\n",
    "        x_test=(x_test_temp-mean)/dev\n",
    "    \n",
    "    with open('dataset/pickled/y_test_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        y_test=pickle.load(f)\n",
    "        \n",
    "    with open('dataset/pickled/instance_weights_cv_orig{}.pkl'.format(fold_n),'rb') as f:\n",
    "        weights=pickle.load(f)\n",
    "        \n",
    "    # 1. \n",
    "    # Unclassified Labels Method: Adds a new label named Unclassified if all of the previous labels are 0\n",
    "    # Issue might wrongly cluster together different unlabeled activities together\n",
    "    # y_train=unclassified_labels(y_train)\n",
    "    # y_test=unclassified_labels(y_test)\n",
    "    \n",
    "    # 2.\n",
    "    # Row removal Method: Remove data points where all of the labels are 0.\n",
    "    train_mask=np.where(np.sum(y_train,axis=1)==0) # Where zeros\n",
    "    x_train=np.delete(x_train,train_mask,axis=0)\n",
    "    weights=weights[:,:-1] # Remove last column because they were created with Method 1 in mind\n",
    "    weights=np.delete(weights,train_mask,axis=0)\n",
    "    y_train=np.delete(y_train,train_mask,axis=0)\n",
    "    test_mask=np.where(np.sum(y_test,axis=1)==0) # Where zeros\n",
    "    x_test=np.delete(x_test,test_mask,axis=0)\n",
    "    y_test=np.delete(y_test,test_mask,axis=0)\n",
    "    \n",
    "    print(\"\\t Loaded datasets\")\n",
    "    \n",
    "    model=MLP_2HDrop() # Initialize function# Creating a new instance of the model every fold\n",
    "    \n",
    "    mlp2Hdrop_train_dict[fold_n],mlp2Hdrop_test_dict[fold_n]=train_sensordropout(model,\n",
    "                                                             X=x_train,\n",
    "                                                             Y=y_train,\n",
    "                                                             X_test=x_test,\n",
    "                                                             Y_test=y_test,\n",
    "                                                             weights=weights,\n",
    "                                                             sensor_list=sensor_types,\n",
    "                                                             feature_names=feature_names,\n",
    "                                                             n_epoch=n_epoch,\n",
    "                                                             batch_size=bs,\n",
    "                                                             lr_init=lr_init,\n",
    "                                                             momentum=momentum,\n",
    "                                                             fold=fold_n)\n",
    "    print(\"Finished for fold {}\".format(fold_n))\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Accuracy outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LYING_DOWN</th>\n",
       "      <td>0.87632</td>\n",
       "      <td>0.80964</td>\n",
       "      <td>0.84193</td>\n",
       "      <td>0.84450</td>\n",
       "      <td>0.81255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SITTING</th>\n",
       "      <td>0.69074</td>\n",
       "      <td>0.71483</td>\n",
       "      <td>0.73690</td>\n",
       "      <td>0.74873</td>\n",
       "      <td>0.68284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIX_walking</th>\n",
       "      <td>0.76435</td>\n",
       "      <td>0.74309</td>\n",
       "      <td>0.78582</td>\n",
       "      <td>0.77651</td>\n",
       "      <td>0.78168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIX_running</th>\n",
       "      <td>0.69821</td>\n",
       "      <td>0.75177</td>\n",
       "      <td>0.63962</td>\n",
       "      <td>0.59183</td>\n",
       "      <td>0.59062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BICYCLING</th>\n",
       "      <td>0.76613</td>\n",
       "      <td>0.82744</td>\n",
       "      <td>0.72546</td>\n",
       "      <td>0.81042</td>\n",
       "      <td>0.75530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SLEEPING</th>\n",
       "      <td>0.88592</td>\n",
       "      <td>0.82411</td>\n",
       "      <td>0.84393</td>\n",
       "      <td>0.86414</td>\n",
       "      <td>0.80557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAB_WORK</th>\n",
       "      <td>0.69647</td>\n",
       "      <td>0.69504</td>\n",
       "      <td>0.73906</td>\n",
       "      <td>0.73176</td>\n",
       "      <td>0.71716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN_CLASS</th>\n",
       "      <td>0.67017</td>\n",
       "      <td>0.67937</td>\n",
       "      <td>0.73849</td>\n",
       "      <td>0.76402</td>\n",
       "      <td>0.72427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN_A_MEETING</th>\n",
       "      <td>0.70808</td>\n",
       "      <td>0.68053</td>\n",
       "      <td>0.70407</td>\n",
       "      <td>0.73915</td>\n",
       "      <td>0.63384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC_main_workplace</th>\n",
       "      <td>0.73492</td>\n",
       "      <td>0.76859</td>\n",
       "      <td>0.70640</td>\n",
       "      <td>0.74985</td>\n",
       "      <td>0.69908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_indoors</th>\n",
       "      <td>0.72039</td>\n",
       "      <td>0.73631</td>\n",
       "      <td>0.67951</td>\n",
       "      <td>0.68275</td>\n",
       "      <td>0.59573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_outside</th>\n",
       "      <td>0.77369</td>\n",
       "      <td>0.80512</td>\n",
       "      <td>0.75320</td>\n",
       "      <td>0.75715</td>\n",
       "      <td>0.80862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN_A_CAR</th>\n",
       "      <td>0.77128</td>\n",
       "      <td>0.76736</td>\n",
       "      <td>0.74121</td>\n",
       "      <td>0.70017</td>\n",
       "      <td>0.70159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ON_A_BUS</th>\n",
       "      <td>0.68828</td>\n",
       "      <td>0.69813</td>\n",
       "      <td>0.75897</td>\n",
       "      <td>0.71757</td>\n",
       "      <td>0.67427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIVE_-_I_M_THE_DRIVER</th>\n",
       "      <td>0.78252</td>\n",
       "      <td>0.79185</td>\n",
       "      <td>0.75189</td>\n",
       "      <td>0.73926</td>\n",
       "      <td>0.70700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIVE_-_I_M_A_PASSENGER</th>\n",
       "      <td>0.63732</td>\n",
       "      <td>0.68485</td>\n",
       "      <td>0.70294</td>\n",
       "      <td>0.73389</td>\n",
       "      <td>0.73603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC_home</th>\n",
       "      <td>0.79813</td>\n",
       "      <td>0.77565</td>\n",
       "      <td>0.71001</td>\n",
       "      <td>0.74329</td>\n",
       "      <td>0.66182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIX_restaurant</th>\n",
       "      <td>0.69348</td>\n",
       "      <td>0.71968</td>\n",
       "      <td>0.74925</td>\n",
       "      <td>0.73463</td>\n",
       "      <td>0.71204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_IN_POCKET</th>\n",
       "      <td>0.70299</td>\n",
       "      <td>0.71436</td>\n",
       "      <td>0.74200</td>\n",
       "      <td>0.76782</td>\n",
       "      <td>0.76141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_exercise</th>\n",
       "      <td>0.64473</td>\n",
       "      <td>0.79134</td>\n",
       "      <td>0.73754</td>\n",
       "      <td>0.73962</td>\n",
       "      <td>0.78344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COOKING</th>\n",
       "      <td>0.67266</td>\n",
       "      <td>0.54826</td>\n",
       "      <td>0.47647</td>\n",
       "      <td>0.49762</td>\n",
       "      <td>0.58323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHOPPING</th>\n",
       "      <td>0.74036</td>\n",
       "      <td>0.64251</td>\n",
       "      <td>0.71045</td>\n",
       "      <td>0.71527</td>\n",
       "      <td>0.66461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STROLLING</th>\n",
       "      <td>0.67531</td>\n",
       "      <td>0.70095</td>\n",
       "      <td>0.70146</td>\n",
       "      <td>0.65902</td>\n",
       "      <td>0.78438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRINKING__ALCOHOL_</th>\n",
       "      <td>0.66505</td>\n",
       "      <td>0.60108</td>\n",
       "      <td>0.67848</td>\n",
       "      <td>0.49318</td>\n",
       "      <td>0.39636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BATHING_-_SHOWER</th>\n",
       "      <td>0.62611</td>\n",
       "      <td>0.56482</td>\n",
       "      <td>0.38874</td>\n",
       "      <td>0.58591</td>\n",
       "      <td>0.51867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLEANING</th>\n",
       "      <td>0.66009</td>\n",
       "      <td>0.34988</td>\n",
       "      <td>0.63339</td>\n",
       "      <td>0.60385</td>\n",
       "      <td>0.59916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOING_LAUNDRY</th>\n",
       "      <td>0.74117</td>\n",
       "      <td>0.49475</td>\n",
       "      <td>0.73490</td>\n",
       "      <td>0.63701</td>\n",
       "      <td>0.49824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WASHING_DISHES</th>\n",
       "      <td>0.59793</td>\n",
       "      <td>0.63408</td>\n",
       "      <td>0.53307</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.62753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WATCHING_TV</th>\n",
       "      <td>0.67919</td>\n",
       "      <td>0.64152</td>\n",
       "      <td>0.70525</td>\n",
       "      <td>0.58841</td>\n",
       "      <td>0.72017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SURFING_THE_INTERNET</th>\n",
       "      <td>0.54384</td>\n",
       "      <td>0.57325</td>\n",
       "      <td>0.68305</td>\n",
       "      <td>0.67624</td>\n",
       "      <td>0.55275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_A_PARTY</th>\n",
       "      <td>0.50324</td>\n",
       "      <td>0.55467</td>\n",
       "      <td>0.82522</td>\n",
       "      <td>0.76141</td>\n",
       "      <td>0.65837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_A_BAR</th>\n",
       "      <td>0.60564</td>\n",
       "      <td>0.62097</td>\n",
       "      <td>0.36407</td>\n",
       "      <td>0.74782</td>\n",
       "      <td>0.48023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC_beach</th>\n",
       "      <td>0.75161</td>\n",
       "      <td>0.51255</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.38601</td>\n",
       "      <td>0.59403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SINGING</th>\n",
       "      <td>0.61262</td>\n",
       "      <td>0.19637</td>\n",
       "      <td>0.75008</td>\n",
       "      <td>0.26421</td>\n",
       "      <td>0.48843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TALKING</th>\n",
       "      <td>0.64929</td>\n",
       "      <td>0.67535</td>\n",
       "      <td>0.69930</td>\n",
       "      <td>0.67289</td>\n",
       "      <td>0.60460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMPUTER_WORK</th>\n",
       "      <td>0.65776</td>\n",
       "      <td>0.77192</td>\n",
       "      <td>0.69471</td>\n",
       "      <td>0.75048</td>\n",
       "      <td>0.67105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EATING</th>\n",
       "      <td>0.62939</td>\n",
       "      <td>0.64595</td>\n",
       "      <td>0.65469</td>\n",
       "      <td>0.65564</td>\n",
       "      <td>0.58514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOILET</th>\n",
       "      <td>0.61801</td>\n",
       "      <td>0.43968</td>\n",
       "      <td>0.56899</td>\n",
       "      <td>0.57026</td>\n",
       "      <td>0.61546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GROOMING</th>\n",
       "      <td>0.58868</td>\n",
       "      <td>0.58042</td>\n",
       "      <td>0.53532</td>\n",
       "      <td>0.58273</td>\n",
       "      <td>0.51847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRESSING</th>\n",
       "      <td>0.53739</td>\n",
       "      <td>0.51428</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.52554</td>\n",
       "      <td>0.53438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_THE_GYM</th>\n",
       "      <td>0.63418</td>\n",
       "      <td>0.72573</td>\n",
       "      <td>0.78144</td>\n",
       "      <td>0.68203</td>\n",
       "      <td>0.85488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STAIRS_-_GOING_UP</th>\n",
       "      <td>0.50792</td>\n",
       "      <td>0.52162</td>\n",
       "      <td>0.67202</td>\n",
       "      <td>0.70630</td>\n",
       "      <td>0.71960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STAIRS_-_GOING_DOWN</th>\n",
       "      <td>0.67771</td>\n",
       "      <td>0.50925</td>\n",
       "      <td>0.57501</td>\n",
       "      <td>0.77154</td>\n",
       "      <td>0.81123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELEVATOR</th>\n",
       "      <td>0.86984</td>\n",
       "      <td>0.47508</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.73624</td>\n",
       "      <td>0.66774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_standing</th>\n",
       "      <td>0.58800</td>\n",
       "      <td>0.61920</td>\n",
       "      <td>0.63312</td>\n",
       "      <td>0.63689</td>\n",
       "      <td>0.57627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT_SCHOOL</th>\n",
       "      <td>0.70747</td>\n",
       "      <td>0.69921</td>\n",
       "      <td>0.69490</td>\n",
       "      <td>0.71928</td>\n",
       "      <td>0.66956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_IN_HAND</th>\n",
       "      <td>0.52740</td>\n",
       "      <td>0.67671</td>\n",
       "      <td>0.63650</td>\n",
       "      <td>0.66503</td>\n",
       "      <td>0.64132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_IN_BAG</th>\n",
       "      <td>0.67258</td>\n",
       "      <td>0.71271</td>\n",
       "      <td>0.72571</td>\n",
       "      <td>0.59967</td>\n",
       "      <td>0.67772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHONE_ON_TABLE</th>\n",
       "      <td>0.61154</td>\n",
       "      <td>0.63310</td>\n",
       "      <td>0.61872</td>\n",
       "      <td>0.62024</td>\n",
       "      <td>0.62229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WITH_CO-WORKERS</th>\n",
       "      <td>0.71584</td>\n",
       "      <td>0.69387</td>\n",
       "      <td>0.73593</td>\n",
       "      <td>0.73106</td>\n",
       "      <td>0.71565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WITH_FRIENDS</th>\n",
       "      <td>0.64807</td>\n",
       "      <td>0.50841</td>\n",
       "      <td>0.62253</td>\n",
       "      <td>0.68515</td>\n",
       "      <td>0.60744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0        1        2        3        4\n",
       "LYING_DOWN               0.87632  0.80964  0.84193  0.84450  0.81255\n",
       "SITTING                  0.69074  0.71483  0.73690  0.74873  0.68284\n",
       "FIX_walking              0.76435  0.74309  0.78582  0.77651  0.78168\n",
       "FIX_running              0.69821  0.75177  0.63962  0.59183  0.59062\n",
       "BICYCLING                0.76613  0.82744  0.72546  0.81042  0.75530\n",
       "SLEEPING                 0.88592  0.82411  0.84393  0.86414  0.80557\n",
       "LAB_WORK                 0.69647  0.69504  0.73906  0.73176  0.71716\n",
       "IN_CLASS                 0.67017  0.67937  0.73849  0.76402  0.72427\n",
       "IN_A_MEETING             0.70808  0.68053  0.70407  0.73915  0.63384\n",
       "LOC_main_workplace       0.73492  0.76859  0.70640  0.74985  0.69908\n",
       "OR_indoors               0.72039  0.73631  0.67951  0.68275  0.59573\n",
       "OR_outside               0.77369  0.80512  0.75320  0.75715  0.80862\n",
       "IN_A_CAR                 0.77128  0.76736  0.74121  0.70017  0.70159\n",
       "ON_A_BUS                 0.68828  0.69813  0.75897  0.71757  0.67427\n",
       "DRIVE_-_I_M_THE_DRIVER   0.78252  0.79185  0.75189  0.73926  0.70700\n",
       "DRIVE_-_I_M_A_PASSENGER  0.63732  0.68485  0.70294  0.73389  0.73603\n",
       "LOC_home                 0.79813  0.77565  0.71001  0.74329  0.66182\n",
       "FIX_restaurant           0.69348  0.71968  0.74925  0.73463  0.71204\n",
       "PHONE_IN_POCKET          0.70299  0.71436  0.74200  0.76782  0.76141\n",
       "OR_exercise              0.64473  0.79134  0.73754  0.73962  0.78344\n",
       "COOKING                  0.67266  0.54826  0.47647  0.49762  0.58323\n",
       "SHOPPING                 0.74036  0.64251  0.71045  0.71527  0.66461\n",
       "STROLLING                0.67531  0.70095  0.70146  0.65902  0.78438\n",
       "DRINKING__ALCOHOL_       0.66505  0.60108  0.67848  0.49318  0.39636\n",
       "BATHING_-_SHOWER         0.62611  0.56482  0.38874  0.58591  0.51867\n",
       "CLEANING                 0.66009  0.34988  0.63339  0.60385  0.59916\n",
       "DOING_LAUNDRY            0.74117  0.49475  0.73490  0.63701  0.49824\n",
       "WASHING_DISHES           0.59793  0.63408  0.53307  0.50000  0.62753\n",
       "WATCHING_TV              0.67919  0.64152  0.70525  0.58841  0.72017\n",
       "SURFING_THE_INTERNET     0.54384  0.57325  0.68305  0.67624  0.55275\n",
       "AT_A_PARTY               0.50324  0.55467  0.82522  0.76141  0.65837\n",
       "AT_A_BAR                 0.60564  0.62097  0.36407  0.74782  0.48023\n",
       "LOC_beach                0.75161  0.51255  0.50000  0.38601  0.59403\n",
       "SINGING                  0.61262  0.19637  0.75008  0.26421  0.48843\n",
       "TALKING                  0.64929  0.67535  0.69930  0.67289  0.60460\n",
       "COMPUTER_WORK            0.65776  0.77192  0.69471  0.75048  0.67105\n",
       "EATING                   0.62939  0.64595  0.65469  0.65564  0.58514\n",
       "TOILET                   0.61801  0.43968  0.56899  0.57026  0.61546\n",
       "GROOMING                 0.58868  0.58042  0.53532  0.58273  0.51847\n",
       "DRESSING                 0.53739  0.51428  0.50000  0.52554  0.53438\n",
       "AT_THE_GYM               0.63418  0.72573  0.78144  0.68203  0.85488\n",
       "STAIRS_-_GOING_UP        0.50792  0.52162  0.67202  0.70630  0.71960\n",
       "STAIRS_-_GOING_DOWN      0.67771  0.50925  0.57501  0.77154  0.81123\n",
       "ELEVATOR                 0.86984  0.47508  0.00000  0.73624  0.66774\n",
       "OR_standing              0.58800  0.61920  0.63312  0.63689  0.57627\n",
       "AT_SCHOOL                0.70747  0.69921  0.69490  0.71928  0.66956\n",
       "PHONE_IN_HAND            0.52740  0.67671  0.63650  0.66503  0.64132\n",
       "PHONE_IN_BAG             0.67258  0.71271  0.72571  0.59967  0.67772\n",
       "PHONE_ON_TABLE           0.61154  0.63310  0.61872  0.62024  0.62229\n",
       "WITH_CO-WORKERS          0.71584  0.69387  0.73593  0.73106  0.71565\n",
       "WITH_FRIENDS             0.64807  0.50841  0.62253  0.68515  0.60744"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean dicts\n",
    "for k,v in mlp2Hdrop_test_dict.items():\n",
    "    if k==0:\n",
    "         mlp2Hdrop_test_dict_df=pd.DataFrame.from_dict(v,orient='index',columns=[str(k)])\n",
    "    else:\n",
    "        temp_df=pd.DataFrame.from_dict(v,orient='index',columns=[str(k)])\n",
    "        mlp2Hdrop_test_dict_df=pd.concat([mlp2Hdrop_test_dict_df,temp_df],axis=1)\n",
    "        \n",
    "mlp2Hdrop_test_dict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsrishankar/Desktop/Self_study/custom_dl_env/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type MLP_2HDrop. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# Saving trained models\n",
    "save_dir='saved_models/multilabel_classifier/'\n",
    "model_path=save_dir+'mlp_2hidden_sensordropout_nomask'\n",
    "\n",
    "torch.save(model,model_path) # Saving the whole model\n",
    "\n",
    "# Save accuracy data\n",
    "data_sv_dir='summaries/'\n",
    "mlp2Hdrop_test_dict_df.to_csv(os.path.join(data_sv_dir,'mlp2Hdrop_rowremoval.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12.2333px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "353.983px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "930.85px",
    "left": "2.28333px",
    "right": "1417.63px",
    "top": "578px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
